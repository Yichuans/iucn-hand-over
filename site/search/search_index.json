{
    "docs": [
        {
            "location": "/", 
            "text": "About\n\n\nThis is the last project I undertake at IUCN's World Heritage Programme. \n\n\nI intend to document important work in the past seven years in order to pass on necessary information to the team. Apart from the usual 'where to find things', I also took the liberty to include high-level explanations of current projects, datasets, tools and methodologies, and in some cases with significant details if deemed useful or has not been documented elsewhere.\n\n\nThe reasons I chose to use a web format in organising the handover content are two folds. First and foremost, it does a fabulously better job in searching - with frictionless access, I hope the my handover is not only useful, but \nusable and used\n. Second, I embrace the idea of \nopen knowledge\n in that the free access to knowledge is a good thing as it benefits all. I will not document any materials in this handover that may be a breach of confidentiality or represent a conflict of interest. My work builds on and benefits from those who share their knowledge in the public domain, and I feel compelled to do the same.\n\n\nBelow shows how content is organised but you may find it easier to use the \nsearch docs\n box to locate content using keywords.\n\n\n\n\nWorld Heritage GIS database\n. This includes, amongst others, the maintenance of the database, updates, and statistics and export that depend on the database.\n\n\nComparative analysis\n. As contrary to the methodology, which has been published as a separate document, will focus on the technical side of the analysis in GIS, should future users require to replicate or improve the semi-automatic process utilised in recent years. \n\n\nWorld Heritage analyses\n. Originally called the knowledge lab, this initiative aims to create a digital platform where new ideas about knowledge dissemination and presentation can be tried and tested\n\n\nWorld Heritage datasheets\n. A descriptive summary of natural World Heritage sites at the time of inscription\n\n\nFolder structure\n, on my workstation. It provides a brief explanation on the overall structure, and more importantly on where to find things\n\n\nWorld Heritage Outlook\n. This flagship project aims to estbalish a proactive monitoring mechanism to improve conservation outlooks. It covers a significant amount of my time and some prototypes in the World Heritage Analyses in fact deliver the activities and tasks required by this project.\n\n\nGeoprocessing methodologies\n. A collection of scripts I developed to automate tasks or increase productivity, such as map batching and parallel computing.\n\n\nPresentations\n. A collection of recent presentations.\n\n\n\n\nAcknowledgement\n\n\nI would also like to take this opportunity to thank Tim and Naomi for their oversight and mentoring, and for making the secondment arrangement a success. Thanks also go to the two wonderful teams at IUCN (\nWorld Heritage Programme\n) and at UNEP-WCMC (\nProtected Areas Programme\n), and to many colleagues I have worked with during the long seven years. I hope to remain in touch with you all and I wish you all the very best in 2018 and onwards.", 
            "title": "Home"
        }, 
        {
            "location": "/#about", 
            "text": "This is the last project I undertake at IUCN's World Heritage Programme.   I intend to document important work in the past seven years in order to pass on necessary information to the team. Apart from the usual 'where to find things', I also took the liberty to include high-level explanations of current projects, datasets, tools and methodologies, and in some cases with significant details if deemed useful or has not been documented elsewhere.  The reasons I chose to use a web format in organising the handover content are two folds. First and foremost, it does a fabulously better job in searching - with frictionless access, I hope the my handover is not only useful, but  usable and used . Second, I embrace the idea of  open knowledge  in that the free access to knowledge is a good thing as it benefits all. I will not document any materials in this handover that may be a breach of confidentiality or represent a conflict of interest. My work builds on and benefits from those who share their knowledge in the public domain, and I feel compelled to do the same.  Below shows how content is organised but you may find it easier to use the  search docs  box to locate content using keywords.   World Heritage GIS database . This includes, amongst others, the maintenance of the database, updates, and statistics and export that depend on the database.  Comparative analysis . As contrary to the methodology, which has been published as a separate document, will focus on the technical side of the analysis in GIS, should future users require to replicate or improve the semi-automatic process utilised in recent years.   World Heritage analyses . Originally called the knowledge lab, this initiative aims to create a digital platform where new ideas about knowledge dissemination and presentation can be tried and tested  World Heritage datasheets . A descriptive summary of natural World Heritage sites at the time of inscription  Folder structure , on my workstation. It provides a brief explanation on the overall structure, and more importantly on where to find things  World Heritage Outlook . This flagship project aims to estbalish a proactive monitoring mechanism to improve conservation outlooks. It covers a significant amount of my time and some prototypes in the World Heritage Analyses in fact deliver the activities and tasks required by this project.  Geoprocessing methodologies . A collection of scripts I developed to automate tasks or increase productivity, such as map batching and parallel computing.  Presentations . A collection of recent presentations.", 
            "title": "About"
        }, 
        {
            "location": "/#acknowledgement", 
            "text": "I would also like to take this opportunity to thank Tim and Naomi for their oversight and mentoring, and for making the secondment arrangement a success. Thanks also go to the two wonderful teams at IUCN ( World Heritage Programme ) and at UNEP-WCMC ( Protected Areas Programme ), and to many colleagues I have worked with during the long seven years. I hope to remain in touch with you all and I wish you all the very best in 2018 and onwards.", 
            "title": "Acknowledgement"
        }, 
        {
            "location": "/world-heritage-database/", 
            "text": "Introduction\n\n\nFor the most part, the database hosts the natural World Heritage sites boundary, i.e., the actual GIS data, and other relevant, essential information such as criteria, types and endangered status. \n\n\nIt was originally built to power the comparative analyses, but at the same time serves as a pipeline to other formats, for example, for inclusion in the WDPA, shapefile/geodatabase feature class, the \nWorld Heritage Feature service\n(via data dumps)\n\n\nThe management of the database is noticeably different from other datasets in the WDPA. Rather than using the proprietary esri ArcGIS suite, it sits in an open source enterprise database \nPostgreSQL\n with the \nPostGIS\n spatial extension. The key benefit, apart from having no lock-in or dependency on expensive commercial software, is that the data can be managed in an industry standard way, and freely, using the SQL language.\n\n\nPostgreSQL database\n\n\nUnfortunately, being only a database, PostgreSQL/PostGIS does not have a GUI(graphic user interface) that allow read, create, update or delete spatial data, essential tasks for a GIS, which ArcGIS excels in.\n\n\nTo make the best of two worlds, a compromised solution was engineered to allow ArcGIS to interface with the underlying database. It was not an easy solution back in the days and some of the \ntechniques\n may have become obsolete over time. There are probably better options these days - therefore I intend not to complicate matters by explaining \nhow to set up\n the system as is, but rather focusing on \nhow to use\n the system.\n\n\nHere are the guiding principles when using the database.\n\n\n\n\nUse ArcGIS (ArcCatalog) to view, add, modify and delete World Heritage data\n\n\nExercise caution\n when using PostgreSQL to \nmodify\n information specifically managed by ArcGIS, notably in the \nsde\n schema, as they might irreversibly corrupt the data, rendering the ArcGIS interface unusable. For example, \nnever delete\n World Heritage tables in the \nsde\n schema in PostgreSQL directly!\n\n\nCreate spatial functions in views, such as \nst_intersects\n, in PostgreSQL/PostGIS to analyse data, instead of using equivalent ArcGIS functions (really, this is the main reason for the setup!)\n\n\n\n\nWhat is in the \nsde\n schema?\n\n\nThe schema is effectively managed by the ArcSDE (Spatial Database Engine), a middleware bridging between the ArcGIS and the underlying enterprise level database. One important thing to note is that in order for the database to work both ways, \nPG_Geometry\n must be specified as the type. If not, by default the native ArcGIS geometry would be use, which PostgreSQL/PostGIS will \nnot\n be able to recognise and use.\n\n\n\n\n\n\nb_wh_iso_geom\n: the single most important table, with the geometry field. It holds boundary data, including past records since 2013 (when the migrate to this system happened). Each site, identified by the id \nwdpaid\n, may have multiple records, i.e. several boundaries or geometries, depending on the number of past modification or updates. The system relies on the \nyear\n and \nevent_cat\n fields to find the most recent boundary. This table also features the \niso3\n field to differentiate transboundary site, enabling easy aggregation of country level statistics\n\n\n\n\n\n\nb_wh_attr\n: non-spatial attributes, including names, status_year (first inscription) and reported areas\n\n\n\n\n\n\nb_wh_ids\n: a mapping of \nwdpaid\n and \nunesid\n, which is a different set of ids referencing the systems used by UNESCO\n\n\n\n\n\n\nb_wh_crit\n: a mapping of \nwdpaid\n and \ncrit\n, useful for grouping results based on criteria\n\n\n\n\n\n\nb_wh_region\n: country name, unesco regions and subregions\n\n\n\n\n\n\nblk_crit\n: a lookup table converting numeric numbers and Roman letters (useful for the \ncrit\n field)\n\n\n\n\n\n\ntag_unesco_xxx\n: tags for UNESCO dangered status, forest and marine\n\n\n\n\n\n\nThe above tables form the basis of the World Heritage database, however, the direct use of them may be inconvenient or difficult, and thus is discouraged. \n\n\nFor the purpose of using the database, a set of convenient views (consider them as temporary or dynamic tables) have been created and placed under the \narcgis\n schema. They are higher-level and more commonly used. \n\n\n\n\nv_wh_spatial\n: a feature class like view, with similar WDPA fields. This builds on the \nv_wh_iso_geom_latest\n and other relevant information\n\n\nv_wh_spatial_non_agg\n: the same as above but transboundary sites are recorded as separate entities, i.e., not aggregated as one geometry.\n\n\nv_wh_iso_geom_latst\n: the latest geometry for each site (queried from the \nsde.b_wh_iso_geom\n table)\n\n\nv_wh_non_spatial_xxx\n: like \nv_wh_spatial\n but without the geometry field - faster view as it does not retrieve any geometry\n\n\nv_wh_geojson\n: geojson format export\n\n\ndump_v_wh_for_wdpa\n: feature class export, for inclusion in the WDPA\n\n\n\n\nThese views can be inspected within a database management environment such as \nPgAdmin\n, exported to other formats (CSV, shapefiles or others), or open directly in the ArcGIS environment via a \ndatabase connection\n. Since they are dynamically generated views, as long as the underlying data are up-to-date, they always reflect the latest in the database.\n\n\nHow-to: update the database\n\n\nI update the database annually, after the conclusion of World Heritage Committee meeting, in July or August. This ensures decisions on nominations and modifications are timely reflected. I also take this opportunity to incorporate improved boundaries (usually higher accuracy) for other sites, for example, those submitted by States Parties as part of their updates to the WDPA.\n\n\nTo update the database, one may go directly to the database without using ArcGIS, but it is less straightforward and requires a higher level of technical sophistication. This method usually involves using command line tools such as \npsql\n and \nshape2pgsql\n to convert input shapefiles (see \nPostGIS pgsql2shp shp2pgsql cheetsheet\n) to SQL statements, and then issue separate SQL commands directly to add or modify records in relevant tables under the \nsde\n schema.\n\n\nFor example, the below command imports a shapefile to a staging location in the database\n\n\nshp2pgsql -g geom -s 4326 -I -w \nLATIN1\n E:\\Yichuan\\sites2013\\edit_38COM.shp public.z_updategeom | psql -p 5432 -d whs_v2 -U postgres\n\n\n\n\nI recommend using ArcGIS to interface with the database - it is a design feature for convenience. Use editing tools in ArcGIS to update the database (as you would normally do with any spatial data). For further convenience, I created an esri map template for this purpose. For example, the 2017 updates can be found at:\n\n\n\n\nE:\\Yichuan\\sites2016\\inscription\\inscription.mxd\n\n\n\n\nEnsure all \nsde.b_xxx\n, \nsde.tag_xxx\n and \nsde.ref_iso3\n tables are updated.\n\n\nStatistics\n\n\nBeing database views, statistics reflect automatically and dynamically from the latest dataset. They are placed under the \narcgis\n schema, and involve: total World Heritage area and average size (in sqkm), number of sites, categories such as forest, marine, danger listing. Since the summaries share similar traits, I re-used the same logic but produced statistics at different levels. The statistics are mainly for the \nWorld Heritage factsheet\n (it requires additional information about global statistics from the WDPA)\n\n\n\n\nstats_wh_site\n: site level\n\n\nstats_wh_country\n: country level\n\n\nstats_wh_region\n: regional level\n\n\nstats_wh_world\n: global level\n\n\n\n\nMarine and terrestrial statistics require an ad-hoc spatial analysis, as site boundaries are not saved as split entities between the two realms. The result can be found in the \narcgis.intersect_wh_wvs\n. This view takes a very long time to open due to significant work involving geometric operations. For convenience, I would recommend a table \nt_intersect_wh_wvs_xxx\n be created (and replace the old table) to save time - it makes subsequent queries a breeze...", 
            "title": "Database"
        }, 
        {
            "location": "/world-heritage-database/#introduction", 
            "text": "For the most part, the database hosts the natural World Heritage sites boundary, i.e., the actual GIS data, and other relevant, essential information such as criteria, types and endangered status.   It was originally built to power the comparative analyses, but at the same time serves as a pipeline to other formats, for example, for inclusion in the WDPA, shapefile/geodatabase feature class, the  World Heritage Feature service (via data dumps)  The management of the database is noticeably different from other datasets in the WDPA. Rather than using the proprietary esri ArcGIS suite, it sits in an open source enterprise database  PostgreSQL  with the  PostGIS  spatial extension. The key benefit, apart from having no lock-in or dependency on expensive commercial software, is that the data can be managed in an industry standard way, and freely, using the SQL language.", 
            "title": "Introduction"
        }, 
        {
            "location": "/world-heritage-database/#postgresql-database", 
            "text": "Unfortunately, being only a database, PostgreSQL/PostGIS does not have a GUI(graphic user interface) that allow read, create, update or delete spatial data, essential tasks for a GIS, which ArcGIS excels in.  To make the best of two worlds, a compromised solution was engineered to allow ArcGIS to interface with the underlying database. It was not an easy solution back in the days and some of the  techniques  may have become obsolete over time. There are probably better options these days - therefore I intend not to complicate matters by explaining  how to set up  the system as is, but rather focusing on  how to use  the system.  Here are the guiding principles when using the database.   Use ArcGIS (ArcCatalog) to view, add, modify and delete World Heritage data  Exercise caution  when using PostgreSQL to  modify  information specifically managed by ArcGIS, notably in the  sde  schema, as they might irreversibly corrupt the data, rendering the ArcGIS interface unusable. For example,  never delete  World Heritage tables in the  sde  schema in PostgreSQL directly!  Create spatial functions in views, such as  st_intersects , in PostgreSQL/PostGIS to analyse data, instead of using equivalent ArcGIS functions (really, this is the main reason for the setup!)   What is in the  sde  schema?  The schema is effectively managed by the ArcSDE (Spatial Database Engine), a middleware bridging between the ArcGIS and the underlying enterprise level database. One important thing to note is that in order for the database to work both ways,  PG_Geometry  must be specified as the type. If not, by default the native ArcGIS geometry would be use, which PostgreSQL/PostGIS will  not  be able to recognise and use.    b_wh_iso_geom : the single most important table, with the geometry field. It holds boundary data, including past records since 2013 (when the migrate to this system happened). Each site, identified by the id  wdpaid , may have multiple records, i.e. several boundaries or geometries, depending on the number of past modification or updates. The system relies on the  year  and  event_cat  fields to find the most recent boundary. This table also features the  iso3  field to differentiate transboundary site, enabling easy aggregation of country level statistics    b_wh_attr : non-spatial attributes, including names, status_year (first inscription) and reported areas    b_wh_ids : a mapping of  wdpaid  and  unesid , which is a different set of ids referencing the systems used by UNESCO    b_wh_crit : a mapping of  wdpaid  and  crit , useful for grouping results based on criteria    b_wh_region : country name, unesco regions and subregions    blk_crit : a lookup table converting numeric numbers and Roman letters (useful for the  crit  field)    tag_unesco_xxx : tags for UNESCO dangered status, forest and marine    The above tables form the basis of the World Heritage database, however, the direct use of them may be inconvenient or difficult, and thus is discouraged.   For the purpose of using the database, a set of convenient views (consider them as temporary or dynamic tables) have been created and placed under the  arcgis  schema. They are higher-level and more commonly used.    v_wh_spatial : a feature class like view, with similar WDPA fields. This builds on the  v_wh_iso_geom_latest  and other relevant information  v_wh_spatial_non_agg : the same as above but transboundary sites are recorded as separate entities, i.e., not aggregated as one geometry.  v_wh_iso_geom_latst : the latest geometry for each site (queried from the  sde.b_wh_iso_geom  table)  v_wh_non_spatial_xxx : like  v_wh_spatial  but without the geometry field - faster view as it does not retrieve any geometry  v_wh_geojson : geojson format export  dump_v_wh_for_wdpa : feature class export, for inclusion in the WDPA   These views can be inspected within a database management environment such as  PgAdmin , exported to other formats (CSV, shapefiles or others), or open directly in the ArcGIS environment via a  database connection . Since they are dynamically generated views, as long as the underlying data are up-to-date, they always reflect the latest in the database.", 
            "title": "PostgreSQL database"
        }, 
        {
            "location": "/world-heritage-database/#how-to-update-the-database", 
            "text": "I update the database annually, after the conclusion of World Heritage Committee meeting, in July or August. This ensures decisions on nominations and modifications are timely reflected. I also take this opportunity to incorporate improved boundaries (usually higher accuracy) for other sites, for example, those submitted by States Parties as part of their updates to the WDPA.  To update the database, one may go directly to the database without using ArcGIS, but it is less straightforward and requires a higher level of technical sophistication. This method usually involves using command line tools such as  psql  and  shape2pgsql  to convert input shapefiles (see  PostGIS pgsql2shp shp2pgsql cheetsheet ) to SQL statements, and then issue separate SQL commands directly to add or modify records in relevant tables under the  sde  schema.  For example, the below command imports a shapefile to a staging location in the database  shp2pgsql -g geom -s 4326 -I -w  LATIN1  E:\\Yichuan\\sites2013\\edit_38COM.shp public.z_updategeom | psql -p 5432 -d whs_v2 -U postgres  I recommend using ArcGIS to interface with the database - it is a design feature for convenience. Use editing tools in ArcGIS to update the database (as you would normally do with any spatial data). For further convenience, I created an esri map template for this purpose. For example, the 2017 updates can be found at:   E:\\Yichuan\\sites2016\\inscription\\inscription.mxd   Ensure all  sde.b_xxx ,  sde.tag_xxx  and  sde.ref_iso3  tables are updated.", 
            "title": "How-to: update the database"
        }, 
        {
            "location": "/world-heritage-database/#statistics", 
            "text": "Being database views, statistics reflect automatically and dynamically from the latest dataset. They are placed under the  arcgis  schema, and involve: total World Heritage area and average size (in sqkm), number of sites, categories such as forest, marine, danger listing. Since the summaries share similar traits, I re-used the same logic but produced statistics at different levels. The statistics are mainly for the  World Heritage factsheet  (it requires additional information about global statistics from the WDPA)   stats_wh_site : site level  stats_wh_country : country level  stats_wh_region : regional level  stats_wh_world : global level   Marine and terrestrial statistics require an ad-hoc spatial analysis, as site boundaries are not saved as split entities between the two realms. The result can be found in the  arcgis.intersect_wh_wvs . This view takes a very long time to open due to significant work involving geometric operations. For convenience, I would recommend a table  t_intersect_wh_wvs_xxx  be created (and replace the old table) to save time - it makes subsequent queries a breeze...", 
            "title": "Statistics"
        }, 
        {
            "location": "/comparative-analysis/", 
            "text": "Introduction\n\n\nThe comparative analysis is an unbiased, consistent analysis of new biodiversity nominations, i.e. criteria (ix) and (x), against existing World Heritage sites. It has been an important piece of information for the consideration of the IUCN World Heritage Panel. The methodology has been documented in the paper \ncomparative analysis methodology for World Heritage nominations under biodiversity criteria\n, published in 2014. \n\n\nThis page aims to introduce the spatial aspect of the analysis and the evolving engineering behind it. \n\n\nUpdates\n\n\n\n\nAdditional documentation can be found on \nGitHub\n, some duplications may be present\n\n\n\n\nFor past comparative analysis results, please visit the section of \nmy folder structure\n\n\nChallenges\n\n\nConceptually, the idea of undertaking spatial analyses for the comparative analysis is rather simple. At the lowest level, it involves the spatial intersection of the boundary of a given new nomination, against a base layer, and then extract some statistics. For example, overlay the terrestrial ecoregion layer with new nominations, and calculate the total area and the proportion of overlap.\n\n\n\n\nThe complexity arises when additional variables come into play. \n\n\nThe first challenge is the plethora of base layers required for the analysis. Despite the increasing availability of global datasets on biodiversity, no data has been so far collated specifically for the purpose of inscriptions of biodiversity World Heritage sites (nor should there ever be). The narratives around the arguments for \nOutstanding Universal Values\n do not provide a clear direction to seek suitable datasets that are also spatially explicit. As a result, the spatial analysis must take into account most, if not all (to the extent possible re time and resources) biodiversity datasets, as to provide the maximum relevant information to support experts in their decision making process.\n\n\nNext, with heterogeneous data sources, different questions require different answers. For example, methodologies to answer a question about the (over)representation of a certain biogeography differ from that inquiring the indicative number of species that might be present. Some seemingly different questions can be grouped employing a similar process to analyse, while others with common traits in the narratives may dictate a drastically different methodology to be developed\n\n\nAdditionally, to enable comparisons, not only nominations are required to be analysed: existing sites, sites on the Tentative List, and in some case certain protected areas need considering as well. These additional datasets multiply the computational load, produces significantly more information, and complicates the process of interpretation and presentation. \n\n\nFinally, the numerous, somewhat abstruse outcome must be put in a lay format that makes sense to the audience. They have to be digested and presented in a clear and logic way.\n\n\nBio-geographic classification, and priorities\n\n\nOne of the classic tasks of the comparative analysis is to look at whether a certain natural environment has been over-represented. If so, justifying another site in the same environment being better than those already on the List becomes significantly more difficult. On the contrary, if a nomination represents a previously unrepresented or under-represented space, the argument for inscription is strengthened.\n\n\nFrom a technical perspective, these types of comparisons require a simple overlay with the same set of statistics. Such data include the terrestrial/marine ecoregions, broad scale priorities such Global 200 priority, biodiversity hotspot, and site level priorities such as KBAs.\n\n\nThe current implementation relies on the PostgreSQL database and the PostGIS spatial extension. ArcGIS plays a role in importing and exporting but is optional.\n\n\nThe source code of the analysis can be found in detail below\n\n\n\n\nhttps://github.com/Yichuans/geoprocessing/blob/master/comparative_analysis/comparative_analysis.py\n\n\n\n\nIn short, the script goes through the dictionary holding look-up information about each layer (name, the schema and table etc), and then intersects a 'theme' with a combined view that has both World Heritage sites and nominations using the function \nrun_ca_for_a_theme\n. It performs two tasks: a) intersection 2) group and combine the resulting statistics. This design accounts for single part polygons, and that results need to reflect different scales (for example, for terrestrial ecoregions, realms and biomes). As the last step, the function \npost_intersection_mk2\n filters only those results relating to the nominated site.\n\n\nTo undertake the analysis, below are the steps required\n\n\n\n\nLoad the newly nominated sites to the \nca_nomi\n schema, preferably via ArcGIS, using the \nPG_Geometry\n configuration. It ensures PostGIS understands and knows how to analyse the geometry\n\n\nCreate in the PostgreSQL database an empty schema, in which results of the analysis would be placed. Typically, this is called \nca_xxxx\n (xxxx refers to the year when the analysis is done)\n\n\nUpdate the base layers if necessary\n\n\nCreate a new main function like below\n\n\n\n\ndef ca_2017():\n    input_nomination = 'ca_nomi.nomi_2017'\n    output_schema = 'ca_2017'\n    for themekey in BASE_LOOKUP.keys():\n        run_ca_for_a_theme(input_nomination, output_schema, themekey, conn_arg=get_ca_conn_arg(2015))\n\n\n\n\nThe first line refers to the table of the nomination in the database, and the second the location of output. The 'for' loop programatically picks each and every base layer, a.k.a., theme, to intersect with the combined view (internally, the \ncreate_combined_wh_nomination_view\n function creates one). Although only the nomination table is specified, the script knows where to find the World Heritage boundary, i.e., \narcgis.v_wh_spatial\n. The maintenance of the World Heritage boundary is documented in the \ndatabase section\n. \n\n\nIt is important to note that an arbitrary threshold of 0.05 is specified to minimise commission errors. This means that any overlap with proportions less than 5% is ignored, as we assume such overlap is likely a result of \nscale issues\n, due to differences in accuracy or scale.\n\n\nIncreased productivity\n\n\nThe above process can be improved simply by running the \nrun_ca_for_a_theme\n simultaneously and in parallel. One example is included below (snippet from the 2017 comparative analysis)\n\n\ndef f(themekey):\n    input_nomination = 'ca_nomi.nomi_2017_with_supp'\n    output_schema = 'ca_2017_with_supp'\n    run_ca_for_a_theme(input_nomination, output_schema, themekey, conn_arg=get_ca_conn_arg(2015))\n\n\n# wrap in main\nif __name__ == '__main__':\n    p = Pool(10)\n    keys = BASE_LOOKUP.keys()\n    print keys\n    print(p.map(f,keys))\n\n\n\n\nTentative List sites\n\n\nIf one were to look deeper, both the World Heritage sites and tentative lists do not change, at least not constantly. Thus the results may be cached, i.e., there is no need to re-do the overlay between them the many base layers. Whenever a comparison is needed, one can simply query the cache and pull out relevant records. This is especially true for Tentative List sites, which tend to stay the same for years.\n\n\nFor tentative list sites, the \nrun_ca_tentative\n utilises the same logic in the above methodology and produces a set of database views holding the result.\n\n\nExport to excel\n\n\nWith the results fully generated in the database, I built a process to stitch relevant tables, including lookup tables for contextual information, and export to excel formats to simplify interpretation.\n\n\nEach nomination has an excel table with three tabs, referring to 'biogeographic', 'priority', and 'site-level' respectively. This table includes numerical comparisons between sites.\n\n\nThe 2017 script can be found below\n\n\n\n\nhttps://github.com/Yichuans/geoprocessing/blob/master/comparative_analysis/comparative_analysis_to_excel_2017.py\n\n\n\n\nFor the export to work, the following variables need to be set in the script. The below is taken from the 2017 script\n\n\n# 2017\n\nCOMBINED_WH_NOMINATION_VIEW = 'z_combined_wh_nomination_view'\nWH_ATTR = 'arcgis.v_wh_non_spatial_full' # attribute look up table for world heritage\nTLS_SHAPE = 'tls.tentative' # tentative list site table\n\nOUTPUT_SCHEMA = 'ca_2017_with_supp' # the location of where the output tables are stored\nTLS_SCHEMA = 'ca_tls'   # the location of where the output table for tentative list sites are stored\nTLS_ORGIN = 'tls.origin'    # the original tentative list sites table, for looking up criteria information\n\n# List of nomination IDs\nNOMI_ID = range(9991701, 9991706) + range(99917011, 99917015) + range(99917021, 99917023)\n\n\n\n\n\nLastly, specify the location of the output \noutputfolder\n\n\nif __name__ == '__main__':\n    outputfolder=r\nE:\\Yichuan\\Comparative_analysis_2017\n\n    main(outputfolder)\n\n\n\n\nFull result export\n\n\nSometimes it is necessary to investigate 'apparent' overlaps. This often means either there is a reasonable doubt about the finding being incorrect or an odd, baffling outcome that contradicts expectations.\n\n\nTo this end, the actual proportion of overlap is often required. This effectively requires a full dump of the database, including intermediate results and it can be cumbersome to read.\n\n\nSince 2016, this functionality has been added in a separate script, called \ncomparative_analysis_to_excel_fullresults\n(\nsource code\n). The principal idea is to dump original results of each base layer into 'tabs' in an excel table, in which the overlap of actual area and proportion are recorded. As mentioned before, this is only to be used as a reference, in cases where apparent overlaps may be questionable or justifying findings that are counter-intuitive.\n\n\nSpecies richness\n\n\nTo counter the often inflated number of species reported in nomination dossiers, a consistent metric has been developed that counts the indicative species richness by interrogating \nthe IUCN Red List of Threatened Species\n. This presents an opportunity to examine the relative abundance of biodiversity through surrogates of comprehensively assessed taxa - it is particularly useful for criterion (x)\n\n\nThe IUCN Red List of Threatened Species has a spatially explicit database that could be requested from its website or directly from the Red List Unit. The usual practice is to include only species range polygons of presence 1 and 2, origin 1 and 2, and seasonality 1, 2, and 3. Clean the database by repairing and dicing if necessary. If not present already, create an index on the field \nid_no\n that uniquely identifies a species. This speeds up the analysis considerably.\n\n\nThe next step is to programatically call \narcpy.SelectLayerByLocation\n for each species and the World Heritage sites and nominations. It may take a long time to go through every one of the 70k+ species with boundaries. The end result is a two column table recording the \nid_no\n for species and \nwdpaid\n for WH sites and nominations. \n\n\nLoad the resulting table and the non-spatial Red List data to the same PostgreSQL database with rest of the results. The species richness makes lots of assumption and refers to the same look up tables in the section on \nBio-geographic classification, and priorities\n\n\nThe following information need to be specified in the script \ncomparative_analysis/comparative_analysis_group_species.py\n (\nsource code\n), which retrieves information from lookup tables and generate a master view with different species richness count for different taxonomies (including only those that are threatened)\n\n\n# the resulting richness analysis table\nall_sp = \nad_hoc.species_ca_2017\n\nall_sp_taxonid = 'species_ca_2017.id_no' # must not be the same as all_sis_taxonid\nall_sp_baseid = 'species_ca_2017.wdpaid'\n\n# look up table, non spatial version of the input Red List data\nall_sis = \nad_hoc.rl_2017_2_pos\n\nall_sis_taxonid = \nrl_2017_2_pos.id_no\n\n\n# WH/nomi name look up table\n# NOTE: assuming wdpaid is present!!!!!\nwh_nomi_lookup = \nca_2017_with_supp.z_combined_wh_nomination_view\n\nwh_nomi_name = \nz_combined_wh_nomination_view.en_name\n\n\n# name\nKINGDOM_FIELD = 'kingdom'\nCLASS_FIELD = 'class'\nBINOMIAL_FIELD = 'binomial'\nRL_FIELD = 'code'\n\n\n\n\nLastly, specify the output schema, typical the same location as the one for the biogeographic/priority results\n\n\nmain('ca_2017_with_supp')\n\n\n\n\nThe technical implementation is also documented in the \nrichness template section in geoprocessing\n\n\nIrreplaceability\n\n\nThe irreplaceability implements the methodology described by \nthe paper in Science\n. \n\n\nThe shortcoming of using the Red List range polygons is rather obvious: it does little to mitigate the effect of over-estimation of their Area of Occupancy (AOO) and that richness counts almost certainly inflate the actual number of species. The irreplaceability metric overcomes this by using sigmoid functions to translate actual proportional overlaps and aggregate to a single value that, to a certain extent, captures both richness and endemism.\n\n\nThe implementation can be found below (\nsource code\n)\n\n\n\ndef species_irreplaceability(x):\n    \nx is the percentage in decimal\n\n\n    x = x*100\n\n    def h(x):\n    # h(x) as specified\n        miu = 39\n        s = 9.5\n        tmp = -(x - miu)/ s\n        denominator = 1 + np.exp(tmp)\n        return 1/denominator\n\n\n    return (h(x) - h(0))/(h(100) - h(0))\n\n\n\n\n\nBecause the irreplaceability analysis builds on the full intersection between the WDPA and the Red List (the pairwise percentage overlap value), which is in itself a massive undertaking, I cannot re-run irreplaceability without being given the result of the full intersection first. For this reason, up to this day, only two runs exist (the latest updated in 2015, which we rely on).\n\n\nStatic maps\n\n\nMaps accompany the comparative analysis are authored using ArcMap, and later ArcPro. \n\n\nThe template is pre-authored and re-used, although labelling and styles may be dynamically adjusted. To the extent possible, I try to automate the production of maps. In ArcMap, this is done via \narcpy.mapping\n package that produces maps by iterating each feature in the nomination feature class. The export process outputs any format supported by ArcMap, usually in PNG for the reports or AI/EPS/PDF for printing. The source code could be found in the \ngeoprocessing section\n\n\nLater, new capabilities in ArcPro make the above process obsolete. There is no need any more to write any code. The iteration instead require a pre-cooked extent feature class be created to represent the viewport of maps. The default data-driven pages can loop through each extent feature and export maps of desired formats. Worth mentioning is that PDF format maps also have the ability to turn on and off layers, that in a way mimics behaviours of a dynamic map, enabling readers to interrogate the map themselves. \n\n\nThe 2017 static maps can be found below:\n\n\n\n\nE:\\Yichuan\\Comparative_analysis_2017\\static_maps\n\n\n\n\nInteractive maps\n\n\nInteractive maps\n are revolutionising the way people use maps. No longer needed are multiple static maps to represent different scales and perspectives, due to the \nprogressive disclosure design\n of these dynamic maps. One can continue to present a particular view to the audience, but more importantly users are now empowered to make their own choices and inspect whatever they deem useful and interesting.\n\n\nBelow is an example of the 2017 dynamic maps for the comparative analysis, and it works on all devices, even allowing embedding in this handover report. \n\n\n\n\n\nConclusion\n\n\nIn hindsight, the current implementation of comparative analysis is probably over-engineered (slightly). The original implementation relies entirely on SQLs, which had severe shortcomings regarding flow control. It was extremely inflexible in a pure database environment. Python, being a generic \n'glue language'\n, was chosen to generate SQL statements and feed to the database management system. The result is a somewhat haphazard concoction of \nspaghetti codes\n, and unnecessarily engineered with an Objective Oriented Design. \n\n\nFor future improvement, a pure function based structure may be a better alternative. Or, considering the massive improvement of the \narcpy\n library and the fine controls of geometry it now has, it may be wise to migrate back to an ArcGIS based system, as it has become increasingly easier to develop and maintain. Further debate on pros and cons is required - going back to a solution based on proprietary software seems counter-intuitive in the present day of open data and open science.", 
            "title": "Comparative analysis"
        }, 
        {
            "location": "/comparative-analysis/#introduction", 
            "text": "The comparative analysis is an unbiased, consistent analysis of new biodiversity nominations, i.e. criteria (ix) and (x), against existing World Heritage sites. It has been an important piece of information for the consideration of the IUCN World Heritage Panel. The methodology has been documented in the paper  comparative analysis methodology for World Heritage nominations under biodiversity criteria , published in 2014.   This page aims to introduce the spatial aspect of the analysis and the evolving engineering behind it.   Updates   Additional documentation can be found on  GitHub , some duplications may be present   For past comparative analysis results, please visit the section of  my folder structure", 
            "title": "Introduction"
        }, 
        {
            "location": "/comparative-analysis/#challenges", 
            "text": "Conceptually, the idea of undertaking spatial analyses for the comparative analysis is rather simple. At the lowest level, it involves the spatial intersection of the boundary of a given new nomination, against a base layer, and then extract some statistics. For example, overlay the terrestrial ecoregion layer with new nominations, and calculate the total area and the proportion of overlap.   The complexity arises when additional variables come into play.   The first challenge is the plethora of base layers required for the analysis. Despite the increasing availability of global datasets on biodiversity, no data has been so far collated specifically for the purpose of inscriptions of biodiversity World Heritage sites (nor should there ever be). The narratives around the arguments for  Outstanding Universal Values  do not provide a clear direction to seek suitable datasets that are also spatially explicit. As a result, the spatial analysis must take into account most, if not all (to the extent possible re time and resources) biodiversity datasets, as to provide the maximum relevant information to support experts in their decision making process.  Next, with heterogeneous data sources, different questions require different answers. For example, methodologies to answer a question about the (over)representation of a certain biogeography differ from that inquiring the indicative number of species that might be present. Some seemingly different questions can be grouped employing a similar process to analyse, while others with common traits in the narratives may dictate a drastically different methodology to be developed  Additionally, to enable comparisons, not only nominations are required to be analysed: existing sites, sites on the Tentative List, and in some case certain protected areas need considering as well. These additional datasets multiply the computational load, produces significantly more information, and complicates the process of interpretation and presentation.   Finally, the numerous, somewhat abstruse outcome must be put in a lay format that makes sense to the audience. They have to be digested and presented in a clear and logic way.", 
            "title": "Challenges"
        }, 
        {
            "location": "/comparative-analysis/#bio-geographic-classification-and-priorities", 
            "text": "One of the classic tasks of the comparative analysis is to look at whether a certain natural environment has been over-represented. If so, justifying another site in the same environment being better than those already on the List becomes significantly more difficult. On the contrary, if a nomination represents a previously unrepresented or under-represented space, the argument for inscription is strengthened.  From a technical perspective, these types of comparisons require a simple overlay with the same set of statistics. Such data include the terrestrial/marine ecoregions, broad scale priorities such Global 200 priority, biodiversity hotspot, and site level priorities such as KBAs.  The current implementation relies on the PostgreSQL database and the PostGIS spatial extension. ArcGIS plays a role in importing and exporting but is optional.  The source code of the analysis can be found in detail below   https://github.com/Yichuans/geoprocessing/blob/master/comparative_analysis/comparative_analysis.py   In short, the script goes through the dictionary holding look-up information about each layer (name, the schema and table etc), and then intersects a 'theme' with a combined view that has both World Heritage sites and nominations using the function  run_ca_for_a_theme . It performs two tasks: a) intersection 2) group and combine the resulting statistics. This design accounts for single part polygons, and that results need to reflect different scales (for example, for terrestrial ecoregions, realms and biomes). As the last step, the function  post_intersection_mk2  filters only those results relating to the nominated site.  To undertake the analysis, below are the steps required   Load the newly nominated sites to the  ca_nomi  schema, preferably via ArcGIS, using the  PG_Geometry  configuration. It ensures PostGIS understands and knows how to analyse the geometry  Create in the PostgreSQL database an empty schema, in which results of the analysis would be placed. Typically, this is called  ca_xxxx  (xxxx refers to the year when the analysis is done)  Update the base layers if necessary  Create a new main function like below   def ca_2017():\n    input_nomination = 'ca_nomi.nomi_2017'\n    output_schema = 'ca_2017'\n    for themekey in BASE_LOOKUP.keys():\n        run_ca_for_a_theme(input_nomination, output_schema, themekey, conn_arg=get_ca_conn_arg(2015))  The first line refers to the table of the nomination in the database, and the second the location of output. The 'for' loop programatically picks each and every base layer, a.k.a., theme, to intersect with the combined view (internally, the  create_combined_wh_nomination_view  function creates one). Although only the nomination table is specified, the script knows where to find the World Heritage boundary, i.e.,  arcgis.v_wh_spatial . The maintenance of the World Heritage boundary is documented in the  database section .   It is important to note that an arbitrary threshold of 0.05 is specified to minimise commission errors. This means that any overlap with proportions less than 5% is ignored, as we assume such overlap is likely a result of  scale issues , due to differences in accuracy or scale.", 
            "title": "Bio-geographic classification, and priorities"
        }, 
        {
            "location": "/comparative-analysis/#increased-productivity", 
            "text": "The above process can be improved simply by running the  run_ca_for_a_theme  simultaneously and in parallel. One example is included below (snippet from the 2017 comparative analysis)  def f(themekey):\n    input_nomination = 'ca_nomi.nomi_2017_with_supp'\n    output_schema = 'ca_2017_with_supp'\n    run_ca_for_a_theme(input_nomination, output_schema, themekey, conn_arg=get_ca_conn_arg(2015))\n\n\n# wrap in main\nif __name__ == '__main__':\n    p = Pool(10)\n    keys = BASE_LOOKUP.keys()\n    print keys\n    print(p.map(f,keys))", 
            "title": "Increased productivity"
        }, 
        {
            "location": "/comparative-analysis/#tentative-list-sites", 
            "text": "If one were to look deeper, both the World Heritage sites and tentative lists do not change, at least not constantly. Thus the results may be cached, i.e., there is no need to re-do the overlay between them the many base layers. Whenever a comparison is needed, one can simply query the cache and pull out relevant records. This is especially true for Tentative List sites, which tend to stay the same for years.  For tentative list sites, the  run_ca_tentative  utilises the same logic in the above methodology and produces a set of database views holding the result.", 
            "title": "Tentative List sites"
        }, 
        {
            "location": "/comparative-analysis/#export-to-excel", 
            "text": "With the results fully generated in the database, I built a process to stitch relevant tables, including lookup tables for contextual information, and export to excel formats to simplify interpretation.  Each nomination has an excel table with three tabs, referring to 'biogeographic', 'priority', and 'site-level' respectively. This table includes numerical comparisons between sites.  The 2017 script can be found below   https://github.com/Yichuans/geoprocessing/blob/master/comparative_analysis/comparative_analysis_to_excel_2017.py   For the export to work, the following variables need to be set in the script. The below is taken from the 2017 script  # 2017\n\nCOMBINED_WH_NOMINATION_VIEW = 'z_combined_wh_nomination_view'\nWH_ATTR = 'arcgis.v_wh_non_spatial_full' # attribute look up table for world heritage\nTLS_SHAPE = 'tls.tentative' # tentative list site table\n\nOUTPUT_SCHEMA = 'ca_2017_with_supp' # the location of where the output tables are stored\nTLS_SCHEMA = 'ca_tls'   # the location of where the output table for tentative list sites are stored\nTLS_ORGIN = 'tls.origin'    # the original tentative list sites table, for looking up criteria information\n\n# List of nomination IDs\nNOMI_ID = range(9991701, 9991706) + range(99917011, 99917015) + range(99917021, 99917023)  Lastly, specify the location of the output  outputfolder  if __name__ == '__main__':\n    outputfolder=r E:\\Yichuan\\Comparative_analysis_2017 \n    main(outputfolder)", 
            "title": "Export to excel"
        }, 
        {
            "location": "/comparative-analysis/#full-result-export", 
            "text": "Sometimes it is necessary to investigate 'apparent' overlaps. This often means either there is a reasonable doubt about the finding being incorrect or an odd, baffling outcome that contradicts expectations.  To this end, the actual proportion of overlap is often required. This effectively requires a full dump of the database, including intermediate results and it can be cumbersome to read.  Since 2016, this functionality has been added in a separate script, called  comparative_analysis_to_excel_fullresults ( source code ). The principal idea is to dump original results of each base layer into 'tabs' in an excel table, in which the overlap of actual area and proportion are recorded. As mentioned before, this is only to be used as a reference, in cases where apparent overlaps may be questionable or justifying findings that are counter-intuitive.", 
            "title": "Full result export"
        }, 
        {
            "location": "/comparative-analysis/#species-richness", 
            "text": "To counter the often inflated number of species reported in nomination dossiers, a consistent metric has been developed that counts the indicative species richness by interrogating  the IUCN Red List of Threatened Species . This presents an opportunity to examine the relative abundance of biodiversity through surrogates of comprehensively assessed taxa - it is particularly useful for criterion (x)  The IUCN Red List of Threatened Species has a spatially explicit database that could be requested from its website or directly from the Red List Unit. The usual practice is to include only species range polygons of presence 1 and 2, origin 1 and 2, and seasonality 1, 2, and 3. Clean the database by repairing and dicing if necessary. If not present already, create an index on the field  id_no  that uniquely identifies a species. This speeds up the analysis considerably.  The next step is to programatically call  arcpy.SelectLayerByLocation  for each species and the World Heritage sites and nominations. It may take a long time to go through every one of the 70k+ species with boundaries. The end result is a two column table recording the  id_no  for species and  wdpaid  for WH sites and nominations.   Load the resulting table and the non-spatial Red List data to the same PostgreSQL database with rest of the results. The species richness makes lots of assumption and refers to the same look up tables in the section on  Bio-geographic classification, and priorities  The following information need to be specified in the script  comparative_analysis/comparative_analysis_group_species.py  ( source code ), which retrieves information from lookup tables and generate a master view with different species richness count for different taxonomies (including only those that are threatened)  # the resulting richness analysis table\nall_sp =  ad_hoc.species_ca_2017 \nall_sp_taxonid = 'species_ca_2017.id_no' # must not be the same as all_sis_taxonid\nall_sp_baseid = 'species_ca_2017.wdpaid'\n\n# look up table, non spatial version of the input Red List data\nall_sis =  ad_hoc.rl_2017_2_pos \nall_sis_taxonid =  rl_2017_2_pos.id_no \n\n# WH/nomi name look up table\n# NOTE: assuming wdpaid is present!!!!!\nwh_nomi_lookup =  ca_2017_with_supp.z_combined_wh_nomination_view \nwh_nomi_name =  z_combined_wh_nomination_view.en_name \n\n# name\nKINGDOM_FIELD = 'kingdom'\nCLASS_FIELD = 'class'\nBINOMIAL_FIELD = 'binomial'\nRL_FIELD = 'code'  Lastly, specify the output schema, typical the same location as the one for the biogeographic/priority results  main('ca_2017_with_supp')  The technical implementation is also documented in the  richness template section in geoprocessing", 
            "title": "Species richness"
        }, 
        {
            "location": "/comparative-analysis/#irreplaceability", 
            "text": "The irreplaceability implements the methodology described by  the paper in Science .   The shortcoming of using the Red List range polygons is rather obvious: it does little to mitigate the effect of over-estimation of their Area of Occupancy (AOO) and that richness counts almost certainly inflate the actual number of species. The irreplaceability metric overcomes this by using sigmoid functions to translate actual proportional overlaps and aggregate to a single value that, to a certain extent, captures both richness and endemism.  The implementation can be found below ( source code )  \ndef species_irreplaceability(x):\n     x is the percentage in decimal \n\n    x = x*100\n\n    def h(x):\n    # h(x) as specified\n        miu = 39\n        s = 9.5\n        tmp = -(x - miu)/ s\n        denominator = 1 + np.exp(tmp)\n        return 1/denominator\n\n\n    return (h(x) - h(0))/(h(100) - h(0))  Because the irreplaceability analysis builds on the full intersection between the WDPA and the Red List (the pairwise percentage overlap value), which is in itself a massive undertaking, I cannot re-run irreplaceability without being given the result of the full intersection first. For this reason, up to this day, only two runs exist (the latest updated in 2015, which we rely on).", 
            "title": "Irreplaceability"
        }, 
        {
            "location": "/comparative-analysis/#static-maps", 
            "text": "Maps accompany the comparative analysis are authored using ArcMap, and later ArcPro.   The template is pre-authored and re-used, although labelling and styles may be dynamically adjusted. To the extent possible, I try to automate the production of maps. In ArcMap, this is done via  arcpy.mapping  package that produces maps by iterating each feature in the nomination feature class. The export process outputs any format supported by ArcMap, usually in PNG for the reports or AI/EPS/PDF for printing. The source code could be found in the  geoprocessing section  Later, new capabilities in ArcPro make the above process obsolete. There is no need any more to write any code. The iteration instead require a pre-cooked extent feature class be created to represent the viewport of maps. The default data-driven pages can loop through each extent feature and export maps of desired formats. Worth mentioning is that PDF format maps also have the ability to turn on and off layers, that in a way mimics behaviours of a dynamic map, enabling readers to interrogate the map themselves.   The 2017 static maps can be found below:   E:\\Yichuan\\Comparative_analysis_2017\\static_maps", 
            "title": "Static maps"
        }, 
        {
            "location": "/comparative-analysis/#interactive-maps", 
            "text": "Interactive maps  are revolutionising the way people use maps. No longer needed are multiple static maps to represent different scales and perspectives, due to the  progressive disclosure design  of these dynamic maps. One can continue to present a particular view to the audience, but more importantly users are now empowered to make their own choices and inspect whatever they deem useful and interesting.  Below is an example of the 2017 dynamic maps for the comparative analysis, and it works on all devices, even allowing embedding in this handover report.", 
            "title": "Interactive maps"
        }, 
        {
            "location": "/comparative-analysis/#conclusion", 
            "text": "In hindsight, the current implementation of comparative analysis is probably over-engineered (slightly). The original implementation relies entirely on SQLs, which had severe shortcomings regarding flow control. It was extremely inflexible in a pure database environment. Python, being a generic  'glue language' , was chosen to generate SQL statements and feed to the database management system. The result is a somewhat haphazard concoction of  spaghetti codes , and unnecessarily engineered with an Objective Oriented Design.   For future improvement, a pure function based structure may be a better alternative. Or, considering the massive improvement of the  arcpy  library and the fine controls of geometry it now has, it may be wise to migrate back to an ArcGIS based system, as it has become increasingly easier to develop and maintain. Further debate on pros and cons is required - going back to a solution based on proprietary software seems counter-intuitive in the present day of open data and open science.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/geoprocessing/", 
            "text": "Introduction\n\n\nThis section explains the \ntechnical\n side of my work, on analysis, maps and automation. \n\n\nThe reason why much of my time has gone into the development, improvement and maintenance of the scripts is simple - next time the same task is requested again, I have an existing method, or previously working solution, that could be re-used or easily adapted. This saves time and increases productivity\n\n\nWherever possible, I try to automate. In some cases, this may represents a poor use of time, i.e., doing it manually step by step may be quicker. Automation for one-off tasks may seem over-engineering, I would argue, however, any time spent on making a tool, or spent on mastering a potentially more productive tool is a justified investment with good productivity return in the long term. \n\n\nFor example, the different implementations of the comparative analysis have led to a reduced workload significantly. See below a graph depicting the amount of days to do the spatial overlay between the plethora of datasets and new nominations. This frees time for species richness, irreplaceability, improved maps, just to name a few.\n\n\n\n\nSome of the tasks share similar technical challenges. Especially for frequent, recurring tasks, it is imperative to develop libraries or templates to minimise repetitive work, and that more time can be made available for more creative tasks and increased productivity. \n\n\nGeoprocessing libraries\n\n\nThe geoprocessing library  (located at \ngeoprocessing\\library\n) is a collection of commonly used functions. They evolve from dedicated solutions to tasks that share technical challenges in common. Most of them relate to spatial analyses, usually involving the use of \narcpy\n library in ArcGIS.\n\n\nIn order to use this library, it must be visible to Python. The easiest way to achieving this is to set the system variable \nPYTHONPATH\n to include the \ngeoprocessing\\library\n path.\n\n\nThe library consists the follow components:\n\n\n\n\nYichuan10.py\n holds a collection of mostly utility functions (with a humble and long history, starting with ArcGIS version 10, which still reflects in the name of the library). Commonly used functions include the two \n'decorators'\n to track memory usage, and time required for executing functions, and the \nGetUniqueValuesFromFeatureLayer_mk2\n function that finds unique values in any given field in a feature layer. The latter simplifies the common task of finding unique IDs, such as the \nwdpaid\n for the WDPA or \nid_no\n for Red List.\n\n\n\n\n\ndef GetUniqueValuesFromFeatureLayer_mk2(inputFc, inputField):\n    \nstring\n, \nstring\n -\n pythonList\n    can be both feature class or feature layer\n\n    pySet = set()\n    with arcpy.da.SearchCursor(inputFc, inputField) as cursor:\n        for row in cursor:\n            pySet.add(row[0])\n\n    return list(pySet)\n\n\n\n\n\n\n\n\n\nYichuanDB.py\n includes a number of useful functions to connect and manipulate PostgreSQL databases. For the most part, I use the \nConnectionParameter\n class to simplify making connections to the database. Convenient functions like \nclean_view\n enables testing and debugging of the comparative analysis, which relies on the direct access and manipulation of data in the database.\n\n\n\n\n\n\nYichuanM.py\n evolved from a suite of utility functions originally resided in the \nYichuan10.py\n that specifically aimed at tasks relating to the automatic/batch production of maps (use pre-authored map template, to be discussed in detail in the \nmap batch template\n)\n\n\n\n\n\n\nYichuanRAS.py\n contains functions that manipulates raster datasets (most of them are for vectors) using both the \narcpy\n but also \ngdal\n, an open source but lower-level alternative. Many of the functions here underpin the \nanalysis of landcover change\n, which implemented the calculation of zonal statistics using \ngdal\n\n\n\n\n\n\nYichuanSR.py\n has many utility functions relating to coordinate systems. For example, the function \nget_desirable_sr\n tries to guess a best optimal coordinate system based on the geometry (the input expects \narcpy.Geometry\n or \narcpy.Extent\n object). This is useful to identify the most appropriate coordinate system when automating map productions - it's never a good idea to use a global coordinate system for a site scale map - use a specific UTM instead! NB. This function pre-dates a similar \nCalculateUTMZone\n function, now included by default in newer releases of ArcGIS.\n\n\n\n\n\n\nMap batching template\n\n\nMaking maps quickly and automatically is essential. \n\n\nImagine a map production task for a paper. Apart from the initial cartographic process, there will be legitimate, repeat requests to adjust layout, styles, and \nminor\n details that entail massive changes, unknown except to the author of the map. Additionally, such adjustments usually involve multiple maps. Finally when it comes to printing or publishing, it is quite common to expect a different format  or a number of formats. The time and efforts required to address these challenges are not trivial but they tend to go unnoticed and under appreciated.\n\n\nI'd like to automate this process as much as possible and reduce workload, therefore I created templates specifically to address the two broad scenarios below:\n\n\n\n\nfrom an ArcGIS Map Document (mxd file) to an exported map of a given format (e.g. pdf file). This is identical to clicking the export button and specify some export parameters.\n\n\nfrom a single Map Document to a series of maps. Typically, each iteration/ feature of a dataset will result in a separate map.\n\n\n\n\nThe underlying technical solution to the above two scenarios involves the \nYichuan10.ExportMXDtoMap\n function, which is a thin, convenient wrapper around built-in map functions in the \narcpy.mapping\n module. \n\n\nThe process usually involves the below steps:\n\n\n\n\npre-author a map (no need to turn on data driven pages)\n\n\nensure there is an index layer with id. They need to be specified and referenced in the map template (only applies to the second scenario).\n\n\n\n\nOne common task in the first scenario is to export a folder of MXD Map Documents, repeatedly with multiple formats. This could be easily achieved with a loop. \n\n\nHere is an \nexample\n to export from a Map Document to a thumbnail, a map for web, and a map for use in Adobe Illustrator (AI format, for further editing and publication).\n\n\nThe second scenario requires a loop to go through the feature class. Depending on its attributes or geometry, the map will then be different. For instance, difference can reflect in the title, extent indicator, spatial reference or on/off of a particular layer. For example, a scenario may be to create a map per species or per protected areas. In this case, the task is less flexible and needs to be adapted to account for unique requirements that cannot be fully captured in a single, reusable template.\n\n\nFor example, the \nsimple map batcher\n sets a definition query, a spatial reference, scale/extent, and then exports a jpg format map with a select resolution. The snippet for the above steps is listed below\n\n\nexportpng = exportfolder + os.sep + str(each) + '.jpg'\n\nquery = '\\\nwdpaid\\\n = ' + str(each)\n\n# only the above wdpaid feature visible\nlayer_index.definitionQuery = query\n\n# set dataframe coordinate system\nsr =arcpy.SpatialReference()\n\nsr_string = Yichuan10.GetFieldValueByID_mk2(layer_index, each, value_field='utm')\n\nsr.loadFromString(sr_string)\n\n# load from its attribute\ndf.spatialReference = sr\n\ndf.extent = layer_index.getExtent()\ndf.scale = df.scale * 1.1\n\n# need to specify a low quality\narcpy.mapping.ExportToJPEG(mxd, exportpng, \nPAGE_LAYOUT\n, resolution = reso, jpeg_quality=60)\n\n\n\n\nN.B. at the end of each iteration, it is good practice to clear the definition query by assigning its value to 'empty' (e.g. in the above case, \nlayer_index.definitionQuery = ''\n)\n\n\nFinally, it is not necessary to adapt the map batch template to automate the production of maps. ArcGIS has a built-in tool call \ndata drive pages\n or \nMap series\n in ArcPro. Their use is beyond the scope of this handover to discuss. In my opinion, data drive pages do not give a fine control in terms of iteration, nor allow the level of customisation that I required for my map production process.\n\n\nRichness template\n\n\nThe origin of this template goes back to 2010 when I was tasked with calculating species richness (counting number of species) using the global hexagon grid. This forms part of the analysis published in the paper titled \nThe Impact of Conservation on the Status of the World\u2019s Vertebrates\n. I re-wrote and improved in Python from the original code written in VBA (by Nick?).\n\n\nThe aim of the template is to simply count the number of overlaying species in each 'unit' for all units in the base layer, be it hexagons or protected areas. While conceptually simple, the implementation may not be straightforward. The beauty of this implementation in my opinion is two fold:\n\n\n\n\nonly perform an intersection test\n\n\nminimal information is recorded\n\n\n\n\nThis means it does not require computationally intensive calculations of the actual intersections (for those who use PostGIS, think of \nst_intersects\n rather than \nst_intersection\n, see \nPostGIS documentation\n), and it only records IDs, minimal information needed for subsequent analyses.\n\n\nThe current implementation relies on iterative \narcpy.SelectLayerByLocation_management\n calls between a species layer and a base layer. For efficiency reasons, the species layer is created within each iteration by \narcpy.MakeFeatureLayer_management\n with a given ID, and deleted after use; in contrast, the base layer is however re-used and resides in the memory persistently, until all iterations are complete. After the base layer is successfully 'selected', a data cursor loops through its records (when \nGetUniqueValuesFromFeatureLayer_mk2\n works on a feature layer with a selection, it will only work on the selected features) and saved as a list of ID strings (each representing a species and a base layer). It is formatted for eventual export to a csv file.\n\n\ndef species_richness_calculation(id, hexagonLyr):\n\n    # make species layer\n    if type(id) in [str, unicode]:\n        exp = '\\\n' + speciesID + '\\\n = ' + '\\'' + str(id) + '\\''\n    elif type(id) in [int, float]:\n        exp = '\\\n' + speciesID + '\\\n = ' + str(id)\n    else:\n        raise Exception('ID field type error')\n\n    # make layers\n    arcpy.MakeFeatureLayer_management(speciesData, speciesLyr, exp)\n\n    # select by locations\n    arcpy.SelectLayerByLocation_management(hexagonLyr, overLapOption, speciesLyr)\n\n    # record it\n    hex_ids = GetUniqueValuesFromFeatureLayer_mk2(hexagonLyr, hexagonID)\n\n    result = list()\n\n    for hex_id in hex_ids:\n        result.append(str(int(id)) + ',' + str(hex_id) + '\\n')\n\n    # get rid of layers\n    arcpy.Delete_management(speciesLyr)\n\n    return result\n\n\n\n\nMore recently, this template has been improved to utilise parallel processing to reduce time. ArcGIS cannot use more than one core at a time, so the challenge dictates dividing the task into smaller chunks for multiple ArcGIS processes, and then finally stitch together. The first parallel solution simply uses the \nparallel template\n, but later solutions incorporate further optimisations in the template to improve efficiency and add additional functionality. More specifically, it involves:\n\n\n\n\nLogger. Analysis with large, multi-source spatial data almost inevitably fail. Therefore it is often necessary to record failing features, investigate and then find specific solutions.\n\n\nMove the making layer logic to the worker. Making layer is expensive (time-consuming) - it is much faster for each worker process to have a layer that they could readily re-use, without having to re-create for each iteration. It's a compromise between keeping task-specific logic from the multi-processing template and achieving better efficiency.\n\n\n\n\nNote the difference between the original worker function (below)\n\n\ndef worker(q, q_out):\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print 'Remaining jobs:', q.qsize()\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = job(job_id)\n        q_out.put(result)\n\n\n\n\nand the adapted worker function. Note the position of \narcpy.MakeFeatureLayer_management\n and \narcpy.Delete_management\n, which would otherwise occur in the \nspecies_richness_calculation\n function.\n\n\ndef worker(q, q_out):\n    # make layer here to reduce overhead\n    arcpy.MakeFeatureLayer_management(hexagonData, hexagonLyr)\n\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print('Time:', datetime.datetime.now().time())\n            print('Remaining jobs:', q.qsize())\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = species_richness_calculation(job_id, hexagonLyr)\n        q_out.put(result)\n\n    arcpy.Delete_management(hexagonLyr)\n\n\n\n\nThe reduction in the time needed to undertake species richness analysis is significant - not quite linear reduction with the increase of CPU cores, but for my computer of 6 cores (12 threads) when using 10 worker process, it sees 7-8 times improvement. For example, the annual richness calculation itself for nomination + existing WH sites takes less than 3 hours, a previously unthinkable performance.\n\n\nFurthermore, due to its generic use case, attempts are made to facilitate a uniform overlapping/binning process, with promising potentials.\n\n\nFor example, by running both the richness calculation between WDPA and the global hexagon grid, and between RedList and the global hexagon grid, the intersection between WDPA and RedList could be easily obtained by using the grid as a look up table. This eliminates the need for complicated spatial overlays. Any further calculations have now reduced to non-spatial tabulation/pivot tables. \n\n\nThe added benefits also include deferring decisions on groupings (for example, threatened species, or country/regional level statistics) towards a later time without re-doing the spatial overlay. This requires analysis be designed in the lowest possible granularity to enable later separation. \n\n\nFor example, if a decision to select species range based on presence, origin and seasonality is to be deferred, the analysis must use the row id (usual \nfid\n) instead of the species id \nid_no\n. This is because \nid_no\n will make no effort to separate between polygons with presence 1 or presence 2 - it only differentiates species\n\n\nThe \nbiodiversity for food and agriculture analysis/BFA FAO analysis\n is a good example with this thinking in mind (Here is \nthe script\n)\n\n\nParallel template\n\n\nThe idea for this template comes from the need to speed up slow-running processes. In the old days, the richness calculation can be painstakingly fiddly and slow: cutting the tasks to smaller chunks to run over a number of machines over a number of days is not only a unpleasant experience nor a sustainable solution (and error prone!). Even with a small subset of protected areas, running richness for all Red List species remains a challenge. \n\n\nOne of the bottlenecks of performance is that ArcGIS Desktop uses one core at a time - it does not take advantage of the full computing capacity of modern multi-core computer architecture. The parallel template was then written as a way of using Python to deploy multiple ArcGIS processes to distribute workload automatically.\n\n\nThe process goes like this:\n\n\n\n\nCreate an input pipe containing a list of unique IDs. Each ID represents a piece of data\n\n\nCreate an initially empty output pipe to host result (will come from the worker processes)\n\n\nCreate a number of worker processes, that actively get data from IDs in the input pipe\n\n\nCreate a worker process, that listens actively for incoming results from the output pipe and then processes them, for example write to an output file\n\n\nThe main function manages the pipes: distributes work, logs and waits for completion\n\n\n\n\nThis is the link to the template\n\n\nNotably, it needs to add the text flag 'STOP' (or for that matter any flag would do as long as it is the same as what the worker process would watch out for) to the input queue.\n\n\n# Add queue of a list of ids to process\nq = get_queue()\n\n# setup and run worker processes\np_workers = list()\nfor i in range(WORKER):\n    print 'Starting worker process:', i\n    p = multiprocessing.Process(target=worker, args=(q, q_out))\n    p_workers.append(p)\n\n# start\nfor p in p_workers:\n    p.start()\n\n# add stop flag to the queue\nfor p in p_workers:\n    q.put('STOP')\n\n\n\n\nThe worker process remains alive until it sees the 'STOP' signal to terminate.\n\n\ndef worker(q, q_out):\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print 'Remaining jobs:', q.qsize()\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = job(job_id)\n        q_out.put(result)\n\n\n\n\nNew data analysis process\n\n\n(It is not really 'new' - but I still have not seen a wide adoption of this novel process being applied in analyses at IUCN or WCMC)\n\n\nThis is an improved work flow that I find myself increasingly using. Typically it begins with a spatial analysis, but only once. All subsequent analyses and visualisation then take place within a \nJupyter notebook\n. For routine analyses, this process seems to work very well, as long as the initial spatial analysis is designed in such a way (with the smallest granularity) that allow further interrogation of data with non-spatial analyses.\n\n\nThe benefits of this process:\n\n\n\n\nless error prone as a result from spatial analysis. Spatial analysis is done only once at the smallest granularity\n\n\nfurther questions of different aggregations can be answered with a non-spatial analysis (as long as the scale is higher than what's used in the spatial analysis)\n\n\nmethodology is fully open, and reproducible with little effort (apart from the initial spatial analysis)\n\n\nthe entire analytical process: data cleaning, analysis, and visualisation is fully documented and can easily be shared\n\n\n\n\nExample analyses:\n\n\n\n\nClimate change vulnerability analysis\n and the \nwriting of report\n\n\nWorld Heritage Outlook 2 analysis\n\n\nBiodiversity for food and agriculture\n\n\nWilderness analysis\n and the \nwriting of methodology\n\n\nICCA species richness\n\n\n\n\nArcGIS and Anaconda\n\n\nOf course, there is no reason why the spatial analysis should be excluded from the above process. Thus, the entire work flow of reading data, performing analysis and exporting or visualising results could be theoretically included in a single workspace such as a Jupyter notebook. \n\n\nThe main challenge is for the Python and its libraries to talk to ArcGIS and vice versa. The problem is that ArcGIS uses a specific version of Python and its numerical libraries, which cannot be updated without breaking the ArcGIS installation (when using \narcpy\n for example).\n\n\nAnaconda\n, a popular Python data science platform, can be used to create an environment to satisfy a specific installation of ArcGIS with compatible versions of other libraries.\n\n\nUSGS has a useful guide on \nhow to use anaconda modules from the esri Python environment\n to set up such a environment.\n\n\nLastly, this may work for small spatial analyses, however, I would argue for the geoprocessing I require (takes hours or even days) this may not be a good idea. Such analyses tend to fail a couple of times and require significant back and forth experimenting and fixing of data manually - these would be better off dealt with separately.\n\n\nI adapted a \ncustomised script\n (authored by \nCurtis Price\n) for my system to connect ArcGIS and Anaconda\n\n\nMiscellaneous\n\n\n(TBA)", 
            "title": "Geoprocessing"
        }, 
        {
            "location": "/geoprocessing/#introduction", 
            "text": "This section explains the  technical  side of my work, on analysis, maps and automation.   The reason why much of my time has gone into the development, improvement and maintenance of the scripts is simple - next time the same task is requested again, I have an existing method, or previously working solution, that could be re-used or easily adapted. This saves time and increases productivity  Wherever possible, I try to automate. In some cases, this may represents a poor use of time, i.e., doing it manually step by step may be quicker. Automation for one-off tasks may seem over-engineering, I would argue, however, any time spent on making a tool, or spent on mastering a potentially more productive tool is a justified investment with good productivity return in the long term.   For example, the different implementations of the comparative analysis have led to a reduced workload significantly. See below a graph depicting the amount of days to do the spatial overlay between the plethora of datasets and new nominations. This frees time for species richness, irreplaceability, improved maps, just to name a few.   Some of the tasks share similar technical challenges. Especially for frequent, recurring tasks, it is imperative to develop libraries or templates to minimise repetitive work, and that more time can be made available for more creative tasks and increased productivity.", 
            "title": "Introduction"
        }, 
        {
            "location": "/geoprocessing/#geoprocessing-libraries", 
            "text": "The geoprocessing library  (located at  geoprocessing\\library ) is a collection of commonly used functions. They evolve from dedicated solutions to tasks that share technical challenges in common. Most of them relate to spatial analyses, usually involving the use of  arcpy  library in ArcGIS.  In order to use this library, it must be visible to Python. The easiest way to achieving this is to set the system variable  PYTHONPATH  to include the  geoprocessing\\library  path.  The library consists the follow components:   Yichuan10.py  holds a collection of mostly utility functions (with a humble and long history, starting with ArcGIS version 10, which still reflects in the name of the library). Commonly used functions include the two  'decorators'  to track memory usage, and time required for executing functions, and the  GetUniqueValuesFromFeatureLayer_mk2  function that finds unique values in any given field in a feature layer. The latter simplifies the common task of finding unique IDs, such as the  wdpaid  for the WDPA or  id_no  for Red List.   \ndef GetUniqueValuesFromFeatureLayer_mk2(inputFc, inputField):\n     string ,  string  -  pythonList\n    can be both feature class or feature layer \n    pySet = set()\n    with arcpy.da.SearchCursor(inputFc, inputField) as cursor:\n        for row in cursor:\n            pySet.add(row[0])\n\n    return list(pySet)    YichuanDB.py  includes a number of useful functions to connect and manipulate PostgreSQL databases. For the most part, I use the  ConnectionParameter  class to simplify making connections to the database. Convenient functions like  clean_view  enables testing and debugging of the comparative analysis, which relies on the direct access and manipulation of data in the database.    YichuanM.py  evolved from a suite of utility functions originally resided in the  Yichuan10.py  that specifically aimed at tasks relating to the automatic/batch production of maps (use pre-authored map template, to be discussed in detail in the  map batch template )    YichuanRAS.py  contains functions that manipulates raster datasets (most of them are for vectors) using both the  arcpy  but also  gdal , an open source but lower-level alternative. Many of the functions here underpin the  analysis of landcover change , which implemented the calculation of zonal statistics using  gdal    YichuanSR.py  has many utility functions relating to coordinate systems. For example, the function  get_desirable_sr  tries to guess a best optimal coordinate system based on the geometry (the input expects  arcpy.Geometry  or  arcpy.Extent  object). This is useful to identify the most appropriate coordinate system when automating map productions - it's never a good idea to use a global coordinate system for a site scale map - use a specific UTM instead! NB. This function pre-dates a similar  CalculateUTMZone  function, now included by default in newer releases of ArcGIS.", 
            "title": "Geoprocessing libraries"
        }, 
        {
            "location": "/geoprocessing/#map-batching-template", 
            "text": "Making maps quickly and automatically is essential.   Imagine a map production task for a paper. Apart from the initial cartographic process, there will be legitimate, repeat requests to adjust layout, styles, and  minor  details that entail massive changes, unknown except to the author of the map. Additionally, such adjustments usually involve multiple maps. Finally when it comes to printing or publishing, it is quite common to expect a different format  or a number of formats. The time and efforts required to address these challenges are not trivial but they tend to go unnoticed and under appreciated.  I'd like to automate this process as much as possible and reduce workload, therefore I created templates specifically to address the two broad scenarios below:   from an ArcGIS Map Document (mxd file) to an exported map of a given format (e.g. pdf file). This is identical to clicking the export button and specify some export parameters.  from a single Map Document to a series of maps. Typically, each iteration/ feature of a dataset will result in a separate map.   The underlying technical solution to the above two scenarios involves the  Yichuan10.ExportMXDtoMap  function, which is a thin, convenient wrapper around built-in map functions in the  arcpy.mapping  module.   The process usually involves the below steps:   pre-author a map (no need to turn on data driven pages)  ensure there is an index layer with id. They need to be specified and referenced in the map template (only applies to the second scenario).   One common task in the first scenario is to export a folder of MXD Map Documents, repeatedly with multiple formats. This could be easily achieved with a loop.   Here is an  example  to export from a Map Document to a thumbnail, a map for web, and a map for use in Adobe Illustrator (AI format, for further editing and publication).  The second scenario requires a loop to go through the feature class. Depending on its attributes or geometry, the map will then be different. For instance, difference can reflect in the title, extent indicator, spatial reference or on/off of a particular layer. For example, a scenario may be to create a map per species or per protected areas. In this case, the task is less flexible and needs to be adapted to account for unique requirements that cannot be fully captured in a single, reusable template.  For example, the  simple map batcher  sets a definition query, a spatial reference, scale/extent, and then exports a jpg format map with a select resolution. The snippet for the above steps is listed below  exportpng = exportfolder + os.sep + str(each) + '.jpg'\n\nquery = '\\ wdpaid\\  = ' + str(each)\n\n# only the above wdpaid feature visible\nlayer_index.definitionQuery = query\n\n# set dataframe coordinate system\nsr =arcpy.SpatialReference()\n\nsr_string = Yichuan10.GetFieldValueByID_mk2(layer_index, each, value_field='utm')\n\nsr.loadFromString(sr_string)\n\n# load from its attribute\ndf.spatialReference = sr\n\ndf.extent = layer_index.getExtent()\ndf.scale = df.scale * 1.1\n\n# need to specify a low quality\narcpy.mapping.ExportToJPEG(mxd, exportpng,  PAGE_LAYOUT , resolution = reso, jpeg_quality=60)  N.B. at the end of each iteration, it is good practice to clear the definition query by assigning its value to 'empty' (e.g. in the above case,  layer_index.definitionQuery = '' )  Finally, it is not necessary to adapt the map batch template to automate the production of maps. ArcGIS has a built-in tool call  data drive pages  or  Map series  in ArcPro. Their use is beyond the scope of this handover to discuss. In my opinion, data drive pages do not give a fine control in terms of iteration, nor allow the level of customisation that I required for my map production process.", 
            "title": "Map batching template"
        }, 
        {
            "location": "/geoprocessing/#richness-template", 
            "text": "The origin of this template goes back to 2010 when I was tasked with calculating species richness (counting number of species) using the global hexagon grid. This forms part of the analysis published in the paper titled  The Impact of Conservation on the Status of the World\u2019s Vertebrates . I re-wrote and improved in Python from the original code written in VBA (by Nick?).  The aim of the template is to simply count the number of overlaying species in each 'unit' for all units in the base layer, be it hexagons or protected areas. While conceptually simple, the implementation may not be straightforward. The beauty of this implementation in my opinion is two fold:   only perform an intersection test  minimal information is recorded   This means it does not require computationally intensive calculations of the actual intersections (for those who use PostGIS, think of  st_intersects  rather than  st_intersection , see  PostGIS documentation ), and it only records IDs, minimal information needed for subsequent analyses.  The current implementation relies on iterative  arcpy.SelectLayerByLocation_management  calls between a species layer and a base layer. For efficiency reasons, the species layer is created within each iteration by  arcpy.MakeFeatureLayer_management  with a given ID, and deleted after use; in contrast, the base layer is however re-used and resides in the memory persistently, until all iterations are complete. After the base layer is successfully 'selected', a data cursor loops through its records (when  GetUniqueValuesFromFeatureLayer_mk2  works on a feature layer with a selection, it will only work on the selected features) and saved as a list of ID strings (each representing a species and a base layer). It is formatted for eventual export to a csv file.  def species_richness_calculation(id, hexagonLyr):\n\n    # make species layer\n    if type(id) in [str, unicode]:\n        exp = '\\ ' + speciesID + '\\  = ' + '\\'' + str(id) + '\\''\n    elif type(id) in [int, float]:\n        exp = '\\ ' + speciesID + '\\  = ' + str(id)\n    else:\n        raise Exception('ID field type error')\n\n    # make layers\n    arcpy.MakeFeatureLayer_management(speciesData, speciesLyr, exp)\n\n    # select by locations\n    arcpy.SelectLayerByLocation_management(hexagonLyr, overLapOption, speciesLyr)\n\n    # record it\n    hex_ids = GetUniqueValuesFromFeatureLayer_mk2(hexagonLyr, hexagonID)\n\n    result = list()\n\n    for hex_id in hex_ids:\n        result.append(str(int(id)) + ',' + str(hex_id) + '\\n')\n\n    # get rid of layers\n    arcpy.Delete_management(speciesLyr)\n\n    return result  More recently, this template has been improved to utilise parallel processing to reduce time. ArcGIS cannot use more than one core at a time, so the challenge dictates dividing the task into smaller chunks for multiple ArcGIS processes, and then finally stitch together. The first parallel solution simply uses the  parallel template , but later solutions incorporate further optimisations in the template to improve efficiency and add additional functionality. More specifically, it involves:   Logger. Analysis with large, multi-source spatial data almost inevitably fail. Therefore it is often necessary to record failing features, investigate and then find specific solutions.  Move the making layer logic to the worker. Making layer is expensive (time-consuming) - it is much faster for each worker process to have a layer that they could readily re-use, without having to re-create for each iteration. It's a compromise between keeping task-specific logic from the multi-processing template and achieving better efficiency.   Note the difference between the original worker function (below)  def worker(q, q_out):\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print 'Remaining jobs:', q.qsize()\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = job(job_id)\n        q_out.put(result)  and the adapted worker function. Note the position of  arcpy.MakeFeatureLayer_management  and  arcpy.Delete_management , which would otherwise occur in the  species_richness_calculation  function.  def worker(q, q_out):\n    # make layer here to reduce overhead\n    arcpy.MakeFeatureLayer_management(hexagonData, hexagonLyr)\n\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print('Time:', datetime.datetime.now().time())\n            print('Remaining jobs:', q.qsize())\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = species_richness_calculation(job_id, hexagonLyr)\n        q_out.put(result)\n\n    arcpy.Delete_management(hexagonLyr)  The reduction in the time needed to undertake species richness analysis is significant - not quite linear reduction with the increase of CPU cores, but for my computer of 6 cores (12 threads) when using 10 worker process, it sees 7-8 times improvement. For example, the annual richness calculation itself for nomination + existing WH sites takes less than 3 hours, a previously unthinkable performance.  Furthermore, due to its generic use case, attempts are made to facilitate a uniform overlapping/binning process, with promising potentials.  For example, by running both the richness calculation between WDPA and the global hexagon grid, and between RedList and the global hexagon grid, the intersection between WDPA and RedList could be easily obtained by using the grid as a look up table. This eliminates the need for complicated spatial overlays. Any further calculations have now reduced to non-spatial tabulation/pivot tables.   The added benefits also include deferring decisions on groupings (for example, threatened species, or country/regional level statistics) towards a later time without re-doing the spatial overlay. This requires analysis be designed in the lowest possible granularity to enable later separation.   For example, if a decision to select species range based on presence, origin and seasonality is to be deferred, the analysis must use the row id (usual  fid ) instead of the species id  id_no . This is because  id_no  will make no effort to separate between polygons with presence 1 or presence 2 - it only differentiates species  The  biodiversity for food and agriculture analysis/BFA FAO analysis  is a good example with this thinking in mind (Here is  the script )", 
            "title": "Richness template"
        }, 
        {
            "location": "/geoprocessing/#parallel-template", 
            "text": "The idea for this template comes from the need to speed up slow-running processes. In the old days, the richness calculation can be painstakingly fiddly and slow: cutting the tasks to smaller chunks to run over a number of machines over a number of days is not only a unpleasant experience nor a sustainable solution (and error prone!). Even with a small subset of protected areas, running richness for all Red List species remains a challenge.   One of the bottlenecks of performance is that ArcGIS Desktop uses one core at a time - it does not take advantage of the full computing capacity of modern multi-core computer architecture. The parallel template was then written as a way of using Python to deploy multiple ArcGIS processes to distribute workload automatically.  The process goes like this:   Create an input pipe containing a list of unique IDs. Each ID represents a piece of data  Create an initially empty output pipe to host result (will come from the worker processes)  Create a number of worker processes, that actively get data from IDs in the input pipe  Create a worker process, that listens actively for incoming results from the output pipe and then processes them, for example write to an output file  The main function manages the pipes: distributes work, logs and waits for completion   This is the link to the template  Notably, it needs to add the text flag 'STOP' (or for that matter any flag would do as long as it is the same as what the worker process would watch out for) to the input queue.  # Add queue of a list of ids to process\nq = get_queue()\n\n# setup and run worker processes\np_workers = list()\nfor i in range(WORKER):\n    print 'Starting worker process:', i\n    p = multiprocessing.Process(target=worker, args=(q, q_out))\n    p_workers.append(p)\n\n# start\nfor p in p_workers:\n    p.start()\n\n# add stop flag to the queue\nfor p in p_workers:\n    q.put('STOP')  The worker process remains alive until it sees the 'STOP' signal to terminate.  def worker(q, q_out):\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print 'Remaining jobs:', q.qsize()\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = job(job_id)\n        q_out.put(result)", 
            "title": "Parallel template"
        }, 
        {
            "location": "/geoprocessing/#new-data-analysis-process", 
            "text": "(It is not really 'new' - but I still have not seen a wide adoption of this novel process being applied in analyses at IUCN or WCMC)  This is an improved work flow that I find myself increasingly using. Typically it begins with a spatial analysis, but only once. All subsequent analyses and visualisation then take place within a  Jupyter notebook . For routine analyses, this process seems to work very well, as long as the initial spatial analysis is designed in such a way (with the smallest granularity) that allow further interrogation of data with non-spatial analyses.  The benefits of this process:   less error prone as a result from spatial analysis. Spatial analysis is done only once at the smallest granularity  further questions of different aggregations can be answered with a non-spatial analysis (as long as the scale is higher than what's used in the spatial analysis)  methodology is fully open, and reproducible with little effort (apart from the initial spatial analysis)  the entire analytical process: data cleaning, analysis, and visualisation is fully documented and can easily be shared   Example analyses:   Climate change vulnerability analysis  and the  writing of report  World Heritage Outlook 2 analysis  Biodiversity for food and agriculture  Wilderness analysis  and the  writing of methodology  ICCA species richness", 
            "title": "New data analysis process"
        }, 
        {
            "location": "/geoprocessing/#arcgis-and-anaconda", 
            "text": "Of course, there is no reason why the spatial analysis should be excluded from the above process. Thus, the entire work flow of reading data, performing analysis and exporting or visualising results could be theoretically included in a single workspace such as a Jupyter notebook.   The main challenge is for the Python and its libraries to talk to ArcGIS and vice versa. The problem is that ArcGIS uses a specific version of Python and its numerical libraries, which cannot be updated without breaking the ArcGIS installation (when using  arcpy  for example).  Anaconda , a popular Python data science platform, can be used to create an environment to satisfy a specific installation of ArcGIS with compatible versions of other libraries.  USGS has a useful guide on  how to use anaconda modules from the esri Python environment  to set up such a environment.  Lastly, this may work for small spatial analyses, however, I would argue for the geoprocessing I require (takes hours or even days) this may not be a good idea. Such analyses tend to fail a couple of times and require significant back and forth experimenting and fixing of data manually - these would be better off dealt with separately.  I adapted a  customised script  (authored by  Curtis Price ) for my system to connect ArcGIS and Anaconda", 
            "title": "ArcGIS and Anaconda"
        }, 
        {
            "location": "/geoprocessing/#miscellaneous", 
            "text": "(TBA)", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/", 
            "text": "Idea\n\n\nLet's face it, research and analytical outputs are generally not interesting - at least not so in their original form. The scientifically accurate language appeals only to very few people. To most of us, reading a thick reasoning paper with lots of numbers poses an intellectual overhead that is not always pleasant and welcome, especially when you are busy. This is true in my own experience - I often argue that if I as an author cannot be 'bothered' to read my own works of 70 pages (I won't), how can I convince those who do not have a vested interest to read? \n\n\nThis needs to change, if our work is to reach more people, make it easy, and reap more impact. \n\n\nFortunately the problem is not uniquely ours - in fact it is a well trodden issue and there are good solutions. I would argue that in order to capture those with very limited attention span we need a new media, and going digital by default is one way to make the information more accessible and perhaps, by doing so, more interesting. For example, the \nUK government digital service\n offers some practical suggestions.\n\n\nThe knowledge lab, or the World Heritage Analysis which came to be known afterwards, represents my effort to put this thinking into action, to deliver the tasks I was leading under the Brighter Outlook project.\n\n\nHere are a few links on the thinking:\n\n\n\n\nWorld Heritage Analysis page on IUCN\n\n\nGitHub\n\n\n\n\nPlatform\n\n\nWhile far from being complete, the World Heritage Analyses (hereafter refer to as the Lab) is not just a portal with links directing to each product or prototype - it is designed to be the central place to foster a mechanism that allows ideas to be prototyped, tested, scrutinised and, as eventually a platform to raise funds for their eventual departure to become fully fledged product or services.\n\n\nAs it stands, it lacks the central pull factor to bring in more traffic and potential interest. This may be due to the lack of a usable feedback system, a limited scope that appeals to only a small audience, and possibly also lacking effective promotion. \n\n\nMost of the traffic come from new users, and a large majority visit the \ndatasheets\n, a well established product before its rebirth as an online service\n\n\n\n\nCurrent implementation\n\n\nIt is written in pure HTML and CSS, and currently hosted by GitHub pages.\n\n\nFull documentation of the World Heritage Analysis can be found on the GitHub below.\n\n\n\n\nGitHub repository for the World Heritage Analysis\n\n\n\n\nLinks to prototypes\n\n\n\n\n\n\nLand cover change website\n | \nSource on Github\n\n\n\n\n\n\nForest Loss\n | \nSource on Github\n\n\n\n\n\n\nHuman Footprint Change\n | \nSource on Github\n\n\n\n\n\n\nClimate change vulnerability\n | \nmethodology\n and \nreport\n | \nSource on Github\n\n\n\n\n\n\nLandsat 8 imagery for natural World Heritage\n | \nSource on Github\n\n\n\n\n\n\nNatural World Heritage Viewer\n | \nesri Feature Service\n | \nREST end point\n\n\n\n\n\n\nComparative analysis\n | \nSource on Github\n\n\n\n\n\n\nWCMC datasheet/Information sheet\n | \nSource on Github\n\n\n\n\n\n\nGlobal surface water\n | \nSource on Github", 
            "title": "World Heritage Analyses"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#idea", 
            "text": "Let's face it, research and analytical outputs are generally not interesting - at least not so in their original form. The scientifically accurate language appeals only to very few people. To most of us, reading a thick reasoning paper with lots of numbers poses an intellectual overhead that is not always pleasant and welcome, especially when you are busy. This is true in my own experience - I often argue that if I as an author cannot be 'bothered' to read my own works of 70 pages (I won't), how can I convince those who do not have a vested interest to read?   This needs to change, if our work is to reach more people, make it easy, and reap more impact.   Fortunately the problem is not uniquely ours - in fact it is a well trodden issue and there are good solutions. I would argue that in order to capture those with very limited attention span we need a new media, and going digital by default is one way to make the information more accessible and perhaps, by doing so, more interesting. For example, the  UK government digital service  offers some practical suggestions.  The knowledge lab, or the World Heritage Analysis which came to be known afterwards, represents my effort to put this thinking into action, to deliver the tasks I was leading under the Brighter Outlook project.  Here are a few links on the thinking:   World Heritage Analysis page on IUCN  GitHub", 
            "title": "Idea"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#platform", 
            "text": "While far from being complete, the World Heritage Analyses (hereafter refer to as the Lab) is not just a portal with links directing to each product or prototype - it is designed to be the central place to foster a mechanism that allows ideas to be prototyped, tested, scrutinised and, as eventually a platform to raise funds for their eventual departure to become fully fledged product or services.  As it stands, it lacks the central pull factor to bring in more traffic and potential interest. This may be due to the lack of a usable feedback system, a limited scope that appeals to only a small audience, and possibly also lacking effective promotion.   Most of the traffic come from new users, and a large majority visit the  datasheets , a well established product before its rebirth as an online service", 
            "title": "Platform"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#current-implementation", 
            "text": "It is written in pure HTML and CSS, and currently hosted by GitHub pages.  Full documentation of the World Heritage Analysis can be found on the GitHub below.   GitHub repository for the World Heritage Analysis", 
            "title": "Current implementation"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#links-to-prototypes", 
            "text": "Land cover change website  |  Source on Github    Forest Loss  |  Source on Github    Human Footprint Change  |  Source on Github    Climate change vulnerability  |  methodology  and  report  |  Source on Github    Landsat 8 imagery for natural World Heritage  |  Source on Github    Natural World Heritage Viewer  |  esri Feature Service  |  REST end point    Comparative analysis  |  Source on Github    WCMC datasheet/Information sheet  |  Source on Github    Global surface water  |  Source on Github", 
            "title": "Links to prototypes"
        }, 
        {
            "location": "/world-heritage-outlook/", 
            "text": "Technical architecture\n\n\nThe assessment module was written in Java, a custom-made system that accepts assessments via the web interface and store in a PostgreSQL database. The user facing front end website displays results of assessment and was implemented in Liferay, which during the second development changed to a Drupal based system (written in PHP).\n\n\nAt the moment, there is a disconnection between the two systems: backend assessment and frontend display. The assessment module (backend) and the old website (frontend display) are on one Amazon EC2 instance (instance name: 'IUCN WHP WHO') and the new, updated website resides another (instance name: 'IUCN WHO V2'). The new website used a database dump to migrate content from the old system. By design, the pipeline built by EDW does not reverse the flow of data in the other direction.\n\n\nThe source codes for the systems can be found on \nGitHub\n\n\nDatabase dump\n\n\nOne of the key functions of the system(s) is to enable the extraction and analysis of information for the World Heritage Outlook report. For that purpose, it is important to obtain a \ncopy\n of the dataset of the live database (the one behind the website, but \nnever\n work directly on the live database itself!). This is called a database dump.\n\n\nFor security reasons, you will need to SSH tunnel to the instance and forward the remote port to a local one. There is a saved session called \nWHO_backend\n that contains necessary connection parameters, including those to forward the remote 5432 port (the database port) to the local 5433 port. \n\n\n\n\nOnce you've successfully logged using that saved session, open PgAdmin, the database management tool to connect to your local port 5433, which now points to the remote machine via an SSH tunnel. \n\n\n\n\nYou may now select the database from which you'd like to make a dump. Use the 'backup' button (see red arrow) to create a file which holds the content of the database.\n\n\n\n\nOn your local machine, create an empty new database and then 'restore' (see the blue arrow) to create an identical copy of the remote database. \n\n\nThis concludes the database dump or 'extraction of data'.\n\n\nConvert to Access database\n\n\nUnfortunately the data behind the website is not designed in a format that allows users to gain useful insight about the assessments directly - after all the system has been designed to power the I/O of assessments. As a result, the database dump contains many system tables that do not hold assessment information, and that this information is fragmented and split into many smaller tables. You may refer to the diagram to better understand the relationships of these tables.\n\n\n\n\nE:\\Yichuan\\IT\\ER_diagram_130802\n\n\n\n\nFor the most commonly used queries, I have built a re-usable Microsoft Access database that connects to the PostgreSQL database. These links must be updated each time a new database dump is made (Matea knows how to do this). By using the Access database as an intermediate, it allows the export to excel spreadsheets, which some find easier to work with. \n\n\nBelow is an example of the 'overall' view in the Access database.\n\n\n\n\nE:\\Yichuan\\Elena\\WHOA_171027\n\n\n\n\n\n\n!IMPORTANT!\n \n\n\nFor the data dump, however, there is one manual step that needs to be performed to assist the analysis before updating the links. \n\n\nWe need to differentiate the most recent assessment ids, and separate them in the two assessment cycles. This is essential to pull out the correct versions of the assessments for analysis. This step can be done in Access but would be much easier in the PostgreSQL database. In your newly restored database, create a view called 'z_wdpa_latest' using the below:\n\n\n-- View: z_wdpaid_latest\n-- DROP VIEW z_wdpaid_latest;\n\nCREATE OR REPLACE VIEW z_wdpaid_latest AS \n WITH a AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2014'::text\n        ), b AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2017'::text\n        ), combined AS (\n                 SELECT a.wdpa_id, a.assessment_cycle, a.assessment_version_id\n                   FROM a\n                  WHERE a.version_code = a.max_code\n        UNION \n                 SELECT b.wdpa_id, b.assessment_cycle, b.assessment_version_id\n                   FROM b\n                  WHERE b.version_code = b.max_code\n        )\n SELECT combined.wdpa_id, combined.assessment_cycle, max(combined.assessment_version_id) AS assessment_version_id\n   FROM combined\n  GROUP BY combined.wdpa_id, combined.assessment_cycle\n  ORDER BY combined.wdpa_id, combined.assessment_cycle;\n\nALTER TABLE z_wdpaid_latest\n  OWNER TO postgres;\n\n\n\n\nThe rationale is to identify the largest value of \nassessment_version_code\n for each cycle and for each \nassessment_id\n, on the assumption that the later/newer created id should be a larger number. By finding the largest value of the id, it is guaranteed that the latest versions are used, irrespective of the version number and stage, which can be wrong due to bugs in the original design (I shall omit the technical details, but in short the system allows but did not expect certain user behaviours that may result in incorrect version numbers and stages being created)\n\n\nOnce the above is done, you can now update the links in the Access database, using the 'Linked Table Manager' so that they point to the most recent local database as a source\n\n\n\n\nMake sure you tick 'Always prompt for new location', so that you update links for all tables in one go\n\n\n\n\nCreate a new data source, here you'll need to specify the details of the newly 'restored' database\n\n\n\n\nAnalysis\n\n\nPlease refer to past emails regarding the analysis.", 
            "title": "World Heritage Outlook"
        }, 
        {
            "location": "/world-heritage-outlook/#technical-architecture", 
            "text": "The assessment module was written in Java, a custom-made system that accepts assessments via the web interface and store in a PostgreSQL database. The user facing front end website displays results of assessment and was implemented in Liferay, which during the second development changed to a Drupal based system (written in PHP).  At the moment, there is a disconnection between the two systems: backend assessment and frontend display. The assessment module (backend) and the old website (frontend display) are on one Amazon EC2 instance (instance name: 'IUCN WHP WHO') and the new, updated website resides another (instance name: 'IUCN WHO V2'). The new website used a database dump to migrate content from the old system. By design, the pipeline built by EDW does not reverse the flow of data in the other direction.  The source codes for the systems can be found on  GitHub", 
            "title": "Technical architecture"
        }, 
        {
            "location": "/world-heritage-outlook/#database-dump", 
            "text": "One of the key functions of the system(s) is to enable the extraction and analysis of information for the World Heritage Outlook report. For that purpose, it is important to obtain a  copy  of the dataset of the live database (the one behind the website, but  never  work directly on the live database itself!). This is called a database dump.  For security reasons, you will need to SSH tunnel to the instance and forward the remote port to a local one. There is a saved session called  WHO_backend  that contains necessary connection parameters, including those to forward the remote 5432 port (the database port) to the local 5433 port.    Once you've successfully logged using that saved session, open PgAdmin, the database management tool to connect to your local port 5433, which now points to the remote machine via an SSH tunnel.    You may now select the database from which you'd like to make a dump. Use the 'backup' button (see red arrow) to create a file which holds the content of the database.   On your local machine, create an empty new database and then 'restore' (see the blue arrow) to create an identical copy of the remote database.   This concludes the database dump or 'extraction of data'.", 
            "title": "Database dump"
        }, 
        {
            "location": "/world-heritage-outlook/#convert-to-access-database", 
            "text": "Unfortunately the data behind the website is not designed in a format that allows users to gain useful insight about the assessments directly - after all the system has been designed to power the I/O of assessments. As a result, the database dump contains many system tables that do not hold assessment information, and that this information is fragmented and split into many smaller tables. You may refer to the diagram to better understand the relationships of these tables.   E:\\Yichuan\\IT\\ER_diagram_130802   For the most commonly used queries, I have built a re-usable Microsoft Access database that connects to the PostgreSQL database. These links must be updated each time a new database dump is made (Matea knows how to do this). By using the Access database as an intermediate, it allows the export to excel spreadsheets, which some find easier to work with.   Below is an example of the 'overall' view in the Access database.   E:\\Yichuan\\Elena\\WHOA_171027    !IMPORTANT!    For the data dump, however, there is one manual step that needs to be performed to assist the analysis before updating the links.   We need to differentiate the most recent assessment ids, and separate them in the two assessment cycles. This is essential to pull out the correct versions of the assessments for analysis. This step can be done in Access but would be much easier in the PostgreSQL database. In your newly restored database, create a view called 'z_wdpa_latest' using the below:  -- View: z_wdpaid_latest\n-- DROP VIEW z_wdpaid_latest;\n\nCREATE OR REPLACE VIEW z_wdpaid_latest AS \n WITH a AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2014'::text\n        ), b AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2017'::text\n        ), combined AS (\n                 SELECT a.wdpa_id, a.assessment_cycle, a.assessment_version_id\n                   FROM a\n                  WHERE a.version_code = a.max_code\n        UNION \n                 SELECT b.wdpa_id, b.assessment_cycle, b.assessment_version_id\n                   FROM b\n                  WHERE b.version_code = b.max_code\n        )\n SELECT combined.wdpa_id, combined.assessment_cycle, max(combined.assessment_version_id) AS assessment_version_id\n   FROM combined\n  GROUP BY combined.wdpa_id, combined.assessment_cycle\n  ORDER BY combined.wdpa_id, combined.assessment_cycle;\n\nALTER TABLE z_wdpaid_latest\n  OWNER TO postgres;  The rationale is to identify the largest value of  assessment_version_code  for each cycle and for each  assessment_id , on the assumption that the later/newer created id should be a larger number. By finding the largest value of the id, it is guaranteed that the latest versions are used, irrespective of the version number and stage, which can be wrong due to bugs in the original design (I shall omit the technical details, but in short the system allows but did not expect certain user behaviours that may result in incorrect version numbers and stages being created)  Once the above is done, you can now update the links in the Access database, using the 'Linked Table Manager' so that they point to the most recent local database as a source   Make sure you tick 'Always prompt for new location', so that you update links for all tables in one go   Create a new data source, here you'll need to specify the details of the newly 'restored' database", 
            "title": "Convert to Access database"
        }, 
        {
            "location": "/world-heritage-outlook/#analysis", 
            "text": "Please refer to past emails regarding the analysis.", 
            "title": "Analysis"
        }, 
        {
            "location": "/datasheet/", 
            "text": "Introduction\n\n\nThroughout the past four decades UNEP-WCMC has inherited \na legacy of datasheets\n, records that cover consistently a wide variety of topics for each and every natural World Heritage site. While still available in the new website as \ninformation sheets\n, such valuable information is fragmented, and their usefulness has been severely limited by the lack of access.\n\n\n\n\nIf we make the datasheets hard to find, people will not use them\n\n\n\n\nThe overhaul of datasheets has been ongoing since I joined the World Heritage Programme in 2011. However, it is not until late 2017, with the help of many interns, this work finally concludes as a prototype, and made accessible online. \n\n\nChallenge\n\n\nThe two distinct challenges are to 1) improve accessibility by publishing the datasheets online, preferably with good search capabilities; and 2) convert the ageing word/pdf format to an easily maintainable and publishable format.\n\n\nThe datasheets in the word format compromise more than decades of editing and evolving styles. They are inherently inconsistent, to say the very least. Stripping off formatting and then structuring them is no small task. \n\n\nPresentation\n\n\nFrom a user point of view, I feel the traditional dynamic website may be an over-engineered option. The content should remain largely unchanged throughout the year and that interactivity is not essential. In fact, all pages can be cached. With this in mind, I decided the system to serve datasheets shall have no database, no web server and thus static. \n\n\nThis thinking bears resemblance of blogging systems, in which articles are written in a pure text format and the web pages are generated using a \nstatic site generator\n. It offers simplicity, superb scalability and above all speed. \n\n\nThe challenge is to somehow adapt the tool to generalise the specific use case of blog to more generic data. I used \nPelican\n, a Python powered static site generator, for this purpose. \nThis repository\n illustrates how I did it.\n\n\nContent\n\n\nConverting and cleaning the content needs a sustainable solution. This is because UNEP-WCMC updates datasheets annually, either adding new sites or modifying those with significant changes (for example, extension). The process will likely run at least once a year.\n\n\nHere are the steps:\n\n\n\n\nUse \nPandoc\n to batch convert word format to markdown format (1st batch)\n\n\nProof read the converted markdown format to check for systematic issues and unique problems within individual datasheets\n\n\nProgrammatically fix systematic issues in the markdown format (notably using regular expressions) and on the spot fixes in the original word format if deemed one off and easier\n\n\nProgrammatically extract information in the markdown format and append them at the beginning for the final export of datasheets in markdown format (2nd batch). This is required by the Pelican site generator.\n\n\n\n\nSee \nthis Github repository\n for more details.", 
            "title": "Datasheet"
        }, 
        {
            "location": "/datasheet/#introduction", 
            "text": "Throughout the past four decades UNEP-WCMC has inherited  a legacy of datasheets , records that cover consistently a wide variety of topics for each and every natural World Heritage site. While still available in the new website as  information sheets , such valuable information is fragmented, and their usefulness has been severely limited by the lack of access.   If we make the datasheets hard to find, people will not use them   The overhaul of datasheets has been ongoing since I joined the World Heritage Programme in 2011. However, it is not until late 2017, with the help of many interns, this work finally concludes as a prototype, and made accessible online.", 
            "title": "Introduction"
        }, 
        {
            "location": "/datasheet/#challenge", 
            "text": "The two distinct challenges are to 1) improve accessibility by publishing the datasheets online, preferably with good search capabilities; and 2) convert the ageing word/pdf format to an easily maintainable and publishable format.  The datasheets in the word format compromise more than decades of editing and evolving styles. They are inherently inconsistent, to say the very least. Stripping off formatting and then structuring them is no small task.", 
            "title": "Challenge"
        }, 
        {
            "location": "/datasheet/#presentation", 
            "text": "From a user point of view, I feel the traditional dynamic website may be an over-engineered option. The content should remain largely unchanged throughout the year and that interactivity is not essential. In fact, all pages can be cached. With this in mind, I decided the system to serve datasheets shall have no database, no web server and thus static.   This thinking bears resemblance of blogging systems, in which articles are written in a pure text format and the web pages are generated using a  static site generator . It offers simplicity, superb scalability and above all speed.   The challenge is to somehow adapt the tool to generalise the specific use case of blog to more generic data. I used  Pelican , a Python powered static site generator, for this purpose.  This repository  illustrates how I did it.", 
            "title": "Presentation"
        }, 
        {
            "location": "/datasheet/#content", 
            "text": "Converting and cleaning the content needs a sustainable solution. This is because UNEP-WCMC updates datasheets annually, either adding new sites or modifying those with significant changes (for example, extension). The process will likely run at least once a year.  Here are the steps:   Use  Pandoc  to batch convert word format to markdown format (1st batch)  Proof read the converted markdown format to check for systematic issues and unique problems within individual datasheets  Programmatically fix systematic issues in the markdown format (notably using regular expressions) and on the spot fixes in the original word format if deemed one off and easier  Programmatically extract information in the markdown format and append them at the beginning for the final export of datasheets in markdown format (2nd batch). This is required by the Pelican site generator.   See  this Github repository  for more details.", 
            "title": "Content"
        }, 
        {
            "location": "/presentation/", 
            "text": "Prior to 2016, all presentations are stored on the workstation\n\n\n\n\nE:\\Yichuan\\Yichuan\\presentations\n\n\n\n\nNew format presentations have been used since late 2016. They can be found online:\n\n\n\n\nNew format presentations", 
            "title": "Presentation"
        }, 
        {
            "location": "/folder-structure/", 
            "text": "The content is located at \nE:\\Yichuan\n\n\nMost of my content is organised into folders with the names of collaborators or those who requested help. This is the highest level. Within this level, project files can be found if this is a one-off request; or folders, each representing a single project. \n\n\nFolders named \nnot after\n people usually refer to one-off/non-specific, large or recurring projects. Below is a concise explanation of important folders, or those needs clarification:\n\n\nAdmin: \n\n\n\n\nall admins, including timesheets, travel authorisations, visas, tickets, cost reclaims and etc\n\n\n\n\nBasedata: \n\n\n\n\nmost base data are located here (spatial). Typically they include base physical, cultural boundaries, grids, hillshade, ecoregions etc. Recommendation: \n\n\nrecommend using esri ArcCatalog to navigate\n\n\n\n\nCOM_XXX: \n\n\n\n\npreparations for World Heritage Committee meetings\n\n\n\n\nComparative_analysis_20XX: \n\n\n\n\nresults of comparative analysis of that year. They will generally include digitised polygons, results of the analysis in tables, and also maps (including map templates that create them)\n\n\n\n\nDggrid: \n\n\n\n\ndiscrete global hexagon grids program\n\n\n\n\nDocuments: \n\n\n\n\nunsorted \ndocx\n or \npdf\n format documents prior to 2014\n\n\n\n\nDump_PG: \n\n\n\n\nunsorted database dump prior to 2014\n\n\n\n\nElena: \n\n\n\n\nWorld Heritage Outlook data dumps and analysis files are located\n\n\n\n\nExperiment: \n\n\n\n\ntest space for various tools, analysis and scripts, unsorted (do not use)\n\n\n\n\nGood_reference_map_collection: \n\n\n\n\na collection of maps for reference\n\n\n\n\nIT: \n\n\n\n\nproject folder for the first World Heritage Outlook system development \n\n\n\n\nLandsat_archiving: \n\n\n\n\npython based script for the archiving project, superseded by the \nnear real time Landsat 8 images\n in the Lab. (do not use)\n\n\n\n\nMap_templates: \n\n\n\n\noutdated template (do not use)\n\n\n\n\nMapmart: \n\n\n\n\n0.5m resolution WorldView imagery purchased to identify mining activities, in Russia\n\n\n\n\nMyGDB.gdb: \n\n\n\n\na somewhat historic/duplicate place holder for base data, but in esri geodatabase format. It mainly contains biogeographic, broad scale priorities and old site level priorities such as KBA datasets. It also includes world vector shore line data (including EEZs). While I do not recommend using any of the datasets, do not delete this database, as there may be map documents referring to the data here-within \n\n\n\n\nMyWorkplace.gdb: \n\n\n\n\nsimilar to 'Experiment' folder but in geodatabase format\n\n\n\n\nPapers: \n\n\n\n\nfeeder folder for Mendeley, the reference management system for papers\n\n\n\n\nRed_List_data: \n\n\n\n\nraw Red List data releases from 2013\n\n\n\n\nRemote sensing: \n\n\n\n\ntest data for raw satellite images (mostly Landsat) collected throughout my time for ad-hoc projects\n\n\n\n\nScripts: \n\n\n\n\narguably the most important folder. Sub folder structure below\n\n\n\n\n\n\n\n\ngeoprocessing: \nscripts and libraries for undertaking spatial analysis, generic data analysis, workflow automation etc. See the \ngeoprocessing\n section for more information. This is also backed up on \nGitHub\n\n\n\n\n\n\nscripts: \nhistoric place holder for scripts. Most one off, ad-hoc scripts are here.\n\n\n\n\n\n\nmysql20110516: \noutdated SQL scripts for maintaining the PostgreSQL database\n\n\n\n\n\n\nsites_improve_geometry_XXX: \n\n\n\n\nseries of attempts to improve the qualities of WH datasets until 2015\n\n\n\n\nsitesXXXX: \n\n\n\n\ndigitised boundaries for new nominations, and successful inscriptions and/or modifications. \n\n\n\n\nTentativeList: \n\n\n\n\noutdated tentative list sites improvement (do not use)\n\n\n\n\nUpdates: \n\n\n\n\noutdated map document to update the database and no longer relevant (do not use)\n\n\n\n\nWDPA: \n\n\n\n\ncopies of WDPA monthly releases for analysis\n\n\n\n\nWH_benefits: \n\n\n\n\nproject folder for the first WH benefits analysis\n\n\n\n\nWH_stats: \n\n\n\n\nstatistics after COM\n\n\n\n\nWHO_geom_updates: \n\n\n\n\nmethodology for updating the maps in the old WH Outlook system. This may not be relevant any more\n\n\n\n\nWHS.gdb: \n\n\n\n\nspatial data for natural and mixed World Heritage sites, in geodatabase format\n\n\n\n\nWHS_arcgisonlineXXX.gdb: \n\n\n\n\nspatial data to be zipped and uploaded to the arcgisonline. This is the data behind the \nesri feature service\n\n\n\n\nWHS_dump_ATTR: \n\n\n\n\nattributes/non spatial data for natural and mixed World Heritage sites\n\n\n\n\nWHS_dump_KML: \n\n\n\n\nhistorical dumps of WH data in KML format, superseded by the feature service\n\n\n\n\nWHS_dump_SHP: \n\n\n\n\nshapefile format of the WH data\n\n\n\n\nWHS_map_batcher: \n\n\n\n\nmap template to automate map production\n\n\n\n\nWHS_quality_check: \n\n\n\n\nmap comparisons between GIS, official maps from UNESCO, including retrospective ones. The numerous comparisons underpin and empower the systematic check and subsequently the improvement of the data.\n\n\n\n\nYichuan: \n\n\n\n\nmiscellaneous", 
            "title": "Folder structure"
        }
    ]
}