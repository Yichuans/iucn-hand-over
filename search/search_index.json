{
    "docs": [
        {
            "location": "/", 
            "text": "About\n\n\nThis is the last project I undertake at IUCN's World Heritage Programme. I intend to document to the best of my knowledge my work in past seven years that my be useful in the future, and also pass necessary information onto the team. Apart from the usual 'where to find things', these will include high-level introductions of past projects, datasets, tools and methodologies, and in some cases with in-depth explanations if such detail is deemed useful and has not been documented elsewhere.\n\n\n[INTERNAL USE ONLY!]\n \n\n\nI choose to use a web format to organise the handover content simply due to the fact that it does a better job in searching. This could be published more widely for easy accessibility, particularly for methodologies that may have a wider use, but I would discourage such use for matters of potential confidentiality.\n\n\nIn terms of content, below is a linked list. You may also use the \nsearch docs\n box to locate content using keywords.\n\n\n\n\nWorld Heritage GIS database. This includes, amongst others, the maintenance of the database, its updates, and statistics and export that depend on the database.\n\n\nComparative analysis. As contrary to the methodology, which has been published as a separate document, will focus on the technical side of the analysis in GIS, should future users require to replicate or improve the semi-automatic process utilised in recent years. \n\n\nWorld Heritage analyses initiative. \n\n\nWorld Heritage datasheets\n\n\nFolder structure on workstation\n\n\nWorld Heritage Outlook\n\n\nGeoprocessing methodologies\n\n\nPresentations\n\n\n\n\nI do not intend to duplicate what is being documented elsewhere in greater details for \nWorld Heritage Analysis\n, alongside their source codes on \nGitHub\n.\n\n\nAcknowledgement\n\n\nI would also like to take this opportunity to thank Tim and Naomi for the oversight and mentoring, and for making the secondment arrangement a success. Thanks also go to the two wonderful teams at IUCN and at UNEP-WCMC, and to many colleagues I have worked with in the long seven years. I hope to remain in touch with you all and wish you all the very best in 2018 and onwards.", 
            "title": "Home"
        }, 
        {
            "location": "/#about", 
            "text": "This is the last project I undertake at IUCN's World Heritage Programme. I intend to document to the best of my knowledge my work in past seven years that my be useful in the future, and also pass necessary information onto the team. Apart from the usual 'where to find things', these will include high-level introductions of past projects, datasets, tools and methodologies, and in some cases with in-depth explanations if such detail is deemed useful and has not been documented elsewhere.  [INTERNAL USE ONLY!]    I choose to use a web format to organise the handover content simply due to the fact that it does a better job in searching. This could be published more widely for easy accessibility, particularly for methodologies that may have a wider use, but I would discourage such use for matters of potential confidentiality.  In terms of content, below is a linked list. You may also use the  search docs  box to locate content using keywords.   World Heritage GIS database. This includes, amongst others, the maintenance of the database, its updates, and statistics and export that depend on the database.  Comparative analysis. As contrary to the methodology, which has been published as a separate document, will focus on the technical side of the analysis in GIS, should future users require to replicate or improve the semi-automatic process utilised in recent years.   World Heritage analyses initiative.   World Heritage datasheets  Folder structure on workstation  World Heritage Outlook  Geoprocessing methodologies  Presentations   I do not intend to duplicate what is being documented elsewhere in greater details for  World Heritage Analysis , alongside their source codes on  GitHub .", 
            "title": "About"
        }, 
        {
            "location": "/#acknowledgement", 
            "text": "I would also like to take this opportunity to thank Tim and Naomi for the oversight and mentoring, and for making the secondment arrangement a success. Thanks also go to the two wonderful teams at IUCN and at UNEP-WCMC, and to many colleagues I have worked with in the long seven years. I hope to remain in touch with you all and wish you all the very best in 2018 and onwards.", 
            "title": "Acknowledgement"
        }, 
        {
            "location": "/world-heritage-database/", 
            "text": "Introduction\n\n\nPostgreSQL technical database\n\n\nHow-to: update the database\n\n\nStatistics", 
            "title": "Database"
        }, 
        {
            "location": "/world-heritage-database/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/world-heritage-database/#postgresql-technical-database", 
            "text": "", 
            "title": "PostgreSQL technical database"
        }, 
        {
            "location": "/world-heritage-database/#how-to-update-the-database", 
            "text": "", 
            "title": "How-to: update the database"
        }, 
        {
            "location": "/world-heritage-database/#statistics", 
            "text": "", 
            "title": "Statistics"
        }, 
        {
            "location": "/comparative-analysis/", 
            "text": "Introduction\n\n\nGIS analysis\n\n\nBio-geographic classification, and priorities\n\n\nSpecies richness\n\n\nIrreplaceability\n\n\nTentative list sites\n\n\nExport to excel\n\n\nFull result export\n\n\nStatic maps\n\n\nInteractive maps", 
            "title": "Comparative analysis"
        }, 
        {
            "location": "/comparative-analysis/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/comparative-analysis/#gis-analysis", 
            "text": "", 
            "title": "GIS analysis"
        }, 
        {
            "location": "/comparative-analysis/#bio-geographic-classification-and-priorities", 
            "text": "", 
            "title": "Bio-geographic classification, and priorities"
        }, 
        {
            "location": "/comparative-analysis/#species-richness", 
            "text": "", 
            "title": "Species richness"
        }, 
        {
            "location": "/comparative-analysis/#irreplaceability", 
            "text": "", 
            "title": "Irreplaceability"
        }, 
        {
            "location": "/comparative-analysis/#tentative-list-sites", 
            "text": "", 
            "title": "Tentative list sites"
        }, 
        {
            "location": "/comparative-analysis/#export-to-excel", 
            "text": "", 
            "title": "Export to excel"
        }, 
        {
            "location": "/comparative-analysis/#full-result-export", 
            "text": "", 
            "title": "Full result export"
        }, 
        {
            "location": "/comparative-analysis/#static-maps", 
            "text": "", 
            "title": "Static maps"
        }, 
        {
            "location": "/comparative-analysis/#interactive-maps", 
            "text": "", 
            "title": "Interactive maps"
        }, 
        {
            "location": "/geoprocessing/", 
            "text": "Introduction\n\n\nThis section explains the \ntechnical\n side of my work, on analysis, maps and automation. \n\n\nThe reason why much of my time has gone into the development, improvement and maintenance of the scripts is simple - next time the same task is requested again, I have an existing method, or previously working solution, that could be re-used or adapted. This saves time and increases my productivity\n\n\nWherever possible, I try to automate. In some cases, this obviously represents a poor investment of time, i.e., doing it manually step by step may be quicker. Automation for one-off tasks is over-engineering. I would argue, however, any time spent on making a tool, or spent on mastering a potentially more productive tool is a just investment that brings benefits in the long term. \n\n\nFor example, the different implementations of the comparative analysis have led to a reduced workload significantly. See below a graph depicting the amount of days to do the spatial overlay between the plethora of datasets and new nominations. This frees time for species richness, irreplaceability, improved maps, just to name a few.\n\n\n\n\nSome of the tasks present similar technical challenges. Especially for frequent, recurring tasks, it is imperative that I develop libraries or templates to minimise repetitive work, and that more time is made available for creative tasks and increased productivity. \n\n\nGeoprocessing libraries\n\n\nMap batching template\n\n\nRichness template\n\n\nParallel template\n\n\nGeneric data analysis\n\n\nArcGIS and Anaconda\n\n\nMiscellaneous", 
            "title": "Geoprocessing"
        }, 
        {
            "location": "/geoprocessing/#introduction", 
            "text": "This section explains the  technical  side of my work, on analysis, maps and automation.   The reason why much of my time has gone into the development, improvement and maintenance of the scripts is simple - next time the same task is requested again, I have an existing method, or previously working solution, that could be re-used or adapted. This saves time and increases my productivity  Wherever possible, I try to automate. In some cases, this obviously represents a poor investment of time, i.e., doing it manually step by step may be quicker. Automation for one-off tasks is over-engineering. I would argue, however, any time spent on making a tool, or spent on mastering a potentially more productive tool is a just investment that brings benefits in the long term.   For example, the different implementations of the comparative analysis have led to a reduced workload significantly. See below a graph depicting the amount of days to do the spatial overlay between the plethora of datasets and new nominations. This frees time for species richness, irreplaceability, improved maps, just to name a few.   Some of the tasks present similar technical challenges. Especially for frequent, recurring tasks, it is imperative that I develop libraries or templates to minimise repetitive work, and that more time is made available for creative tasks and increased productivity.", 
            "title": "Introduction"
        }, 
        {
            "location": "/geoprocessing/#geoprocessing-libraries", 
            "text": "", 
            "title": "Geoprocessing libraries"
        }, 
        {
            "location": "/geoprocessing/#map-batching-template", 
            "text": "", 
            "title": "Map batching template"
        }, 
        {
            "location": "/geoprocessing/#richness-template", 
            "text": "", 
            "title": "Richness template"
        }, 
        {
            "location": "/geoprocessing/#parallel-template", 
            "text": "", 
            "title": "Parallel template"
        }, 
        {
            "location": "/geoprocessing/#generic-data-analysis", 
            "text": "", 
            "title": "Generic data analysis"
        }, 
        {
            "location": "/geoprocessing/#arcgis-and-anaconda", 
            "text": "", 
            "title": "ArcGIS and Anaconda"
        }, 
        {
            "location": "/geoprocessing/#miscellaneous", 
            "text": "", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/", 
            "text": "Idea\n\n\nLet's face it, research and analytical outputs are generally not interesting - at least not so in their original form. The scientifically accurate language appeals only to very few people. To most of us, reading a thick reasoning paper with lots of numbers poses an intellectual overhead that is not always pleasant and welcome, especially when you are busy. This is true in my own experience - I often argue that if I as an author cannot be 'bothered' to read my own works of 70 pages (I won't), how can I convince those who do not have a vested interest to read? \n\n\nThis needs to change, if our work is to reach more people, make it easy, and reap more impact. \n\n\nFortunately the problem is not uniquely ours - in fact it is a well trodden issue and there are good solutions. I would argue that in order to capture those with very limited attention span we need a new media, and going digital by default is one way to make the information more accessible and perhaps, by doing so, more interesting. For example, the \nUK government digital service\n offers some practical suggestions.\n\n\nThe knowledge lab, or the World Heritage Analysis which came to be known afterwards, represents my effort to put this thinking into action, to deliver the tasks I was leading under the Brighter Outlook project.\n\n\nHere are a few links on the thinking:\n\n\n\n\nWorld Heritage Analysis page on IUCN\n\n\nGitHub\n\n\n\n\nPlatform\n\n\nWhile far from being complete, the World Heritage Analysis (hereafter refer to as the Lab) is not just a portal website with links pointing to each product - it is designed to be the central place to foster a mechanism that allows ideas to be prototyped, tested, scrutinised and, as a platform to raise funds for their eventual departure to become fully fledged product or services.\n\n\nAs it stands, it lacks the central pull factor to bring in more traffic and thus potential interest. This may be due to the lack of a usable feedback system, a small scope that appeals to a small audience and possibly also lacking effective promotion. \n\n\nMost of the traffic come from new users, and a large majority visit the \ndatasheets\n, a well established product before its appearance as an online service\n\n\n\n\nCurrent implementation\n\n\nIt is written in pure HTML and CSS, and currently hosted by GitHub pages.\n\n\nFull documentation of the World Heritage Analysis can be found on the GitHub below.\n\n\n\n\nGitHub repository for the World Heritage Analysis\n\n\n\n\nLinks to prototypes\n\n\n\n\n\n\nLand cover change website\n | \nSource on Github\n\n\n\n\n\n\nForest Loss\n | \nSource on Github\n\n\n\n\n\n\nHuman Footprint Change\n | \nSource on Github\n\n\n\n\n\n\nClimate change vulnerability\n | \nmethodology\n and \nreport\n | \nSource on Github\n\n\n\n\n\n\nLandsat 8 imagery for natural World Heritage\n | \nSource on Github\n\n\n\n\n\n\nNatural World Heritage Viewer\n | \nesri Feature Service\n | \nREST end point\n\n\n\n\n\n\nComparative analysis\n | \nSource on Github\n\n\n\n\n\n\nInformation sheet\n | \nSource on Github\n\n\n\n\n\n\nGlobal surface water\n | \nSource on Github", 
            "title": "World Heritage Analyses"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#idea", 
            "text": "Let's face it, research and analytical outputs are generally not interesting - at least not so in their original form. The scientifically accurate language appeals only to very few people. To most of us, reading a thick reasoning paper with lots of numbers poses an intellectual overhead that is not always pleasant and welcome, especially when you are busy. This is true in my own experience - I often argue that if I as an author cannot be 'bothered' to read my own works of 70 pages (I won't), how can I convince those who do not have a vested interest to read?   This needs to change, if our work is to reach more people, make it easy, and reap more impact.   Fortunately the problem is not uniquely ours - in fact it is a well trodden issue and there are good solutions. I would argue that in order to capture those with very limited attention span we need a new media, and going digital by default is one way to make the information more accessible and perhaps, by doing so, more interesting. For example, the  UK government digital service  offers some practical suggestions.  The knowledge lab, or the World Heritage Analysis which came to be known afterwards, represents my effort to put this thinking into action, to deliver the tasks I was leading under the Brighter Outlook project.  Here are a few links on the thinking:   World Heritage Analysis page on IUCN  GitHub", 
            "title": "Idea"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#platform", 
            "text": "While far from being complete, the World Heritage Analysis (hereafter refer to as the Lab) is not just a portal website with links pointing to each product - it is designed to be the central place to foster a mechanism that allows ideas to be prototyped, tested, scrutinised and, as a platform to raise funds for their eventual departure to become fully fledged product or services.  As it stands, it lacks the central pull factor to bring in more traffic and thus potential interest. This may be due to the lack of a usable feedback system, a small scope that appeals to a small audience and possibly also lacking effective promotion.   Most of the traffic come from new users, and a large majority visit the  datasheets , a well established product before its appearance as an online service", 
            "title": "Platform"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#current-implementation", 
            "text": "It is written in pure HTML and CSS, and currently hosted by GitHub pages.  Full documentation of the World Heritage Analysis can be found on the GitHub below.   GitHub repository for the World Heritage Analysis", 
            "title": "Current implementation"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#links-to-prototypes", 
            "text": "Land cover change website  |  Source on Github    Forest Loss  |  Source on Github    Human Footprint Change  |  Source on Github    Climate change vulnerability  |  methodology  and  report  |  Source on Github    Landsat 8 imagery for natural World Heritage  |  Source on Github    Natural World Heritage Viewer  |  esri Feature Service  |  REST end point    Comparative analysis  |  Source on Github    Information sheet  |  Source on Github    Global surface water  |  Source on Github", 
            "title": "Links to prototypes"
        }, 
        {
            "location": "/world-heritage-outlook/", 
            "text": "History\n\n\nI built the first prototype of the Outlook assessment module in Python (Web2Py framework), as proof of concept. The underlying principle is to store the results of assessment in a database so that it could be managed and analysed more easily and effectively. A database approach is the only way to systematically manage the data the project generates.\n\n\nFor the development, RSMI was commissioned to undertake the design, implementation and maintenance of the three modules: assessment, site information, and front-end website for showcasing findings. In hindsight, it was an unrealistic ambition with impossible funds, but at the time the can-do spirit prevailed and I was technically in-mature and inexperienced as to make do with developing everything, rather than reality check and focusing on the minimally viable product.\n\n\nIn hindsight, part of the reason was that the roles were never clear, I think I was given the role of project management, yet I had no power in making design choices nor allocate funds. My role became an inefficient intermediate - I led numerous consultations with the team during every milestone, and had constant struggles to accommodate opinions and then feed to the developers. The decision making process was unnecessarily long and painful. The team was tired and so was I. \n\n\nDespite the hiccups along the way, a product was developed with all three modules, albeit late. The database played a major role to fulfilling the requirement of data analysis. I built the pipelines from database dumps to queries that output assessments, and then turn into usable formats. These powered the first outlook report.\n\n\nMy role in the second development is a technical advisory one and not involved in the daily management of the development.\n\n\nTechnical architecture\n\n\nThe assessment module is written in Java, a custom-made system that accepts assessments via the web and store in a PostgreSQL database. The user facing front end website displays results of assessment and was implemented in Liferay, which during the second development changed to a Drupal based system (written in PHP).\n\n\nAt the moment, there is a disconnection between the two systems: assessment and display. The assessment module and the old website is on one Amazon EC2 instance (instance name: 'IUCN WHP WHO') and the new, updated website resides another (instance name: 'IUCN WHO V2'). The new website used a database dump to migrate content from the old system. By design, the pipeline built by EDW does not reverse the flow of data in the other direction.\n\n\nThe source codes can be found on \nGitHub\n\n\nAdministration\n\n\nAs mentioned previously, the assessment module and the front facing website displaying results of assessment are hosted by Amazon (Amazon Web Services, AWS), and the servers are known as EC2 instances or servers. The old and new websites are of the type 'medium' (used to describe the capacity of the server or instance), physically in the 'eu-west-1a' region (in Ireland). Additionally, there is a 'micro' instance for the developers called 'IUCN WHO test', who used it for testing. It was decided that this 'micro instance' will be kept for future use.\n\n\n\n\nThe two additional 'micro' instances are used for hosting 1) the prototype of the World Heritage comparative analysis , 'WH CA' (seed funded by UNESCO WHC) and 2) some products on the World Heritage Analyses, 'WH APP'. \n\n\nIn order to assess these instances (use SSH), you will need 'private keys'. The keys are located at (WCMC-PC-01918)\n\n\n\n\nE:\\Yichuan\\Yichuan\\amazon_putty_ssh\n\n\n\n\nThe keys may be revoked in the \nAWS management console\n. The login credentials are:\n\n\n\n\nYichuan.shi@iucn.org\n\n\n(see my last email)\n\n\n\n\nYou may copy and send 'keys' to developers so that they have access to the servers, however, I would highly recommend \nnot\n giving external people access to our Amazon AWS account.\n\n\nThe management of the instances are beyond the scope of this documentation, however, the below are the most commonly used features:\n\n\n\n\ncreate new instances\n\n\nthe IP address of the instance (address for access)\n\n\nthe private keys (key for access)\n\n\nthe ports that need to be open (TCP22 for SSH, and TCP80 and 443 for HTTP/S)\n\n\n\n\nDatabase dump\n\n\nOne of the key function of the system(s) is to enable the extraction and analysis of information for the World Heritage Outlook report. For that purpose, it is important to obtain a copy of the dataset in the live database behind the website (but \nnever\n work on the live database itself!). This is called a database dump.\n\n\nFor security reasons, you will need to SSH tunnel to the instance where the database is located and forward the remote port to local. There is a saved session called \nWHO_backend\n that contains necessary connection information and also forwards the remote 5432 port (the database port) to the local 5433 port. \n\n\n\n\nOnce you've successfully logged using that saved session, open PgAdmin, the database management tool to connect to your local port 5433, which now points to the remote machine via SSH tunnel. \n\n\n\n\nYou may now select the database from which you'd like to make a dump. Use the 'backup' button (see red arrow) to create a file which holds the content of the database.\n\n\n\n\nOn your local machine, create an empty new database and then 'restore' (see the blue arrow) to create an identical copy of the remote database. \n\n\nThis concludes the database dump or 'extraction of data'.\n\n\nConvert to Access database\n\n\nThe data in the database dump is a replica of the database serving the website. Unfortunately the data is not stored in a format that allows us to gain useful insight, as bits of information scatter throughout numerous tables. You may refer to the diagram for additional information\n\n\n\n\nE:\\Yichuan\\IT\\ER_diagram_130802\n\n\n\n\nFor the most commonly used queries, I have built a re-usable Microsoft Access database that connects to the PostgreSQL database. This allows also to export to more familiar excel spreadsheet people find easy to work with.\n\n\n\n\nE:\\Yichuan\\Elena\\WHOA_171027\n\n\n\n\n\n\n!IMPORTANT!\n \n\n\nThere is one manual step that needs to be performed to assist the analysis before updating the links. \n\n\nWe need to differentiate the most recent assessment ids, and separate them for the two assessment cycles. This is essential to pull out the correct versions of the assessment for analysis. This step can be done in Access but would be much easier in the PostgreSQL database. In your newly restored database, create a view called 'z_wdpa_latest' using the below:\n\n\n-- View: z_wdpaid_latest\n-- DROP VIEW z_wdpaid_latest;\n\nCREATE OR REPLACE VIEW z_wdpaid_latest AS \n WITH a AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2014'::text\n        ), b AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2017'::text\n        ), combined AS (\n                 SELECT a.wdpa_id, a.assessment_cycle, a.assessment_version_id\n                   FROM a\n                  WHERE a.version_code = a.max_code\n        UNION \n                 SELECT b.wdpa_id, b.assessment_cycle, b.assessment_version_id\n                   FROM b\n                  WHERE b.version_code = b.max_code\n        )\n SELECT combined.wdpa_id, combined.assessment_cycle, max(combined.assessment_version_id) AS assessment_version_id\n   FROM combined\n  GROUP BY combined.wdpa_id, combined.assessment_cycle\n  ORDER BY combined.wdpa_id, combined.assessment_cycle;\n\nALTER TABLE z_wdpaid_latest\n  OWNER TO postgres;\n\n\n\n\nThe rationale is to identify the largest value of \nassessment_version_code\n for each cycle and for each \nassessment_id\n, on the basis that the latest/newly created id should be naturally larger. By finding this code, it is guaranteed that the latest versions are used, irrespective of the version number and stage, which may be wrong.\n\n\nOnce the above is done, you can now update the links in the MS Access database, using the 'Linked Table Manager' so that they point to the most recent local database as a source\n\n\n\n\nMake sure you tick 'Always prompt for new location', so that you update links for all tables in one go\n\n\n\n\nCreate a new data source, here you'll need to specify the details of the newly 'restored' database\n\n\n\n\nAnalysis\n\n\nThe analysis is fully documented below:\n\n\n\n\nAnalysis of World Heritage Outlook assessment", 
            "title": "World Heritage Outlook"
        }, 
        {
            "location": "/world-heritage-outlook/#history", 
            "text": "I built the first prototype of the Outlook assessment module in Python (Web2Py framework), as proof of concept. The underlying principle is to store the results of assessment in a database so that it could be managed and analysed more easily and effectively. A database approach is the only way to systematically manage the data the project generates.  For the development, RSMI was commissioned to undertake the design, implementation and maintenance of the three modules: assessment, site information, and front-end website for showcasing findings. In hindsight, it was an unrealistic ambition with impossible funds, but at the time the can-do spirit prevailed and I was technically in-mature and inexperienced as to make do with developing everything, rather than reality check and focusing on the minimally viable product.  In hindsight, part of the reason was that the roles were never clear, I think I was given the role of project management, yet I had no power in making design choices nor allocate funds. My role became an inefficient intermediate - I led numerous consultations with the team during every milestone, and had constant struggles to accommodate opinions and then feed to the developers. The decision making process was unnecessarily long and painful. The team was tired and so was I.   Despite the hiccups along the way, a product was developed with all three modules, albeit late. The database played a major role to fulfilling the requirement of data analysis. I built the pipelines from database dumps to queries that output assessments, and then turn into usable formats. These powered the first outlook report.  My role in the second development is a technical advisory one and not involved in the daily management of the development.", 
            "title": "History"
        }, 
        {
            "location": "/world-heritage-outlook/#technical-architecture", 
            "text": "The assessment module is written in Java, a custom-made system that accepts assessments via the web and store in a PostgreSQL database. The user facing front end website displays results of assessment and was implemented in Liferay, which during the second development changed to a Drupal based system (written in PHP).  At the moment, there is a disconnection between the two systems: assessment and display. The assessment module and the old website is on one Amazon EC2 instance (instance name: 'IUCN WHP WHO') and the new, updated website resides another (instance name: 'IUCN WHO V2'). The new website used a database dump to migrate content from the old system. By design, the pipeline built by EDW does not reverse the flow of data in the other direction.  The source codes can be found on  GitHub", 
            "title": "Technical architecture"
        }, 
        {
            "location": "/world-heritage-outlook/#administration", 
            "text": "As mentioned previously, the assessment module and the front facing website displaying results of assessment are hosted by Amazon (Amazon Web Services, AWS), and the servers are known as EC2 instances or servers. The old and new websites are of the type 'medium' (used to describe the capacity of the server or instance), physically in the 'eu-west-1a' region (in Ireland). Additionally, there is a 'micro' instance for the developers called 'IUCN WHO test', who used it for testing. It was decided that this 'micro instance' will be kept for future use.   The two additional 'micro' instances are used for hosting 1) the prototype of the World Heritage comparative analysis , 'WH CA' (seed funded by UNESCO WHC) and 2) some products on the World Heritage Analyses, 'WH APP'.   In order to assess these instances (use SSH), you will need 'private keys'. The keys are located at (WCMC-PC-01918)   E:\\Yichuan\\Yichuan\\amazon_putty_ssh   The keys may be revoked in the  AWS management console . The login credentials are:   Yichuan.shi@iucn.org  (see my last email)   You may copy and send 'keys' to developers so that they have access to the servers, however, I would highly recommend  not  giving external people access to our Amazon AWS account.  The management of the instances are beyond the scope of this documentation, however, the below are the most commonly used features:   create new instances  the IP address of the instance (address for access)  the private keys (key for access)  the ports that need to be open (TCP22 for SSH, and TCP80 and 443 for HTTP/S)", 
            "title": "Administration"
        }, 
        {
            "location": "/world-heritage-outlook/#database-dump", 
            "text": "One of the key function of the system(s) is to enable the extraction and analysis of information for the World Heritage Outlook report. For that purpose, it is important to obtain a copy of the dataset in the live database behind the website (but  never  work on the live database itself!). This is called a database dump.  For security reasons, you will need to SSH tunnel to the instance where the database is located and forward the remote port to local. There is a saved session called  WHO_backend  that contains necessary connection information and also forwards the remote 5432 port (the database port) to the local 5433 port.    Once you've successfully logged using that saved session, open PgAdmin, the database management tool to connect to your local port 5433, which now points to the remote machine via SSH tunnel.    You may now select the database from which you'd like to make a dump. Use the 'backup' button (see red arrow) to create a file which holds the content of the database.   On your local machine, create an empty new database and then 'restore' (see the blue arrow) to create an identical copy of the remote database.   This concludes the database dump or 'extraction of data'.", 
            "title": "Database dump"
        }, 
        {
            "location": "/world-heritage-outlook/#convert-to-access-database", 
            "text": "The data in the database dump is a replica of the database serving the website. Unfortunately the data is not stored in a format that allows us to gain useful insight, as bits of information scatter throughout numerous tables. You may refer to the diagram for additional information   E:\\Yichuan\\IT\\ER_diagram_130802   For the most commonly used queries, I have built a re-usable Microsoft Access database that connects to the PostgreSQL database. This allows also to export to more familiar excel spreadsheet people find easy to work with.   E:\\Yichuan\\Elena\\WHOA_171027    !IMPORTANT!    There is one manual step that needs to be performed to assist the analysis before updating the links.   We need to differentiate the most recent assessment ids, and separate them for the two assessment cycles. This is essential to pull out the correct versions of the assessment for analysis. This step can be done in Access but would be much easier in the PostgreSQL database. In your newly restored database, create a view called 'z_wdpa_latest' using the below:  -- View: z_wdpaid_latest\n-- DROP VIEW z_wdpaid_latest;\n\nCREATE OR REPLACE VIEW z_wdpaid_latest AS \n WITH a AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2014'::text\n        ), b AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2017'::text\n        ), combined AS (\n                 SELECT a.wdpa_id, a.assessment_cycle, a.assessment_version_id\n                   FROM a\n                  WHERE a.version_code = a.max_code\n        UNION \n                 SELECT b.wdpa_id, b.assessment_cycle, b.assessment_version_id\n                   FROM b\n                  WHERE b.version_code = b.max_code\n        )\n SELECT combined.wdpa_id, combined.assessment_cycle, max(combined.assessment_version_id) AS assessment_version_id\n   FROM combined\n  GROUP BY combined.wdpa_id, combined.assessment_cycle\n  ORDER BY combined.wdpa_id, combined.assessment_cycle;\n\nALTER TABLE z_wdpaid_latest\n  OWNER TO postgres;  The rationale is to identify the largest value of  assessment_version_code  for each cycle and for each  assessment_id , on the basis that the latest/newly created id should be naturally larger. By finding this code, it is guaranteed that the latest versions are used, irrespective of the version number and stage, which may be wrong.  Once the above is done, you can now update the links in the MS Access database, using the 'Linked Table Manager' so that they point to the most recent local database as a source   Make sure you tick 'Always prompt for new location', so that you update links for all tables in one go   Create a new data source, here you'll need to specify the details of the newly 'restored' database", 
            "title": "Convert to Access database"
        }, 
        {
            "location": "/world-heritage-outlook/#analysis", 
            "text": "The analysis is fully documented below:   Analysis of World Heritage Outlook assessment", 
            "title": "Analysis"
        }, 
        {
            "location": "/presentation/", 
            "text": "Prior to 2016, all presentations are stored on the workstation (WCMC-PC-01918.internal.wcmc):\n\n\n\n\nE:\\Yichuan\\Yichuan\\presentations\n\n\n\n\nNew format presentations have been used since late 2016. They can be found online:\n\n\n\n\nNew format presentations", 
            "title": "Presentation"
        }, 
        {
            "location": "/folder-structure/", 
            "text": "The content can be located at \nE:\\Yichuan\n, on the workstation (WCMC-PC-01918.internal.wcmc)\n\n\nMost of my content is organised into folders with names of collaborator or those who request help. This is the highest level. Within this level, project files can be found if this is a one-off request; or folders representing different projects. \n\n\nFolders named \nnot after\n people usually refer to one-off/non-specific, large or recurring projects. Below is a concise explanation of important folders (the most important folders are in bold), or those needs clarification:\n\n\n\n\nAdmin: all admins, including timesheets, travel authorisations, visas, tickets, cost reclaims and etc\n\n\nBasedata\n: most base data are located here (spatial). Typically they include base physical, cultural boundaries, grids, hillshade, ecoregions etc. Recommendation: use esri ArcCatalog to navigate\n\n\nCOM_XXX: preparations for World Heritage Committee meetings\n\n\nComparative_analysis_20XX\n: results of comparative analysis of that year. They will generally include digitised polygons, results of the analysis in tables, and also maps (including map templates that create them)\n\n\nDggrid: discrete global hexagon grids program\n\n\nDocuments: unsorted \ndocx\n or \npdf\n format documents prior to 2014\n\n\nDump_PG: unsorted database dump prior to 2014\n\n\nElena\n: including World Heritage Outlook data dumps and analysis files are located\n\n\nExperiment: test space for various tools, analysis and scripts, unsorted and \ndo not use\n\n\nGood_reference_map_collection: a collection of maps for reference\n\n\nIT\n: project folder for the first World Heritage Outlook system development \n\n\nLandsat_archiving: python based script for the archiving project, superseded by the \nnear real time Landsat 8 images\n in the Lab. \ndo not use\n\n\nMap_templates: outdated template \ndo not use\n\n\nMapmart: 0.5m resolution WorldView imagery purchased to identify mining activities, in Russia\n\n\nMyGDB.gdb\n: a somewhat historic/duplicate place holder for base data, but in esri geodatabase format. It mainly contains biogeographic, broadscale priorities and old site level priorities such as KBA datasets. It also includes world vector shore line data (including EEZs). While I do not recommend using any of the datasets, \ndo not delete\n this database, as there may be map documents referring to the data here-within \n\n\nMyWorkplace.gdb: similar to 'Experiment' folder but in geodatabase format\n\n\nPapers: feeder folder for Mendeley, the reference management system for papers\n\n\nRed_List_data\n: raw Red List data releases from 2013\n\n\nRemote sensing: test data for raw satellite images (mostly Landsat) collected throughout my time for ad-hoc projects\n\n\nScripts\n: arguably the most important folder. Sub folder structure below\n\n\n/geoprocessing\n: scripts and libraries for undertaking spatial analysis, generic data analysis, workflow automation etc. See the \ngeoprocessing\n section for more information. This is also backed up on \nGitHub\n\n\n/scripts\n: historic place holder for scripts. Most one off, ad-hoc scripts are here.\n\n\n/mysql20110516: outdated SQL scripts for maintaining the PostgreSQL database\n\n\n\n\n\n\nsites_improve_geometry_XXX: series of attempts to improve the qualities of WH datasets until 2015\n\n\nsitesXXXX: digitised boundaries for new nominations, and successful inscriptions and/or modifications. \n\n\nTentativeList: outdated tentative list sites improvement. \ndo not use\n\n\nUpdates: outdated map document to update the database and no longer relevant \ndo not use\n\n\nWDPA: copies of WDPA monthly releases for analysis\n\n\nWH_benefits: project folder for the first WH benefits analysis\n\n\nWH_stats\n: statistics after COM\n\n\nWHO_geom_updates: methodology for updating the maps in the old WH Outlook system. This may not be relevant any more\n\n\nWHS.gdb\n: spatial data for natural and mixed World Heritage sites, in geodatabase format\n\n\nWHS_arcgisonlineXXX.gdb\n: spatial data to be zipped and uploaded to the arcgisonline. This is the data behind the \nesri feature service\n\n\nWHS_dump_ATTR\n: attributes/non spatial data for natural and mixed World Heritage sites\n\n\nWHS_dump_KML: historical dumps of WH data in KML format, superseded by the feature service\n\n\nWHS_dump_SHP: shapefile format of the WH data\n\n\nWHS_map_batcher: map template to automate map production\n\n\nWHS_quality_check: map comparisons between GIS, official maps from UNESCO, including retrospective ones. The numerous comparisons underpin and empower the systematic check and subsequently the improvement of the data.\n\n\nYichuan: miscellaneous", 
            "title": "Folder structure"
        }
    ]
}