{
    "docs": [
        {
            "location": "/", 
            "text": "About\n\n\nThis is the last project I undertake at IUCN's World Heritage Programme. I intend to document to the best of my knowledge my work in the past seven years that my be useful in the future, and also pass necessary information onto the team. Apart from the usual 'where to find things', these will include high-level introductions of past projects, datasets, tools and methodologies, and in some cases with in-depth explanations if such detail is deemed useful and has not been documented elsewhere.\n\n\n[INTERNAL USE ONLY!]\n \n\n\nI choose to use a web format to organise the handover content simply due to the fact that it does a better job in searching. This could be published more widely for easy accessibility, particularly for methodologies that may have a wider use, but I would discourage such use for matters of potential confidentiality.\n\n\nIn terms of content, below is a linked list. You may also use the \nsearch docs\n box to locate content using keywords.\n\n\n\n\nWorld Heritage GIS database. This includes, amongst others, the maintenance of the database, updates, and statistics and export that depend on the database.\n\n\nComparative analysis. As contrary to the methodology, which has been published as a separate document, will focus on the technical side of the analysis in GIS, should future users require to replicate or improve the semi-automatic process utilised in recent years. \n\n\nWorld Heritage analyses initiative. \n\n\nWorld Heritage datasheets\n\n\nFolder structure on workstation\n\n\nWorld Heritage Outlook\n\n\nGeoprocessing methodologies\n\n\nPresentations\n\n\n\n\nI do not intend to duplicate what is being documented elsewhere in greater details for \nWorld Heritage Analysis\n, alongside their source codes on \nGitHub\n.\n\n\nAcknowledgement\n\n\nI would also like to take this opportunity to thank Tim and Naomi for the oversight and mentoring, and for making the secondment arrangement a success. Thanks also go to the two wonderful teams at IUCN and at UNEP-WCMC, and to many colleagues I have worked with in the long seven years. I hope to remain in touch with you all and wish you all the very best in 2018 and onwards.", 
            "title": "Home"
        }, 
        {
            "location": "/#about", 
            "text": "This is the last project I undertake at IUCN's World Heritage Programme. I intend to document to the best of my knowledge my work in the past seven years that my be useful in the future, and also pass necessary information onto the team. Apart from the usual 'where to find things', these will include high-level introductions of past projects, datasets, tools and methodologies, and in some cases with in-depth explanations if such detail is deemed useful and has not been documented elsewhere.  [INTERNAL USE ONLY!]    I choose to use a web format to organise the handover content simply due to the fact that it does a better job in searching. This could be published more widely for easy accessibility, particularly for methodologies that may have a wider use, but I would discourage such use for matters of potential confidentiality.  In terms of content, below is a linked list. You may also use the  search docs  box to locate content using keywords.   World Heritage GIS database. This includes, amongst others, the maintenance of the database, updates, and statistics and export that depend on the database.  Comparative analysis. As contrary to the methodology, which has been published as a separate document, will focus on the technical side of the analysis in GIS, should future users require to replicate or improve the semi-automatic process utilised in recent years.   World Heritage analyses initiative.   World Heritage datasheets  Folder structure on workstation  World Heritage Outlook  Geoprocessing methodologies  Presentations   I do not intend to duplicate what is being documented elsewhere in greater details for  World Heritage Analysis , alongside their source codes on  GitHub .", 
            "title": "About"
        }, 
        {
            "location": "/#acknowledgement", 
            "text": "I would also like to take this opportunity to thank Tim and Naomi for the oversight and mentoring, and for making the secondment arrangement a success. Thanks also go to the two wonderful teams at IUCN and at UNEP-WCMC, and to many colleagues I have worked with in the long seven years. I hope to remain in touch with you all and wish you all the very best in 2018 and onwards.", 
            "title": "Acknowledgement"
        }, 
        {
            "location": "/world-heritage-database/", 
            "text": "Introduction\n\n\nPostgreSQL technical database\n\n\nHow-to: update the database\n\n\nStatistics", 
            "title": "Database"
        }, 
        {
            "location": "/world-heritage-database/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/world-heritage-database/#postgresql-technical-database", 
            "text": "", 
            "title": "PostgreSQL technical database"
        }, 
        {
            "location": "/world-heritage-database/#how-to-update-the-database", 
            "text": "", 
            "title": "How-to: update the database"
        }, 
        {
            "location": "/world-heritage-database/#statistics", 
            "text": "", 
            "title": "Statistics"
        }, 
        {
            "location": "/comparative-analysis/", 
            "text": "Introduction\n\n\nThe comparative analysis is an unbiased, consistent analysis of new biodiversity nominations, i.e. criteria (ix) and (x), against existing World Heritage sites. It has been used an important material to the IUCN World Heritage Panel. The methodology has been documented in the paper \nComparative analysis methodology for World Heritage nominations under biodiversity criteria\n, published in 2014. \n\n\nThis page aims to introduce the spatial aspect of the analysis and the evolving engineering behind it. \n\n\nUpdates\n\n\n\n\nSome duplication but additional documentation can be found on \nGitHub\n\n\n\n\nFor past comparative analysis results, please visit section of \nmy folder structure\n\n\nChallenges\n\n\nConceptually, the idea is rather simple. At the lowest level, it involves the spatial intersection of the boundary of a given new nomination, against a base layer, and then extract some statistics. For example, overlay the terrestrial ecoregion layer with new nominations, and calculate the area and percentage of overlap.\n\n\n\n\nThe complexity arises when additional variables come into play. \n\n\nThe first challenge is the plethora of base layers required for the analysis. Despite the booming availability of global datasets on biodiversity, no data has been so far collated for the purpose of inscriptions of biodiversity World Heritage sites. The narratives around the arguments for \nOutstanding Universal Value\n do not provide a clear direction to find suitable datasets that are also spatially explicit. Thus, no single dataset can be utilised (nor should there ever be), and the decision must be deferred to the expert judgement of experienced professionals. The tasks however for the GIS analyses thus require all, to the extent possible, related datasets be overlaid and analysed.\n\n\nNext, with heterogeneous data sources, different questions require different answers. For example, methodologies to answer a question about the (over)representation of a certain biogeography differ from that inquiring the indicative number of species that might be present. Some seemingly different questions can be grouped using similar processes to analyse, while others albeit sharing common traits in the narratives may dictate a drastically different methodology to be developed\n\n\nAdditionally, to enable comparisons, not only nominations should be overlaid, but also existing sites, sites from the Tentative List, and in some case certain protected areas may need to be considered as well. This multiplies the computational load, produces significantly more information, and complicates the process of interpretation and presentation. \n\n\nFinally, the numerous, abstruse numbers must be put in a lay format that makes sense to the audience. They have to be digested and presented in a clear and logic way - it's no use to have volumes of raw results, as it will only confuse the Panel.\n\n\nBio-geographic classification, and priorities\n\n\nOne of the classic tasks of the comparative analysis is to look at whether a certain natural environment has been over-represented, therefore supporting an argument of refusal, as justifying it being better than those already on the List becomes significantly more difficult, as more sites crowd the already crowded space. On the contrary, if a nomination represents a previously unrepresented or under-represented space, the argument for inscribing is strengthened. \n\n\nFrom a technical perspective, these types of comparisons are done with a simple overlay, re-using a similar template. Such data include the terrestrial/marine ecoregions, broad scale priorities such Global 200 priority, biodiversity hotspot and etc.\n\n\nThe current implementation relies on the PostgreSQL database and the PostGIS spatial extension. ArcGIS is optional but preferred, as it simplifies the process of importing and updating the underlying database.\n\n\nThe source code of the analysis can be found in detail below\n\n\n\n\nhttps://github.com/Yichuans/geoprocessing/blob/master/comparative_analysis/comparative_analysis.py\n\n\n\n\nIn short, the script goes through the dictionary holding look-up information about each layer (name, the schema and table etc), and then intersects a 'theme' with a combined view that has both World Heritage sites and nominations with \nrun_ca_for_a_theme\n, which does two tasks: a) intersect 2) group and combine the numbers. This design reflects the need to account for single part polygons and results required at different scales (for example, for terrestrial ecoregions, realms and biomes). As the last step, \npost_intersection_mk2\n filters only those results relating to the nominated site.\n\n\nTo undertake the analysis, below are the steps required\n\n\n\n\nLoad the newly nominated sites to the \nca_nomi\n schema, preferably via ArcGIS, using \nPG_Geometry\n configuration, so that PostGIS can read the geometry\n\n\nCreate in the postgres database an empty schema, to which results of the analysis would be placed, typically, this is called \nca_xxxx\n (xxxx refers to the year when the analysis is done)\n\n\nUpdate the base layers if necessary\n\n\nCreate a new function like below and run it\n\n\n\n\ndef ca_2017():\n    input_nomination = 'ca_nomi.nomi_2017'\n    output_schema = 'ca_2017'\n    for themekey in BASE_LOOKUP.keys():\n        run_ca_for_a_theme(input_nomination, output_schema, themekey, conn_arg=get_ca_conn_arg(2015))\n\n\n\n\nThe first line refers to the database table of the nomination. The second the location of output. The 'for' loop programatically pick each and every base layer, a.k.a., theme, to intersect with the combined view. Although only the nomination table is specified, the script knows where to find the World Heritage boundary, i.e., \narcgis.v_wh_spatial\n. The maintenance of the World Heritage boundary is documented in the \ndatabase section\n. Internally, the \ncreate_combined_wh_nomination_view\n function creates one.\n\n\nIt is important to note that an arbitrary threshold of 0.05 is employed to minimise commission errors.\n\n\nIncreased productivity\n\n\nThe above process can be improved simply by running the \nrun_ca_for_a_theme\n simultaneously. One example is listed below (the 2017 comparative analysis)\n\n\ndef f(themekey):\n    input_nomination = 'ca_nomi.nomi_2017_with_supp'\n    output_schema = 'ca_2017_with_supp'\n    run_ca_for_a_theme(input_nomination, output_schema, themekey, conn_arg=get_ca_conn_arg(2015))\n\n\n# wrap in main\nif __name__ == '__main__':\n    p = Pool(10)\n    keys = BASE_LOOKUP.keys()\n    print keys\n    print(p.map(f,keys))\n\n\n\n\nTentative list sites\n\n\nIf one were to look deeper, both the World Heritage sites and tentative lists don't change, at least not constantly. Thus the results could cached, i.e., there is no need to re-do the overlay between them the many base layers. Whenever a comparison is needed, I can simply query the cache and pull out relevant records. This is especially true for tentative list sites, which tend to stay the same for years.\n\n\nFor tentative list sites, the \nrun_ca_tentative\n utilises the same logic in the above methodology and produce a set of database views holding dynamic results\n\n\nExport to excel\n\n\nWith the results fully generated in the database, I built a process to stitch relevant tables, including lookup tables for contextual information, and export excel formats to aid interpretation.\n\n\nEach nomination has an excel table with three tabs, referring to 'biogeographic', 'priority', and 'site-level' respectively. \n\n\nThe 2017 script can be found below\n\n\n\n\nhttps://github.com/Yichuans/geoprocessing/blob/master/comparative_analysis/comparative_analysis_to_excel_2017.py\n\n\n\n\nFor the export to work, the following information needs to be specified in the script. The below is taken from the 2017 script\n\n\n# 2017\n\nCOMBINED_WH_NOMINATION_VIEW = 'z_combined_wh_nomination_view'\nWH_ATTR = 'arcgis.v_wh_non_spatial_full' # attribute look up table for world heritage\nTLS_SHAPE = 'tls.tentative' # tentative list site table\n\nOUTPUT_SCHEMA = 'ca_2017_with_supp' # the location of where the output tables are stored\nTLS_SCHEMA = 'ca_tls'   # the location of where the output table for tentative list sites are stored\nTLS_ORGIN = 'tls.origin'    # the original tentative list sites table, for looking up criteria information\n\n# List of nomination IDs\nNOMI_ID = range(9991701, 9991706) + range(99917011, 99917015) + range(99917021, 99917023)\n\n\n\n\n\nLastly, specify the location of the output \noutputfolder\n\n\nif __name__ == '__main__':\n    outputfolder=r\nE:\\Yichuan\\Comparative_analysis_2017\n\n    main(outputfolder)\n\n\n\n\nFull result export\n\n\nTo accommodate needs to go beyond the question of 'what they are', though uncommon, questions also arise regarding the actual proportion of overlap. This effectively requires a full dump of the database with all results - this can be cumbersome to read.\n\n\nSince 2016, this functionality has been added in a separate script, called \ncomparative_analysis_to_excel_fullresults\n(\nsource code\n). The principal idea is to dump original results of each base layer into tabs in an excel table, in which the overlap in area and proportion are recorded. This is meant to be used as a reference in cases where apparent overlaps may be questionable or justifying findings that are counter-intuitive.\n\n\nSpecies richness\n\n\nTo counter the often inflated number of species reported in nomination dossiers, a consistent metric has been developed that counts the indicative species richness by interrogating \nthe IUCN Red List of Threatened Species\n. This presents an opportunity of examining comparatively the abundance of biodiversity through surrogates of comprehensively assessed species, particularly useful for criterion (x)\n\n\nThe IUCN Red List of Threatened Species has a spatially explicit database that could be requested from its website or directly from the Red List Unit. The usual practice is to include only species range polygons of presence 1 and 2, origin 1 and 2, and seasonality 1, 2, and 3. Clean the database by repairing and dicing if necessary. If not present already, create an index on the field \nid_no\n that uniquely identifies a species. This speeds up the analysis considerably.\n\n\nThe next step is to programatically call \narcpy.SelectLayerByLocation\n for each species and the World Heritage sites and nominations. It may take a long time to go through every one of the 70k+ species with boundaries. The end result is a two column table recording the \nid_no\n for species and \nwdpaid\n for WH sites and nominations. \n\n\nLoad the resulting table and the non-spatial Red List data to the same PostgreSQL database with rest of the results. The species richness makes lots of assumption and refers to the same look up tables in the section on \nBio-geographic classification, and priorities\n\n\nThe following information need to be specified in the script \ncomparative_analysis/comparative_analysis_group_species.py\n (\nsource code\n), which retrieves information from lookup tables and generate a master view with different species richness count for different taxonomies (including only those that are threatened)\n\n\n# the resulting richness analysis table\nall_sp = \nad_hoc.species_ca_2017\n\nall_sp_taxonid = 'species_ca_2017.id_no' # must not be the same as all_sis_taxonid\nall_sp_baseid = 'species_ca_2017.wdpaid'\n\n# look up table, non spatial version of the input Red List data\nall_sis = \nad_hoc.rl_2017_2_pos\n\nall_sis_taxonid = \nrl_2017_2_pos.id_no\n\n\n# WH/nomi name look up table\n# NOTE: assuming wdpaid is present!!!!!\nwh_nomi_lookup = \nca_2017_with_supp.z_combined_wh_nomination_view\n\nwh_nomi_name = \nz_combined_wh_nomination_view.en_name\n\n\n# name\nKINGDOM_FIELD = 'kingdom'\nCLASS_FIELD = 'class'\nBINOMIAL_FIELD = 'binomial'\nRL_FIELD = 'code'\n\n\n\n\nLastly, specify the output schema, typical the same location as the one for the biogeographic/priority results\n\n\nmain('ca_2017_with_supp')\n\n\n\n\nThe technical implementation is also documented in the \nrichness template section in geoprocessing\n\n\nIrreplaceability\n\n\nThe irreplaceability utilised the methodology published at \nScience\n. \n\n\nThe shortcoming of using the Red List range polygons is rather obvious: it does little to mitigate the effect of over-estimation of their Area of Occupancy (AOO) and that richness counts almost certainly inflate the actual number of species. The irreplaceability metric overcomes this by using sigmoid functions to translate actual proportional overlaps and aggregate to a single value that, to a certain extent, captures both richness and endemism.\n\n\nThe implementation can be found below (\nsource code\n)\n\n\n\ndef species_irreplaceability(x):\n    \nx is the percentage in decimal\n\n\n    x = x*100\n\n    def h(x):\n    # h(x) as specified\n        miu = 39\n        s = 9.5\n        tmp = -(x - miu)/ s\n        denominator = 1 + np.exp(tmp)\n        return 1/denominator\n\n\n    return (h(x) - h(0))/(h(100) - h(0))\n\n\n\n\n\nBecause the irreplaceability analysis builds on the full intersection between the WDPA and the Red List (the pairwise percentage overlap value), which is in itself a massive undertaking, I cannot re-run irreplaceability without being given the result of the full intersection first. For this reason, up to this day, only two runs exist (the latest updated in 2015, which we rely on).\n\n\nStatic maps\n\n\nMaps accompany the comparative analysis are authored using ArcMap, and later ArcPro. \n\n\nThe template is pre-authored and re-used across all maps, although labelling and styles may be dynamically adjusted. To the extent possible, I try to automate the production of maps. In ArcMap, this is done via \narcpy.mapping\n package that produces maps by iterating each feature in the nomination feature class. The export process outputs any format supported by ArcMap, usually in PNG for the reports or AI/EPS/PDF for printing. The source code could be found in the \ngeoprocessing section\n\n\nLater, new capabilities in ArcPro make the above process obsolete. There is no need any more to write any code. The iteration instead require a pre-cooked extent feature class be create to represent the viewport of maps. The default data-driven pages can loop through each extent feature and export maps of desired formats. Worth mentioning is that PDF format maps also have the ability to turn on and off layers, that in a way mimics behaviours of a dynamic maps, giving readers powers to interrogate the map themselves for information. \n\n\nThe 2017 static maps can be found below:\n\n\n\n\nE:\\Yichuan\\Comparative_analysis_2017\\static_maps\n\n\n\n\nInteractive maps\n\n\nTO BE ADDED\n\n\n\n\n\nConclusion\n\n\nIn hindsight, the analysis using intersections is slightly over-engineered. The original implementation relies entirely on SQLs, which had severe shortcomings that flow control was extremely inflexible in a pure SQL environment. Python, being a generic glue language, was picked to generate SQL statement and feed to the database management system. The result is a somewhat haphazard concoction of spaghetti codes, with unnecessarily engineered towards objective-oriented design. \n\n\nFor future improvement, a pure function based structure may be a better alternative. Or, considering the massive improvement of the \narcpy\n library and the fine controls of geometry it now has, it may be wise to migrated back to an esri based system, which may be easier to manage, unifying the database management and the analysis. Further debate on pros and cons is required - going back to a solution based on proprietary software seems counter-intuitive in the present day of open data and open science.", 
            "title": "Comparative analysis"
        }, 
        {
            "location": "/comparative-analysis/#introduction", 
            "text": "The comparative analysis is an unbiased, consistent analysis of new biodiversity nominations, i.e. criteria (ix) and (x), against existing World Heritage sites. It has been used an important material to the IUCN World Heritage Panel. The methodology has been documented in the paper  Comparative analysis methodology for World Heritage nominations under biodiversity criteria , published in 2014.   This page aims to introduce the spatial aspect of the analysis and the evolving engineering behind it.   Updates   Some duplication but additional documentation can be found on  GitHub   For past comparative analysis results, please visit section of  my folder structure", 
            "title": "Introduction"
        }, 
        {
            "location": "/comparative-analysis/#challenges", 
            "text": "Conceptually, the idea is rather simple. At the lowest level, it involves the spatial intersection of the boundary of a given new nomination, against a base layer, and then extract some statistics. For example, overlay the terrestrial ecoregion layer with new nominations, and calculate the area and percentage of overlap.   The complexity arises when additional variables come into play.   The first challenge is the plethora of base layers required for the analysis. Despite the booming availability of global datasets on biodiversity, no data has been so far collated for the purpose of inscriptions of biodiversity World Heritage sites. The narratives around the arguments for  Outstanding Universal Value  do not provide a clear direction to find suitable datasets that are also spatially explicit. Thus, no single dataset can be utilised (nor should there ever be), and the decision must be deferred to the expert judgement of experienced professionals. The tasks however for the GIS analyses thus require all, to the extent possible, related datasets be overlaid and analysed.  Next, with heterogeneous data sources, different questions require different answers. For example, methodologies to answer a question about the (over)representation of a certain biogeography differ from that inquiring the indicative number of species that might be present. Some seemingly different questions can be grouped using similar processes to analyse, while others albeit sharing common traits in the narratives may dictate a drastically different methodology to be developed  Additionally, to enable comparisons, not only nominations should be overlaid, but also existing sites, sites from the Tentative List, and in some case certain protected areas may need to be considered as well. This multiplies the computational load, produces significantly more information, and complicates the process of interpretation and presentation.   Finally, the numerous, abstruse numbers must be put in a lay format that makes sense to the audience. They have to be digested and presented in a clear and logic way - it's no use to have volumes of raw results, as it will only confuse the Panel.", 
            "title": "Challenges"
        }, 
        {
            "location": "/comparative-analysis/#bio-geographic-classification-and-priorities", 
            "text": "One of the classic tasks of the comparative analysis is to look at whether a certain natural environment has been over-represented, therefore supporting an argument of refusal, as justifying it being better than those already on the List becomes significantly more difficult, as more sites crowd the already crowded space. On the contrary, if a nomination represents a previously unrepresented or under-represented space, the argument for inscribing is strengthened.   From a technical perspective, these types of comparisons are done with a simple overlay, re-using a similar template. Such data include the terrestrial/marine ecoregions, broad scale priorities such Global 200 priority, biodiversity hotspot and etc.  The current implementation relies on the PostgreSQL database and the PostGIS spatial extension. ArcGIS is optional but preferred, as it simplifies the process of importing and updating the underlying database.  The source code of the analysis can be found in detail below   https://github.com/Yichuans/geoprocessing/blob/master/comparative_analysis/comparative_analysis.py   In short, the script goes through the dictionary holding look-up information about each layer (name, the schema and table etc), and then intersects a 'theme' with a combined view that has both World Heritage sites and nominations with  run_ca_for_a_theme , which does two tasks: a) intersect 2) group and combine the numbers. This design reflects the need to account for single part polygons and results required at different scales (for example, for terrestrial ecoregions, realms and biomes). As the last step,  post_intersection_mk2  filters only those results relating to the nominated site.  To undertake the analysis, below are the steps required   Load the newly nominated sites to the  ca_nomi  schema, preferably via ArcGIS, using  PG_Geometry  configuration, so that PostGIS can read the geometry  Create in the postgres database an empty schema, to which results of the analysis would be placed, typically, this is called  ca_xxxx  (xxxx refers to the year when the analysis is done)  Update the base layers if necessary  Create a new function like below and run it   def ca_2017():\n    input_nomination = 'ca_nomi.nomi_2017'\n    output_schema = 'ca_2017'\n    for themekey in BASE_LOOKUP.keys():\n        run_ca_for_a_theme(input_nomination, output_schema, themekey, conn_arg=get_ca_conn_arg(2015))  The first line refers to the database table of the nomination. The second the location of output. The 'for' loop programatically pick each and every base layer, a.k.a., theme, to intersect with the combined view. Although only the nomination table is specified, the script knows where to find the World Heritage boundary, i.e.,  arcgis.v_wh_spatial . The maintenance of the World Heritage boundary is documented in the  database section . Internally, the  create_combined_wh_nomination_view  function creates one.  It is important to note that an arbitrary threshold of 0.05 is employed to minimise commission errors.", 
            "title": "Bio-geographic classification, and priorities"
        }, 
        {
            "location": "/comparative-analysis/#increased-productivity", 
            "text": "The above process can be improved simply by running the  run_ca_for_a_theme  simultaneously. One example is listed below (the 2017 comparative analysis)  def f(themekey):\n    input_nomination = 'ca_nomi.nomi_2017_with_supp'\n    output_schema = 'ca_2017_with_supp'\n    run_ca_for_a_theme(input_nomination, output_schema, themekey, conn_arg=get_ca_conn_arg(2015))\n\n\n# wrap in main\nif __name__ == '__main__':\n    p = Pool(10)\n    keys = BASE_LOOKUP.keys()\n    print keys\n    print(p.map(f,keys))", 
            "title": "Increased productivity"
        }, 
        {
            "location": "/comparative-analysis/#tentative-list-sites", 
            "text": "If one were to look deeper, both the World Heritage sites and tentative lists don't change, at least not constantly. Thus the results could cached, i.e., there is no need to re-do the overlay between them the many base layers. Whenever a comparison is needed, I can simply query the cache and pull out relevant records. This is especially true for tentative list sites, which tend to stay the same for years.  For tentative list sites, the  run_ca_tentative  utilises the same logic in the above methodology and produce a set of database views holding dynamic results", 
            "title": "Tentative list sites"
        }, 
        {
            "location": "/comparative-analysis/#export-to-excel", 
            "text": "With the results fully generated in the database, I built a process to stitch relevant tables, including lookup tables for contextual information, and export excel formats to aid interpretation.  Each nomination has an excel table with three tabs, referring to 'biogeographic', 'priority', and 'site-level' respectively.   The 2017 script can be found below   https://github.com/Yichuans/geoprocessing/blob/master/comparative_analysis/comparative_analysis_to_excel_2017.py   For the export to work, the following information needs to be specified in the script. The below is taken from the 2017 script  # 2017\n\nCOMBINED_WH_NOMINATION_VIEW = 'z_combined_wh_nomination_view'\nWH_ATTR = 'arcgis.v_wh_non_spatial_full' # attribute look up table for world heritage\nTLS_SHAPE = 'tls.tentative' # tentative list site table\n\nOUTPUT_SCHEMA = 'ca_2017_with_supp' # the location of where the output tables are stored\nTLS_SCHEMA = 'ca_tls'   # the location of where the output table for tentative list sites are stored\nTLS_ORGIN = 'tls.origin'    # the original tentative list sites table, for looking up criteria information\n\n# List of nomination IDs\nNOMI_ID = range(9991701, 9991706) + range(99917011, 99917015) + range(99917021, 99917023)  Lastly, specify the location of the output  outputfolder  if __name__ == '__main__':\n    outputfolder=r E:\\Yichuan\\Comparative_analysis_2017 \n    main(outputfolder)", 
            "title": "Export to excel"
        }, 
        {
            "location": "/comparative-analysis/#full-result-export", 
            "text": "To accommodate needs to go beyond the question of 'what they are', though uncommon, questions also arise regarding the actual proportion of overlap. This effectively requires a full dump of the database with all results - this can be cumbersome to read.  Since 2016, this functionality has been added in a separate script, called  comparative_analysis_to_excel_fullresults ( source code ). The principal idea is to dump original results of each base layer into tabs in an excel table, in which the overlap in area and proportion are recorded. This is meant to be used as a reference in cases where apparent overlaps may be questionable or justifying findings that are counter-intuitive.", 
            "title": "Full result export"
        }, 
        {
            "location": "/comparative-analysis/#species-richness", 
            "text": "To counter the often inflated number of species reported in nomination dossiers, a consistent metric has been developed that counts the indicative species richness by interrogating  the IUCN Red List of Threatened Species . This presents an opportunity of examining comparatively the abundance of biodiversity through surrogates of comprehensively assessed species, particularly useful for criterion (x)  The IUCN Red List of Threatened Species has a spatially explicit database that could be requested from its website or directly from the Red List Unit. The usual practice is to include only species range polygons of presence 1 and 2, origin 1 and 2, and seasonality 1, 2, and 3. Clean the database by repairing and dicing if necessary. If not present already, create an index on the field  id_no  that uniquely identifies a species. This speeds up the analysis considerably.  The next step is to programatically call  arcpy.SelectLayerByLocation  for each species and the World Heritage sites and nominations. It may take a long time to go through every one of the 70k+ species with boundaries. The end result is a two column table recording the  id_no  for species and  wdpaid  for WH sites and nominations.   Load the resulting table and the non-spatial Red List data to the same PostgreSQL database with rest of the results. The species richness makes lots of assumption and refers to the same look up tables in the section on  Bio-geographic classification, and priorities  The following information need to be specified in the script  comparative_analysis/comparative_analysis_group_species.py  ( source code ), which retrieves information from lookup tables and generate a master view with different species richness count for different taxonomies (including only those that are threatened)  # the resulting richness analysis table\nall_sp =  ad_hoc.species_ca_2017 \nall_sp_taxonid = 'species_ca_2017.id_no' # must not be the same as all_sis_taxonid\nall_sp_baseid = 'species_ca_2017.wdpaid'\n\n# look up table, non spatial version of the input Red List data\nall_sis =  ad_hoc.rl_2017_2_pos \nall_sis_taxonid =  rl_2017_2_pos.id_no \n\n# WH/nomi name look up table\n# NOTE: assuming wdpaid is present!!!!!\nwh_nomi_lookup =  ca_2017_with_supp.z_combined_wh_nomination_view \nwh_nomi_name =  z_combined_wh_nomination_view.en_name \n\n# name\nKINGDOM_FIELD = 'kingdom'\nCLASS_FIELD = 'class'\nBINOMIAL_FIELD = 'binomial'\nRL_FIELD = 'code'  Lastly, specify the output schema, typical the same location as the one for the biogeographic/priority results  main('ca_2017_with_supp')  The technical implementation is also documented in the  richness template section in geoprocessing", 
            "title": "Species richness"
        }, 
        {
            "location": "/comparative-analysis/#irreplaceability", 
            "text": "The irreplaceability utilised the methodology published at  Science .   The shortcoming of using the Red List range polygons is rather obvious: it does little to mitigate the effect of over-estimation of their Area of Occupancy (AOO) and that richness counts almost certainly inflate the actual number of species. The irreplaceability metric overcomes this by using sigmoid functions to translate actual proportional overlaps and aggregate to a single value that, to a certain extent, captures both richness and endemism.  The implementation can be found below ( source code )  \ndef species_irreplaceability(x):\n     x is the percentage in decimal \n\n    x = x*100\n\n    def h(x):\n    # h(x) as specified\n        miu = 39\n        s = 9.5\n        tmp = -(x - miu)/ s\n        denominator = 1 + np.exp(tmp)\n        return 1/denominator\n\n\n    return (h(x) - h(0))/(h(100) - h(0))  Because the irreplaceability analysis builds on the full intersection between the WDPA and the Red List (the pairwise percentage overlap value), which is in itself a massive undertaking, I cannot re-run irreplaceability without being given the result of the full intersection first. For this reason, up to this day, only two runs exist (the latest updated in 2015, which we rely on).", 
            "title": "Irreplaceability"
        }, 
        {
            "location": "/comparative-analysis/#static-maps", 
            "text": "Maps accompany the comparative analysis are authored using ArcMap, and later ArcPro.   The template is pre-authored and re-used across all maps, although labelling and styles may be dynamically adjusted. To the extent possible, I try to automate the production of maps. In ArcMap, this is done via  arcpy.mapping  package that produces maps by iterating each feature in the nomination feature class. The export process outputs any format supported by ArcMap, usually in PNG for the reports or AI/EPS/PDF for printing. The source code could be found in the  geoprocessing section  Later, new capabilities in ArcPro make the above process obsolete. There is no need any more to write any code. The iteration instead require a pre-cooked extent feature class be create to represent the viewport of maps. The default data-driven pages can loop through each extent feature and export maps of desired formats. Worth mentioning is that PDF format maps also have the ability to turn on and off layers, that in a way mimics behaviours of a dynamic maps, giving readers powers to interrogate the map themselves for information.   The 2017 static maps can be found below:   E:\\Yichuan\\Comparative_analysis_2017\\static_maps", 
            "title": "Static maps"
        }, 
        {
            "location": "/comparative-analysis/#interactive-maps", 
            "text": "TO BE ADDED", 
            "title": "Interactive maps"
        }, 
        {
            "location": "/comparative-analysis/#conclusion", 
            "text": "In hindsight, the analysis using intersections is slightly over-engineered. The original implementation relies entirely on SQLs, which had severe shortcomings that flow control was extremely inflexible in a pure SQL environment. Python, being a generic glue language, was picked to generate SQL statement and feed to the database management system. The result is a somewhat haphazard concoction of spaghetti codes, with unnecessarily engineered towards objective-oriented design.   For future improvement, a pure function based structure may be a better alternative. Or, considering the massive improvement of the  arcpy  library and the fine controls of geometry it now has, it may be wise to migrated back to an esri based system, which may be easier to manage, unifying the database management and the analysis. Further debate on pros and cons is required - going back to a solution based on proprietary software seems counter-intuitive in the present day of open data and open science.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/geoprocessing/", 
            "text": "Introduction\n\n\nThis section explains the \ntechnical\n side of my work, on analysis, maps and automation. \n\n\nThe reason why much of my time has gone into the development, improvement and maintenance of the scripts is simple - next time the same task is requested again, I have an existing method, or previously working solution, that could be re-used or easily adapted. This saves time and increases productivity\n\n\nWherever possible, I try to automate. In some cases, this obviously represents a poor investment of time, i.e., doing it manually step by step may be quicker. Automation for one-off tasks is over-engineering. I would argue, however, any time spent on making a tool, or spent on mastering a potentially more productive tool is a just investment that brings benefits in the long term. \n\n\nFor example, the different implementations of the comparative analysis have led to a reduced workload significantly. See below a graph depicting the amount of days to do the spatial overlay between the plethora of datasets and new nominations. This frees time for species richness, irreplaceability, improved maps, just to name a few.\n\n\n\n\nSome of the tasks present similar technical challenges. Especially for frequent, recurring tasks, it is imperative that I develop libraries or templates to minimise repetitive work, and that more time is made available for creative tasks and increased productivity. \n\n\nGeoprocessing libraries\n\n\nThe geoprocessing library  (located at \ngeoprocessing\\library\n) is a collection of commonly used functions, which evolve from specific solutions to tasks sharing similarities. Most of them relate to spatial analyses, usually involving the use of the \narcpy\n library, which requires the installation of ArcGIS and a license. \n\n\nIn order to use these libraries, the scripts need to be visible to Python. The easiest way to achieving this is to set system variable \nPYTHONPATH\n to include the \ngeoprocessing\\library\n path.\n\n\nThe library consist the follow components:\n\n\n\n\nYichuan10.py\n holds a collection of mostly utility functions (with a humble and long history, starting with ArcGIS version 10, which still reflects in the name of the library). Commonly used functions include the two \n'decorators'\n to track memory usage, and time required for executing functions, and the \nGetUniqueValuesFromFeatureLayer_mk2\n function that finds unique values in any given field in a feature layer. The latter simplifies the common task of finding unique IDs, such as the \nwdpaid\n for the WDPA or \nid_no\n for Red List.\n\n\n\n\n\ndef GetUniqueValuesFromFeatureLayer_mk2(inputFc, inputField):\n    \nstring\n, \nstring\n -\n pythonList\n    can be both feature class or feature layer\n\n    pySet = set()\n    with arcpy.da.SearchCursor(inputFc, inputField) as cursor:\n        for row in cursor:\n            pySet.add(row[0])\n\n    return list(pySet)\n\n\n\n\n\n\n\n\n\nYichuanDB.py\n includes a number of useful functions to connect and manipulate postgres databases. For the most part, I use the \nConnectionParameter\n class to simplify making connections to the postgres database. Convenient functions like \nclean_view\n enables testing and debugging of the comparative analyses, which relies on direct access and manipulation of data in the database.\n\n\n\n\n\n\nYichuanM.py\n evolved from a suite of utility functions originally resided in the \nYichuan10.py\n that specifically aimed at tasks relating to the automatic/batch production of maps (use pre-authored map template, to be discussed in detail in the \nmap batch template\n)\n\n\n\n\n\n\nYichuanRAS.py\n contains functions that manipulates raster datasets (the rest on vectors) using both the \narcpy\n but also \ngdal\n, an open source but more lower level alternative. Many of the functions here underpin the \nanalysis of landcover change\n which was mostly implemented using \ngdal\n\n\n\n\n\n\nYichuanSR.py\n has many functions relating to coordinate systems. For example, the function \nget_desirable_sr\n tries to guess a best optimal coordinate system based on the geometry (the input expects \narcpy.Geometry\n or \narcpy.Extent\n object). This is useful to identify the most appropriate coordinate system when automating map productions - it's never a good idea to use a global coordinate system for a site scale map - use a specific UTM instead! NB. This function pre-dates a similar \nCalculateUTMZone\n function, now included by default in newer releases of ArcGIS.\n\n\n\n\n\n\nMap batching template\n\n\nMaking maps quickly and automatically is essential. \n\n\nImagine a map production task for a paper. Apart from the initial cartographic process, there will be repeat requests to adjust layout, styles, and \nminor\n details that entail massive changes, unknown except to the author of the map. Additionally, Usually, more than one map needs adjusting. Finally when it comes to printing or publishing, it is quite common to expect a different format to the one at hand already. These are legitimate requests, but the time and efforts required to address these challenges tend to go unnoticed.\n\n\nThe map batching template attempts to offer a solution to make this process easier. These templates aim to address the two mundane scenarios below:\n\n\n\n\nfrom an ArcGIS Map Document (mxd file) to an exported map of a given format (e.g. pdf file). This is the same to someone clicking the export button and specify some export parameters\n\n\nfrom a single Map Document to a series of maps. Typically, for each feature of a given dataset, there will be an export map.\n\n\n\n\nThe underlying technical solution to the above two scenarios involves the \nYichuan10.ExportMXDtoMap\n function, which is a thin, for convenience wrapper around built-in map functions in the \narcpy.mapping\n module. \n\n\nThe process usually involves the below steps:\n\n\n\n\npre-author a map (no need to turn on data driven pages)\n\n\nensure there is an index layer with id, and map elements. They need to be specified and referenced in the map template (only applies to the second scenario).\n\n\n\n\nOne common task in the first scenario is to export a folder of mxd documents, repeatedly with multiple formats. This could be easily achieved with a loop. \n\n\nHere is an \nexample\n to export from a map document to a thumbnail, a map for web, and a map for use in Adobe Illustrator (AI format, for further editing and publication).\n\n\nThe second scenario requires a loop to go through the feature class. Depending on its attributes or geometry, the map will then be different, for instance, in the title, extent indicator, spatial reference or on/off of a particular layer. For example, the scenario may be to create a map per species or per protected areas. In this sense it is less flexible and needs to be adapted to account for unique requirements that cannot be fully captured in a single, reusable template.\n\n\nFor example, the \nsimple map batcher\n sets a definition query, a spatial reference, scale/extent, and then then export a jpg format map with a given resolution. See the snippet below\n\n\nexportpng = exportfolder + os.sep + str(each) + '.jpg'\n\nquery = '\\\nwdpaid\\\n = ' + str(each)\n\n# only the above wdpaid feature visible\nlayer_index.definitionQuery = query\n\n# set dataframe coordinate system\nsr =arcpy.SpatialReference()\n\nsr_string = Yichuan10.GetFieldValueByID_mk2(layer_index, each, value_field='utm')\n\nsr.loadFromString(sr_string)\n\n# load from its attribute\ndf.spatialReference = sr\n\ndf.extent = layer_index.getExtent()\ndf.scale = df.scale * 1.1\n\n# need to specify a low quality\narcpy.mapping.ExportToJPEG(mxd, exportpng, \nPAGE_LAYOUT\n, resolution = reso, jpeg_quality=60)\n\n\n\n\nN.B. at the end of iteration, it is good practice to clear the definition query by specifying it to empty (e.g. in the above case, \nlayer_index.definitionQuery = ''\n)\n\n\nFinally, it is not necessary to adapt the map batch template to automate the production of maps. ArcGIS has a built-in tool call \ndata drive pages\n or \nMap series\n in ArcPro. Their use is beyond the scope of this handover. In comparison, data drive pages do not give a fine control, nor the same level of customisation that I required for my map production process.\n\n\nRichness template\n\n\nThe origin of this template goes back to 2010 when I was tasked with calculating richness (counting number of species) in the global hexagon grid. This forms part of the analysis published in paper titled \nThe Impact of Conservation on the Status of the World\u2019s Vertebrates\n. I re-wrote and improved in Python from the original code written in VBA (by Nick?).\n\n\nThe aim is to simply count the number of overlaying species in a given base layer, be it hexagons or protected areas. While conceptually simple, the implementation may not be straightforward - the beauty of this implementation in my opinion is two fold:\n\n\n\n\nonly perform an intersection test\n\n\nonly minimal information is recorded\n\n\n\n\nThis means it does not require the computationally intensive calculation of actual intersections (think of \nst_intersects\n rather than \nst_intersection\n, see \nPostGIS documentation\n), and it only records IDs, minimal information needed for subsequent analysis.\n\n\nThe current implementation relies on the iterative \narcpy.SelectLayerByLocation_management\n calls between a species layer and a base layer. For efficiency reasons, the species layer is created within each iteration by \narcpy.MakeFeatureLayer_management\n with a given ID, and deleted after use; on the contrary, the base layer is re-used and resides in the memory persistently, until all iterations are complete. After the base layer is successfully 'selected', a data cursor loops through its record (\nGetUniqueValuesFromFeatureLayer_mk2\n works on a feature layer, and if that layer has a selection, it will only work on the selection) and saved as a list of strings (each representing a species id and a base layer id). It is formatted for eventual export to a csv file.\n\n\ndef species_richness_calculation(id, hexagonLyr):\n\n    # make species layer\n    if type(id) in [str, unicode]:\n        exp = '\\\n' + speciesID + '\\\n = ' + '\\'' + str(id) + '\\''\n    elif type(id) in [int, float]:\n        exp = '\\\n' + speciesID + '\\\n = ' + str(id)\n    else:\n        raise Exception('ID field type error')\n\n    # make layers\n    arcpy.MakeFeatureLayer_management(speciesData, speciesLyr, exp)\n\n    # select by locations\n    arcpy.SelectLayerByLocation_management(hexagonLyr, overLapOption, speciesLyr)\n\n    # record it\n    hex_ids = GetUniqueValuesFromFeatureLayer_mk2(hexagonLyr, hexagonID)\n\n    result = list()\n\n    for hex_id in hex_ids:\n        result.append(str(int(id)) + ',' + str(hex_id) + '\\n')\n\n    # get rid of layers\n    arcpy.Delete_management(speciesLyr)\n\n    return result\n\n\n\n\nMore recently, this template has been improved to utilise parallel processing to reduce time. ArcGIS cannot use more than one core at a time, so the challenge dictates dividing the task into smaller chunks for multiple ArcGIS processes, and then finally stitch together. The first parallel solution simply uses the \nparallel template\n, but later solutions incorporate further optimisation adjustment in the template to improve efficiency and add additional functionality. More specifically, it involves:\n\n\n\n\nLogger. Analysis with large, multi-source spatial data almost inevitably fail.\n\n\nMove the making layer logic to the worker. Making layer is expensive (time-consuming) - it is much fast for worker process to have a layer that they could re-use, without having to re-create for each iteration. It's a compromise between keep task-specific logic from the multi-processing template and achieving better efficiency.\n\n\n\n\nThe original worker function\n\n\ndef worker(q, q_out):\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print 'Remaining jobs:', q.qsize()\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = job(job_id)\n        q_out.put(result)\n\n\n\n\nThe adapted worker function. Note the \narcpy.MakeFeatureLayer_management\n and \narcpy.Delete_management\n that would otherwise occur in the \nspecies_richness_calculation\n function.\n\n\ndef worker(q, q_out):\n    # make layer here to reduce overhead\n    arcpy.MakeFeatureLayer_management(hexagonData, hexagonLyr)\n\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print('Time:', datetime.datetime.now().time())\n            print('Remaining jobs:', q.qsize())\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = species_richness_calculation(job_id, hexagonLyr)\n        q_out.put(result)\n\n    arcpy.Delete_management(hexagonLyr)\n\n\n\n\nThe reduction in the time needed to undertake species richness analysis is significant - not quite linear reduction with the increase of CPU cores, but for my computer of 6 cores (12 threads) when specifying 10 worker process, it sees 7-8 times improvement. For example, the annual richness calculation itself for nomination + existing WH sites takes less than 3 hours, a previously unthinkable performance.\n\n\nFurthermore, due to its generic use case, attempts are being made to facilitate a uniform overlapping/binning process, with promising potentials.\n\n\nFor example, by running both the richness calculation between WDPA and the global hexagon grid, and between RedList and the global hexagon grid, the intersection between WDPA and RedList could be easily obtained by using the grid as a look up table. This eliminates the need for complicated spatial overlays. Any further calculations have now reduced to non-spatial tabulation/pivot tables. \n\n\nThe added benefits also include deferring decisions on groupings (for example, threatened species, or country/regional level statistics) towards a later time without re-doing the spatial overlay. This requires analysis be designed in the lowest possible granularity, that allows separation. \n\n\nFor example, if a decision to select species range based on presence, origin and seasonality is to be deferred, the analysis must use the row id (usual \nfid\n) instead of the species id \nid_no\n. This is because \nid_no\n will make no effort to separate between polygons with presence 1 or presence 2 - it only differentiates species\n\n\nThe \nbiodiversity for food and agriculture analysis/BFA FAO analysis\n is a good example with this thinking in mind(\nscript\n)\n\n\nParallel template\n\n\nThe idea for this template comes from the need to speed up slow-running processes. In the old days, the richness calculation can be painstakingly slow: cutting the tasks to smaller chunks to run over a number of machines over a number of days is not a pleasant experience. Even with a small subset of protected areas, running for all Red List species remains a challenge. \n\n\nOne of the bottleneck was that of the single process nature of ArcGIS Desktop - it does not take advantage of modern multi-core computers. The parallel template then started as a way of using Python to deploy multiple ArcGIS processes to distribute workload automatically.\n\n\nThe process goes like:\n1. Create an input pipe containing a list of unique IDs. Each ID represents a piece of data\n2. Create an initially empty output pipe hosting result (will come from the worker processes)\n3. Create a number of worker processes, that actively get data from IDs in the input pip\n4. Create a worker process, that listens actively for get results from the output pipe and then processes them (write to an output file)\n5. The main function manages the pipes: distributes work, logs and waits for completion\n\n\nThis is the link to the template\n\n\nNotably, it needs to add the text string 'STOP' (or for that matter any flag signal would do as long as it is what the worker process would watch out for) to the input queue.\n\n\n# Add queue of a list of ids to process\nq = get_queue()\n\n# setup and run worker processes\np_workers = list()\nfor i in range(WORKER):\n    print 'Starting worker process:', i\n    p = multiprocessing.Process(target=worker, args=(q, q_out))\n    p_workers.append(p)\n\n# start\nfor p in p_workers:\n    p.start()\n\n# add stop flag to the queue\nfor p in p_workers:\n    q.put('STOP')\n\n\n\n\nThe worker process then remains alive until it sees the 'STOP' signal to terminate.\n\n\ndef worker(q, q_out):\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print 'Remaining jobs:', q.qsize()\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = job(job_id)\n        q_out.put(result)\n\n\n\n\nNew data analysis process\n\n\nIt is not really 'new' - but I still have not seen a wide adoption of this novel process being applied in conservation analysis at IUCN or WCMC. \n\n\nThis is an improved work flow that I increasingly use. Typically it begins with a spatial analysis, but only once. All subsequent analyses and visualisation then take place within a \nJupyter notebook\n. For routine analyses, this process seems to work very well, as long as the initial spatial analysis is designed in such a way (with the smallest granularity) that allow further interrogation of data with non-spatial analyses.\n\n\nThe benefits of this process:\n\n\n\n\nless error prone as a result from spatial analysis. Spatial analysis is done only once at the smallest granularity\n\n\nfurther questions of different aggregations can be answered with a non-spatial analysis (as long as the scale is higher than what's used in the spatial analysis)\n\n\nmethodology is fully open, and reproducible with little effort (apart from the initial spatial analysis)\n\n\nthe entire analytical process: data cleaning, analysis, and visualisation is fully documented and can easily be shared\n\n\n\n\nExample analyses:\n\n\n\n\nClimate change vulnerability analysis\n and the \nwriting of report\n\n\nWorld Heritage Outlook 2 analysis\n\n\nBiodiversity for food and agriculture\n\n\nWilderness analysis\n and the \nwriting of methodology\n\n\nICCA species richness\n\n\n\n\nArcGIS and Anaconda\n\n\nOf course, there is no reason why the spatial analysis should be excluded from this process. Thus, the entire work flow of data, analysis and result could be theoretically included in a single workspace such as a Jupyter notebook. \n\n\nThe main challenge is for the generic Python and its libraries to talk to ArcGIS and vice versa. The problem is that ArcGIS uses a specific version of python and its numerical libraries, which cannot be updated without breaking the ArcGIS installation (when using \narcpy\n for example).\n\n\nAnaconda\n, the most popular Python data science platform, can be used to create an environment to satisfy a specific installation of ArcGIS with compatible versions of other libraries.\n\n\nUSGS has a useful guide on \nhow to use anaconda modules from the esri Python environment\n to set up such a environment.\n\n\nLastly, this may work for small spatial analyses, however, I would argue for the geoprocessing we require (takes hours or even days) this may not be a good idea. Spatial as such analyse tends to fail a couple of times and require significant back and forth manual experimenting and fixing of data - these would be better off dealt with separately.\n\n\nMiscellaneous", 
            "title": "Geoprocessing"
        }, 
        {
            "location": "/geoprocessing/#introduction", 
            "text": "This section explains the  technical  side of my work, on analysis, maps and automation.   The reason why much of my time has gone into the development, improvement and maintenance of the scripts is simple - next time the same task is requested again, I have an existing method, or previously working solution, that could be re-used or easily adapted. This saves time and increases productivity  Wherever possible, I try to automate. In some cases, this obviously represents a poor investment of time, i.e., doing it manually step by step may be quicker. Automation for one-off tasks is over-engineering. I would argue, however, any time spent on making a tool, or spent on mastering a potentially more productive tool is a just investment that brings benefits in the long term.   For example, the different implementations of the comparative analysis have led to a reduced workload significantly. See below a graph depicting the amount of days to do the spatial overlay between the plethora of datasets and new nominations. This frees time for species richness, irreplaceability, improved maps, just to name a few.   Some of the tasks present similar technical challenges. Especially for frequent, recurring tasks, it is imperative that I develop libraries or templates to minimise repetitive work, and that more time is made available for creative tasks and increased productivity.", 
            "title": "Introduction"
        }, 
        {
            "location": "/geoprocessing/#geoprocessing-libraries", 
            "text": "The geoprocessing library  (located at  geoprocessing\\library ) is a collection of commonly used functions, which evolve from specific solutions to tasks sharing similarities. Most of them relate to spatial analyses, usually involving the use of the  arcpy  library, which requires the installation of ArcGIS and a license.   In order to use these libraries, the scripts need to be visible to Python. The easiest way to achieving this is to set system variable  PYTHONPATH  to include the  geoprocessing\\library  path.  The library consist the follow components:   Yichuan10.py  holds a collection of mostly utility functions (with a humble and long history, starting with ArcGIS version 10, which still reflects in the name of the library). Commonly used functions include the two  'decorators'  to track memory usage, and time required for executing functions, and the  GetUniqueValuesFromFeatureLayer_mk2  function that finds unique values in any given field in a feature layer. The latter simplifies the common task of finding unique IDs, such as the  wdpaid  for the WDPA or  id_no  for Red List.   \ndef GetUniqueValuesFromFeatureLayer_mk2(inputFc, inputField):\n     string ,  string  -  pythonList\n    can be both feature class or feature layer \n    pySet = set()\n    with arcpy.da.SearchCursor(inputFc, inputField) as cursor:\n        for row in cursor:\n            pySet.add(row[0])\n\n    return list(pySet)    YichuanDB.py  includes a number of useful functions to connect and manipulate postgres databases. For the most part, I use the  ConnectionParameter  class to simplify making connections to the postgres database. Convenient functions like  clean_view  enables testing and debugging of the comparative analyses, which relies on direct access and manipulation of data in the database.    YichuanM.py  evolved from a suite of utility functions originally resided in the  Yichuan10.py  that specifically aimed at tasks relating to the automatic/batch production of maps (use pre-authored map template, to be discussed in detail in the  map batch template )    YichuanRAS.py  contains functions that manipulates raster datasets (the rest on vectors) using both the  arcpy  but also  gdal , an open source but more lower level alternative. Many of the functions here underpin the  analysis of landcover change  which was mostly implemented using  gdal    YichuanSR.py  has many functions relating to coordinate systems. For example, the function  get_desirable_sr  tries to guess a best optimal coordinate system based on the geometry (the input expects  arcpy.Geometry  or  arcpy.Extent  object). This is useful to identify the most appropriate coordinate system when automating map productions - it's never a good idea to use a global coordinate system for a site scale map - use a specific UTM instead! NB. This function pre-dates a similar  CalculateUTMZone  function, now included by default in newer releases of ArcGIS.", 
            "title": "Geoprocessing libraries"
        }, 
        {
            "location": "/geoprocessing/#map-batching-template", 
            "text": "Making maps quickly and automatically is essential.   Imagine a map production task for a paper. Apart from the initial cartographic process, there will be repeat requests to adjust layout, styles, and  minor  details that entail massive changes, unknown except to the author of the map. Additionally, Usually, more than one map needs adjusting. Finally when it comes to printing or publishing, it is quite common to expect a different format to the one at hand already. These are legitimate requests, but the time and efforts required to address these challenges tend to go unnoticed.  The map batching template attempts to offer a solution to make this process easier. These templates aim to address the two mundane scenarios below:   from an ArcGIS Map Document (mxd file) to an exported map of a given format (e.g. pdf file). This is the same to someone clicking the export button and specify some export parameters  from a single Map Document to a series of maps. Typically, for each feature of a given dataset, there will be an export map.   The underlying technical solution to the above two scenarios involves the  Yichuan10.ExportMXDtoMap  function, which is a thin, for convenience wrapper around built-in map functions in the  arcpy.mapping  module.   The process usually involves the below steps:   pre-author a map (no need to turn on data driven pages)  ensure there is an index layer with id, and map elements. They need to be specified and referenced in the map template (only applies to the second scenario).   One common task in the first scenario is to export a folder of mxd documents, repeatedly with multiple formats. This could be easily achieved with a loop.   Here is an  example  to export from a map document to a thumbnail, a map for web, and a map for use in Adobe Illustrator (AI format, for further editing and publication).  The second scenario requires a loop to go through the feature class. Depending on its attributes or geometry, the map will then be different, for instance, in the title, extent indicator, spatial reference or on/off of a particular layer. For example, the scenario may be to create a map per species or per protected areas. In this sense it is less flexible and needs to be adapted to account for unique requirements that cannot be fully captured in a single, reusable template.  For example, the  simple map batcher  sets a definition query, a spatial reference, scale/extent, and then then export a jpg format map with a given resolution. See the snippet below  exportpng = exportfolder + os.sep + str(each) + '.jpg'\n\nquery = '\\ wdpaid\\  = ' + str(each)\n\n# only the above wdpaid feature visible\nlayer_index.definitionQuery = query\n\n# set dataframe coordinate system\nsr =arcpy.SpatialReference()\n\nsr_string = Yichuan10.GetFieldValueByID_mk2(layer_index, each, value_field='utm')\n\nsr.loadFromString(sr_string)\n\n# load from its attribute\ndf.spatialReference = sr\n\ndf.extent = layer_index.getExtent()\ndf.scale = df.scale * 1.1\n\n# need to specify a low quality\narcpy.mapping.ExportToJPEG(mxd, exportpng,  PAGE_LAYOUT , resolution = reso, jpeg_quality=60)  N.B. at the end of iteration, it is good practice to clear the definition query by specifying it to empty (e.g. in the above case,  layer_index.definitionQuery = '' )  Finally, it is not necessary to adapt the map batch template to automate the production of maps. ArcGIS has a built-in tool call  data drive pages  or  Map series  in ArcPro. Their use is beyond the scope of this handover. In comparison, data drive pages do not give a fine control, nor the same level of customisation that I required for my map production process.", 
            "title": "Map batching template"
        }, 
        {
            "location": "/geoprocessing/#richness-template", 
            "text": "The origin of this template goes back to 2010 when I was tasked with calculating richness (counting number of species) in the global hexagon grid. This forms part of the analysis published in paper titled  The Impact of Conservation on the Status of the World\u2019s Vertebrates . I re-wrote and improved in Python from the original code written in VBA (by Nick?).  The aim is to simply count the number of overlaying species in a given base layer, be it hexagons or protected areas. While conceptually simple, the implementation may not be straightforward - the beauty of this implementation in my opinion is two fold:   only perform an intersection test  only minimal information is recorded   This means it does not require the computationally intensive calculation of actual intersections (think of  st_intersects  rather than  st_intersection , see  PostGIS documentation ), and it only records IDs, minimal information needed for subsequent analysis.  The current implementation relies on the iterative  arcpy.SelectLayerByLocation_management  calls between a species layer and a base layer. For efficiency reasons, the species layer is created within each iteration by  arcpy.MakeFeatureLayer_management  with a given ID, and deleted after use; on the contrary, the base layer is re-used and resides in the memory persistently, until all iterations are complete. After the base layer is successfully 'selected', a data cursor loops through its record ( GetUniqueValuesFromFeatureLayer_mk2  works on a feature layer, and if that layer has a selection, it will only work on the selection) and saved as a list of strings (each representing a species id and a base layer id). It is formatted for eventual export to a csv file.  def species_richness_calculation(id, hexagonLyr):\n\n    # make species layer\n    if type(id) in [str, unicode]:\n        exp = '\\ ' + speciesID + '\\  = ' + '\\'' + str(id) + '\\''\n    elif type(id) in [int, float]:\n        exp = '\\ ' + speciesID + '\\  = ' + str(id)\n    else:\n        raise Exception('ID field type error')\n\n    # make layers\n    arcpy.MakeFeatureLayer_management(speciesData, speciesLyr, exp)\n\n    # select by locations\n    arcpy.SelectLayerByLocation_management(hexagonLyr, overLapOption, speciesLyr)\n\n    # record it\n    hex_ids = GetUniqueValuesFromFeatureLayer_mk2(hexagonLyr, hexagonID)\n\n    result = list()\n\n    for hex_id in hex_ids:\n        result.append(str(int(id)) + ',' + str(hex_id) + '\\n')\n\n    # get rid of layers\n    arcpy.Delete_management(speciesLyr)\n\n    return result  More recently, this template has been improved to utilise parallel processing to reduce time. ArcGIS cannot use more than one core at a time, so the challenge dictates dividing the task into smaller chunks for multiple ArcGIS processes, and then finally stitch together. The first parallel solution simply uses the  parallel template , but later solutions incorporate further optimisation adjustment in the template to improve efficiency and add additional functionality. More specifically, it involves:   Logger. Analysis with large, multi-source spatial data almost inevitably fail.  Move the making layer logic to the worker. Making layer is expensive (time-consuming) - it is much fast for worker process to have a layer that they could re-use, without having to re-create for each iteration. It's a compromise between keep task-specific logic from the multi-processing template and achieving better efficiency.   The original worker function  def worker(q, q_out):\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print 'Remaining jobs:', q.qsize()\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = job(job_id)\n        q_out.put(result)  The adapted worker function. Note the  arcpy.MakeFeatureLayer_management  and  arcpy.Delete_management  that would otherwise occur in the  species_richness_calculation  function.  def worker(q, q_out):\n    # make layer here to reduce overhead\n    arcpy.MakeFeatureLayer_management(hexagonData, hexagonLyr)\n\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print('Time:', datetime.datetime.now().time())\n            print('Remaining jobs:', q.qsize())\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = species_richness_calculation(job_id, hexagonLyr)\n        q_out.put(result)\n\n    arcpy.Delete_management(hexagonLyr)  The reduction in the time needed to undertake species richness analysis is significant - not quite linear reduction with the increase of CPU cores, but for my computer of 6 cores (12 threads) when specifying 10 worker process, it sees 7-8 times improvement. For example, the annual richness calculation itself for nomination + existing WH sites takes less than 3 hours, a previously unthinkable performance.  Furthermore, due to its generic use case, attempts are being made to facilitate a uniform overlapping/binning process, with promising potentials.  For example, by running both the richness calculation between WDPA and the global hexagon grid, and between RedList and the global hexagon grid, the intersection between WDPA and RedList could be easily obtained by using the grid as a look up table. This eliminates the need for complicated spatial overlays. Any further calculations have now reduced to non-spatial tabulation/pivot tables.   The added benefits also include deferring decisions on groupings (for example, threatened species, or country/regional level statistics) towards a later time without re-doing the spatial overlay. This requires analysis be designed in the lowest possible granularity, that allows separation.   For example, if a decision to select species range based on presence, origin and seasonality is to be deferred, the analysis must use the row id (usual  fid ) instead of the species id  id_no . This is because  id_no  will make no effort to separate between polygons with presence 1 or presence 2 - it only differentiates species  The  biodiversity for food and agriculture analysis/BFA FAO analysis  is a good example with this thinking in mind( script )", 
            "title": "Richness template"
        }, 
        {
            "location": "/geoprocessing/#parallel-template", 
            "text": "The idea for this template comes from the need to speed up slow-running processes. In the old days, the richness calculation can be painstakingly slow: cutting the tasks to smaller chunks to run over a number of machines over a number of days is not a pleasant experience. Even with a small subset of protected areas, running for all Red List species remains a challenge.   One of the bottleneck was that of the single process nature of ArcGIS Desktop - it does not take advantage of modern multi-core computers. The parallel template then started as a way of using Python to deploy multiple ArcGIS processes to distribute workload automatically.  The process goes like:\n1. Create an input pipe containing a list of unique IDs. Each ID represents a piece of data\n2. Create an initially empty output pipe hosting result (will come from the worker processes)\n3. Create a number of worker processes, that actively get data from IDs in the input pip\n4. Create a worker process, that listens actively for get results from the output pipe and then processes them (write to an output file)\n5. The main function manages the pipes: distributes work, logs and waits for completion  This is the link to the template  Notably, it needs to add the text string 'STOP' (or for that matter any flag signal would do as long as it is what the worker process would watch out for) to the input queue.  # Add queue of a list of ids to process\nq = get_queue()\n\n# setup and run worker processes\np_workers = list()\nfor i in range(WORKER):\n    print 'Starting worker process:', i\n    p = multiprocessing.Process(target=worker, args=(q, q_out))\n    p_workers.append(p)\n\n# start\nfor p in p_workers:\n    p.start()\n\n# add stop flag to the queue\nfor p in p_workers:\n    q.put('STOP')  The worker process then remains alive until it sees the 'STOP' signal to terminate.  def worker(q, q_out):\n    while True:\n        # monitoring\n        if q.qsize() %100 == 0:\n            print 'Remaining jobs:', q.qsize()\n\n        # get and ID from job id queue\n        job_id = q.get()\n        if job_id == 'STOP':\n            break\n\n        result = job(job_id)\n        q_out.put(result)", 
            "title": "Parallel template"
        }, 
        {
            "location": "/geoprocessing/#new-data-analysis-process", 
            "text": "It is not really 'new' - but I still have not seen a wide adoption of this novel process being applied in conservation analysis at IUCN or WCMC.   This is an improved work flow that I increasingly use. Typically it begins with a spatial analysis, but only once. All subsequent analyses and visualisation then take place within a  Jupyter notebook . For routine analyses, this process seems to work very well, as long as the initial spatial analysis is designed in such a way (with the smallest granularity) that allow further interrogation of data with non-spatial analyses.  The benefits of this process:   less error prone as a result from spatial analysis. Spatial analysis is done only once at the smallest granularity  further questions of different aggregations can be answered with a non-spatial analysis (as long as the scale is higher than what's used in the spatial analysis)  methodology is fully open, and reproducible with little effort (apart from the initial spatial analysis)  the entire analytical process: data cleaning, analysis, and visualisation is fully documented and can easily be shared   Example analyses:   Climate change vulnerability analysis  and the  writing of report  World Heritage Outlook 2 analysis  Biodiversity for food and agriculture  Wilderness analysis  and the  writing of methodology  ICCA species richness", 
            "title": "New data analysis process"
        }, 
        {
            "location": "/geoprocessing/#arcgis-and-anaconda", 
            "text": "Of course, there is no reason why the spatial analysis should be excluded from this process. Thus, the entire work flow of data, analysis and result could be theoretically included in a single workspace such as a Jupyter notebook.   The main challenge is for the generic Python and its libraries to talk to ArcGIS and vice versa. The problem is that ArcGIS uses a specific version of python and its numerical libraries, which cannot be updated without breaking the ArcGIS installation (when using  arcpy  for example).  Anaconda , the most popular Python data science platform, can be used to create an environment to satisfy a specific installation of ArcGIS with compatible versions of other libraries.  USGS has a useful guide on  how to use anaconda modules from the esri Python environment  to set up such a environment.  Lastly, this may work for small spatial analyses, however, I would argue for the geoprocessing we require (takes hours or even days) this may not be a good idea. Spatial as such analyse tends to fail a couple of times and require significant back and forth manual experimenting and fixing of data - these would be better off dealt with separately.", 
            "title": "ArcGIS and Anaconda"
        }, 
        {
            "location": "/geoprocessing/#miscellaneous", 
            "text": "", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/", 
            "text": "Idea\n\n\nLet's face it, research and analytical outputs are generally not interesting - at least not so in their original form. The scientifically accurate language appeals only to very few people. To most of us, reading a thick reasoning paper with lots of numbers poses an intellectual overhead that is not always pleasant and welcome, especially when you are busy. This is true in my own experience - I often argue that if I as an author cannot be 'bothered' to read my own works of 70 pages (I won't), how can I convince those who do not have a vested interest to read? \n\n\nThis needs to change, if our work is to reach more people, make it easy, and reap more impact. \n\n\nFortunately the problem is not uniquely ours - in fact it is a well trodden issue and there are good solutions. I would argue that in order to capture those with very limited attention span we need a new media, and going digital by default is one way to make the information more accessible and perhaps, by doing so, more interesting. For example, the \nUK government digital service\n offers some practical suggestions.\n\n\nThe knowledge lab, or the World Heritage Analysis which came to be known afterwards, represents my effort to put this thinking into action, to deliver the tasks I was leading under the Brighter Outlook project.\n\n\nHere are a few links on the thinking:\n\n\n\n\nWorld Heritage Analysis page on IUCN\n\n\nGitHub\n\n\n\n\nPlatform\n\n\nWhile far from being complete, the World Heritage Analysis (hereafter refer to as the Lab) is not just a portal website with links pointing to each product - it is designed to be the central place to foster a mechanism that allows ideas to be prototyped, tested, scrutinised and, as a platform to raise funds for their eventual departure to become fully fledged product or services.\n\n\nAs it stands, it lacks the central pull factor to bring in more traffic and thus potential interest. This may be due to the lack of a usable feedback system, a small scope that appeals to a small audience and possibly also lacking effective promotion. \n\n\nMost of the traffic come from new users, and a large majority visit the \ndatasheets\n, a well established product before its appearance as an online service\n\n\n\n\nCurrent implementation\n\n\nIt is written in pure HTML and CSS, and currently hosted by GitHub pages.\n\n\nFull documentation of the World Heritage Analysis can be found on the GitHub below.\n\n\n\n\nGitHub repository for the World Heritage Analysis\n\n\n\n\nLinks to prototypes\n\n\n\n\n\n\nLand cover change website\n | \nSource on Github\n\n\n\n\n\n\nForest Loss\n | \nSource on Github\n\n\n\n\n\n\nHuman Footprint Change\n | \nSource on Github\n\n\n\n\n\n\nClimate change vulnerability\n | \nmethodology\n and \nreport\n | \nSource on Github\n\n\n\n\n\n\nLandsat 8 imagery for natural World Heritage\n | \nSource on Github\n\n\n\n\n\n\nNatural World Heritage Viewer\n | \nesri Feature Service\n | \nREST end point\n\n\n\n\n\n\nComparative analysis\n | \nSource on Github\n\n\n\n\n\n\nInformation sheet\n | \nSource on Github\n\n\n\n\n\n\nGlobal surface water\n | \nSource on Github", 
            "title": "World Heritage Analyses"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#idea", 
            "text": "Let's face it, research and analytical outputs are generally not interesting - at least not so in their original form. The scientifically accurate language appeals only to very few people. To most of us, reading a thick reasoning paper with lots of numbers poses an intellectual overhead that is not always pleasant and welcome, especially when you are busy. This is true in my own experience - I often argue that if I as an author cannot be 'bothered' to read my own works of 70 pages (I won't), how can I convince those who do not have a vested interest to read?   This needs to change, if our work is to reach more people, make it easy, and reap more impact.   Fortunately the problem is not uniquely ours - in fact it is a well trodden issue and there are good solutions. I would argue that in order to capture those with very limited attention span we need a new media, and going digital by default is one way to make the information more accessible and perhaps, by doing so, more interesting. For example, the  UK government digital service  offers some practical suggestions.  The knowledge lab, or the World Heritage Analysis which came to be known afterwards, represents my effort to put this thinking into action, to deliver the tasks I was leading under the Brighter Outlook project.  Here are a few links on the thinking:   World Heritage Analysis page on IUCN  GitHub", 
            "title": "Idea"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#platform", 
            "text": "While far from being complete, the World Heritage Analysis (hereafter refer to as the Lab) is not just a portal website with links pointing to each product - it is designed to be the central place to foster a mechanism that allows ideas to be prototyped, tested, scrutinised and, as a platform to raise funds for their eventual departure to become fully fledged product or services.  As it stands, it lacks the central pull factor to bring in more traffic and thus potential interest. This may be due to the lack of a usable feedback system, a small scope that appeals to a small audience and possibly also lacking effective promotion.   Most of the traffic come from new users, and a large majority visit the  datasheets , a well established product before its appearance as an online service", 
            "title": "Platform"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#current-implementation", 
            "text": "It is written in pure HTML and CSS, and currently hosted by GitHub pages.  Full documentation of the World Heritage Analysis can be found on the GitHub below.   GitHub repository for the World Heritage Analysis", 
            "title": "Current implementation"
        }, 
        {
            "location": "/world-heritage-knowledge-lab/#links-to-prototypes", 
            "text": "Land cover change website  |  Source on Github    Forest Loss  |  Source on Github    Human Footprint Change  |  Source on Github    Climate change vulnerability  |  methodology  and  report  |  Source on Github    Landsat 8 imagery for natural World Heritage  |  Source on Github    Natural World Heritage Viewer  |  esri Feature Service  |  REST end point    Comparative analysis  |  Source on Github    Information sheet  |  Source on Github    Global surface water  |  Source on Github", 
            "title": "Links to prototypes"
        }, 
        {
            "location": "/world-heritage-outlook/", 
            "text": "History\n\n\nI built the first prototype of the Outlook assessment module in Python (Web2Py framework), as proof of concept. The underlying principle is to store the results of assessment in a database so that it could be managed and analysed more easily and effectively. A database approach is the only way to systematically manage the data the project generates.\n\n\nFor the development, RSMI was commissioned to undertake the design, implementation and maintenance of the three modules: assessment, site information, and front-end website for showcasing findings. In hindsight, it was an unrealistic ambition with impossible funds, but at the time the can-do spirit prevailed and I was technically in-mature and inexperienced as to make do with developing everything, rather than reality check and focusing on the minimally viable product.\n\n\nIn hindsight, part of the reason was that the roles were never clear, I think I was given the role of project management, yet I had no power in making design choices nor allocate funds. My role became an inefficient intermediate - I led numerous consultations with the team during every milestone, and had constant struggles to accommodate opinions and then feed to the developers. The decision making process was unnecessarily long and painful. The team was tired and so was I. \n\n\nDespite the hiccups along the way, a product was developed with all three modules, albeit late. The database played a major role to fulfilling the requirement of data analysis. I built the pipelines from database dumps to queries that output assessments, and then turn into usable formats. These powered the first outlook report.\n\n\nMy role in the second development is a technical advisory one and not involved in the daily management of the development.\n\n\nTechnical architecture\n\n\nThe assessment module is written in Java, a custom-made system that accepts assessments via the web and store in a PostgreSQL database. The user facing front end website displays results of assessment and was implemented in Liferay, which during the second development changed to a Drupal based system (written in PHP).\n\n\nAt the moment, there is a disconnection between the two systems: assessment and display. The assessment module and the old website is on one Amazon EC2 instance (instance name: 'IUCN WHP WHO') and the new, updated website resides another (instance name: 'IUCN WHO V2'). The new website used a database dump to migrate content from the old system. By design, the pipeline built by EDW does not reverse the flow of data in the other direction.\n\n\nThe source codes can be found on \nGitHub\n\n\nAdministration\n\n\nAs mentioned previously, the assessment module and the front facing website displaying results of assessment are hosted by Amazon (Amazon Web Services, AWS), and the servers are known as EC2 instances or servers. The old and new websites are of the type 'medium' (used to describe the capacity of the server or instance), physically in the 'eu-west-1a' region (in Ireland). Additionally, there is a 'micro' instance for the developers called 'IUCN WHO test', who used it for testing. It was decided that this 'micro instance' will be kept for future use.\n\n\n\n\nThe two additional 'micro' instances are used for hosting 1) the prototype of the World Heritage comparative analysis , 'WH CA' (seed funded by UNESCO WHC) and 2) some products on the World Heritage Analyses, 'WH APP'. \n\n\nIn order to assess these instances (use SSH), you will need 'private keys'. The keys are located at (WCMC-PC-01918)\n\n\n\n\nE:\\Yichuan\\Yichuan\\amazon_putty_ssh\n\n\n\n\nThe keys may be revoked in the \nAWS management console\n. The login credentials are:\n\n\n\n\nYichuan.shi@iucn.org\n\n\n(see my last email)\n\n\n\n\nYou may copy and send 'keys' to developers so that they have access to the servers, however, I would highly recommend \nnot\n giving external people access to our Amazon AWS account.\n\n\nThe management of the instances are beyond the scope of this documentation, however, the below are the most commonly used features:\n\n\n\n\ncreate new instances\n\n\nthe IP address of the instance (address for access)\n\n\nthe private keys (key for access)\n\n\nthe ports that need to be open (TCP22 for SSH, and TCP80 and 443 for HTTP/S)\n\n\n\n\nDatabase dump\n\n\nOne of the key function of the system(s) is to enable the extraction and analysis of information for the World Heritage Outlook report. For that purpose, it is important to obtain a copy of the dataset in the live database behind the website (but \nnever\n work on the live database itself!). This is called a database dump.\n\n\nFor security reasons, you will need to SSH tunnel to the instance where the database is located and forward the remote port to local. There is a saved session called \nWHO_backend\n that contains necessary connection information and also forwards the remote 5432 port (the database port) to the local 5433 port. \n\n\n\n\nOnce you've successfully logged using that saved session, open PgAdmin, the database management tool to connect to your local port 5433, which now points to the remote machine via SSH tunnel. \n\n\n\n\nYou may now select the database from which you'd like to make a dump. Use the 'backup' button (see red arrow) to create a file which holds the content of the database.\n\n\n\n\nOn your local machine, create an empty new database and then 'restore' (see the blue arrow) to create an identical copy of the remote database. \n\n\nThis concludes the database dump or 'extraction of data'.\n\n\nConvert to Access database\n\n\nThe data in the database dump is a replica of the database serving the website. Unfortunately the data is not stored in a format that allows us to gain useful insight, as bits of information scatter throughout numerous tables. You may refer to the diagram for additional information\n\n\n\n\nE:\\Yichuan\\IT\\ER_diagram_130802\n\n\n\n\nFor the most commonly used queries, I have built a re-usable Microsoft Access database that connects to the PostgreSQL database. This allows also to export to more familiar excel spreadsheet people find easy to work with.\n\n\n\n\nE:\\Yichuan\\Elena\\WHOA_171027\n\n\n\n\n\n\n!IMPORTANT!\n \n\n\nThere is one manual step that needs to be performed to assist the analysis before updating the links. \n\n\nWe need to differentiate the most recent assessment ids, and separate them for the two assessment cycles. This is essential to pull out the correct versions of the assessment for analysis. This step can be done in Access but would be much easier in the PostgreSQL database. In your newly restored database, create a view called 'z_wdpa_latest' using the below:\n\n\n-- View: z_wdpaid_latest\n-- DROP VIEW z_wdpaid_latest;\n\nCREATE OR REPLACE VIEW z_wdpaid_latest AS \n WITH a AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2014'::text\n        ), b AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2017'::text\n        ), combined AS (\n                 SELECT a.wdpa_id, a.assessment_cycle, a.assessment_version_id\n                   FROM a\n                  WHERE a.version_code = a.max_code\n        UNION \n                 SELECT b.wdpa_id, b.assessment_cycle, b.assessment_version_id\n                   FROM b\n                  WHERE b.version_code = b.max_code\n        )\n SELECT combined.wdpa_id, combined.assessment_cycle, max(combined.assessment_version_id) AS assessment_version_id\n   FROM combined\n  GROUP BY combined.wdpa_id, combined.assessment_cycle\n  ORDER BY combined.wdpa_id, combined.assessment_cycle;\n\nALTER TABLE z_wdpaid_latest\n  OWNER TO postgres;\n\n\n\n\nThe rationale is to identify the largest value of \nassessment_version_code\n for each cycle and for each \nassessment_id\n, on the basis that the latest/newly created id should be naturally larger. By finding this code, it is guaranteed that the latest versions are used, irrespective of the version number and stage, which may be wrong.\n\n\nOnce the above is done, you can now update the links in the MS Access database, using the 'Linked Table Manager' so that they point to the most recent local database as a source\n\n\n\n\nMake sure you tick 'Always prompt for new location', so that you update links for all tables in one go\n\n\n\n\nCreate a new data source, here you'll need to specify the details of the newly 'restored' database\n\n\n\n\nAnalysis\n\n\nThe analysis is fully documented below:\n\n\n\n\nAnalysis of World Heritage Outlook assessment", 
            "title": "World Heritage Outlook"
        }, 
        {
            "location": "/world-heritage-outlook/#history", 
            "text": "I built the first prototype of the Outlook assessment module in Python (Web2Py framework), as proof of concept. The underlying principle is to store the results of assessment in a database so that it could be managed and analysed more easily and effectively. A database approach is the only way to systematically manage the data the project generates.  For the development, RSMI was commissioned to undertake the design, implementation and maintenance of the three modules: assessment, site information, and front-end website for showcasing findings. In hindsight, it was an unrealistic ambition with impossible funds, but at the time the can-do spirit prevailed and I was technically in-mature and inexperienced as to make do with developing everything, rather than reality check and focusing on the minimally viable product.  In hindsight, part of the reason was that the roles were never clear, I think I was given the role of project management, yet I had no power in making design choices nor allocate funds. My role became an inefficient intermediate - I led numerous consultations with the team during every milestone, and had constant struggles to accommodate opinions and then feed to the developers. The decision making process was unnecessarily long and painful. The team was tired and so was I.   Despite the hiccups along the way, a product was developed with all three modules, albeit late. The database played a major role to fulfilling the requirement of data analysis. I built the pipelines from database dumps to queries that output assessments, and then turn into usable formats. These powered the first outlook report.  My role in the second development is a technical advisory one and not involved in the daily management of the development.", 
            "title": "History"
        }, 
        {
            "location": "/world-heritage-outlook/#technical-architecture", 
            "text": "The assessment module is written in Java, a custom-made system that accepts assessments via the web and store in a PostgreSQL database. The user facing front end website displays results of assessment and was implemented in Liferay, which during the second development changed to a Drupal based system (written in PHP).  At the moment, there is a disconnection between the two systems: assessment and display. The assessment module and the old website is on one Amazon EC2 instance (instance name: 'IUCN WHP WHO') and the new, updated website resides another (instance name: 'IUCN WHO V2'). The new website used a database dump to migrate content from the old system. By design, the pipeline built by EDW does not reverse the flow of data in the other direction.  The source codes can be found on  GitHub", 
            "title": "Technical architecture"
        }, 
        {
            "location": "/world-heritage-outlook/#administration", 
            "text": "As mentioned previously, the assessment module and the front facing website displaying results of assessment are hosted by Amazon (Amazon Web Services, AWS), and the servers are known as EC2 instances or servers. The old and new websites are of the type 'medium' (used to describe the capacity of the server or instance), physically in the 'eu-west-1a' region (in Ireland). Additionally, there is a 'micro' instance for the developers called 'IUCN WHO test', who used it for testing. It was decided that this 'micro instance' will be kept for future use.   The two additional 'micro' instances are used for hosting 1) the prototype of the World Heritage comparative analysis , 'WH CA' (seed funded by UNESCO WHC) and 2) some products on the World Heritage Analyses, 'WH APP'.   In order to assess these instances (use SSH), you will need 'private keys'. The keys are located at (WCMC-PC-01918)   E:\\Yichuan\\Yichuan\\amazon_putty_ssh   The keys may be revoked in the  AWS management console . The login credentials are:   Yichuan.shi@iucn.org  (see my last email)   You may copy and send 'keys' to developers so that they have access to the servers, however, I would highly recommend  not  giving external people access to our Amazon AWS account.  The management of the instances are beyond the scope of this documentation, however, the below are the most commonly used features:   create new instances  the IP address of the instance (address for access)  the private keys (key for access)  the ports that need to be open (TCP22 for SSH, and TCP80 and 443 for HTTP/S)", 
            "title": "Administration"
        }, 
        {
            "location": "/world-heritage-outlook/#database-dump", 
            "text": "One of the key function of the system(s) is to enable the extraction and analysis of information for the World Heritage Outlook report. For that purpose, it is important to obtain a copy of the dataset in the live database behind the website (but  never  work on the live database itself!). This is called a database dump.  For security reasons, you will need to SSH tunnel to the instance where the database is located and forward the remote port to local. There is a saved session called  WHO_backend  that contains necessary connection information and also forwards the remote 5432 port (the database port) to the local 5433 port.    Once you've successfully logged using that saved session, open PgAdmin, the database management tool to connect to your local port 5433, which now points to the remote machine via SSH tunnel.    You may now select the database from which you'd like to make a dump. Use the 'backup' button (see red arrow) to create a file which holds the content of the database.   On your local machine, create an empty new database and then 'restore' (see the blue arrow) to create an identical copy of the remote database.   This concludes the database dump or 'extraction of data'.", 
            "title": "Database dump"
        }, 
        {
            "location": "/world-heritage-outlook/#convert-to-access-database", 
            "text": "The data in the database dump is a replica of the database serving the website. Unfortunately the data is not stored in a format that allows us to gain useful insight, as bits of information scatter throughout numerous tables. You may refer to the diagram for additional information   E:\\Yichuan\\IT\\ER_diagram_130802   For the most commonly used queries, I have built a re-usable Microsoft Access database that connects to the PostgreSQL database. This allows also to export to more familiar excel spreadsheet people find easy to work with.   E:\\Yichuan\\Elena\\WHOA_171027    !IMPORTANT!    There is one manual step that needs to be performed to assist the analysis before updating the links.   We need to differentiate the most recent assessment ids, and separate them for the two assessment cycles. This is essential to pull out the correct versions of the assessment for analysis. This step can be done in Access but would be much easier in the PostgreSQL database. In your newly restored database, create a view called 'z_wdpa_latest' using the below:  -- View: z_wdpaid_latest\n-- DROP VIEW z_wdpaid_latest;\n\nCREATE OR REPLACE VIEW z_wdpaid_latest AS \n WITH a AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2014'::text\n        ), b AS (\n         SELECT whp_whp_sites.wdpa_id, whp_site_assessment_versions.assessment_id, whp_site_assessment_versions.assessment_version_id, whp_whp_sites.name_en, whp_site_assessment.assessment_cycle, whp_site_assessment_versions.version_code, max(whp_site_assessment_versions.version_code) OVER (PARTITION BY whp_site_assessment_versions.assessment_id) AS max_code\n           FROM whp_site_assessment, whp_site_assessment_versions, whp_whp_sites\n          WHERE whp_site_assessment.assessment_id = whp_site_assessment_versions.assessment_id AND whp_whp_sites.site_id = whp_site_assessment.site_id AND whp_site_assessment.assessment_cycle::text = '2017'::text\n        ), combined AS (\n                 SELECT a.wdpa_id, a.assessment_cycle, a.assessment_version_id\n                   FROM a\n                  WHERE a.version_code = a.max_code\n        UNION \n                 SELECT b.wdpa_id, b.assessment_cycle, b.assessment_version_id\n                   FROM b\n                  WHERE b.version_code = b.max_code\n        )\n SELECT combined.wdpa_id, combined.assessment_cycle, max(combined.assessment_version_id) AS assessment_version_id\n   FROM combined\n  GROUP BY combined.wdpa_id, combined.assessment_cycle\n  ORDER BY combined.wdpa_id, combined.assessment_cycle;\n\nALTER TABLE z_wdpaid_latest\n  OWNER TO postgres;  The rationale is to identify the largest value of  assessment_version_code  for each cycle and for each  assessment_id , on the basis that the latest/newly created id should be naturally larger. By finding this code, it is guaranteed that the latest versions are used, irrespective of the version number and stage, which may be wrong.  Once the above is done, you can now update the links in the MS Access database, using the 'Linked Table Manager' so that they point to the most recent local database as a source   Make sure you tick 'Always prompt for new location', so that you update links for all tables in one go   Create a new data source, here you'll need to specify the details of the newly 'restored' database", 
            "title": "Convert to Access database"
        }, 
        {
            "location": "/world-heritage-outlook/#analysis", 
            "text": "The analysis is fully documented below:   Analysis of World Heritage Outlook assessment", 
            "title": "Analysis"
        }, 
        {
            "location": "/presentation/", 
            "text": "Prior to 2016, all presentations are stored on the workstation (WCMC-PC-01918.internal.wcmc):\n\n\n\n\nE:\\Yichuan\\Yichuan\\presentations\n\n\n\n\nNew format presentations have been used since late 2016. They can be found online:\n\n\n\n\nNew format presentations", 
            "title": "Presentation"
        }, 
        {
            "location": "/folder-structure/", 
            "text": "The content can be located at \nE:\\Yichuan\n, on the workstation (WCMC-PC-01918.internal.wcmc)\n\n\nMost of my content is organised into folders with names of collaborator or those who request help. This is the highest level. Within this level, project files can be found if this is a one-off request; or folders representing different projects. \n\n\nFolders named \nnot after\n people usually refer to one-off/non-specific, large or recurring projects. Below is a concise explanation of important folders (the most important folders are in bold), or those needs clarification:\n\n\n\n\nAdmin: all admins, including timesheets, travel authorisations, visas, tickets, cost reclaims and etc\n\n\nBasedata\n: most base data are located here (spatial). Typically they include base physical, cultural boundaries, grids, hillshade, ecoregions etc. Recommendation: use esri ArcCatalog to navigate\n\n\nCOM_XXX: preparations for World Heritage Committee meetings\n\n\nComparative_analysis_20XX\n: results of comparative analysis of that year. They will generally include digitised polygons, results of the analysis in tables, and also maps (including map templates that create them)\n\n\nDggrid: discrete global hexagon grids program\n\n\nDocuments: unsorted \ndocx\n or \npdf\n format documents prior to 2014\n\n\nDump_PG: unsorted database dump prior to 2014\n\n\nElena\n: including World Heritage Outlook data dumps and analysis files are located\n\n\nExperiment: test space for various tools, analysis and scripts, unsorted and \ndo not use\n\n\nGood_reference_map_collection: a collection of maps for reference\n\n\nIT\n: project folder for the first World Heritage Outlook system development \n\n\nLandsat_archiving: python based script for the archiving project, superseded by the \nnear real time Landsat 8 images\n in the Lab. \ndo not use\n\n\nMap_templates: outdated template \ndo not use\n\n\nMapmart: 0.5m resolution WorldView imagery purchased to identify mining activities, in Russia\n\n\nMyGDB.gdb\n: a somewhat historic/duplicate place holder for base data, but in esri geodatabase format. It mainly contains biogeographic, broadscale priorities and old site level priorities such as KBA datasets. It also includes world vector shore line data (including EEZs). While I do not recommend using any of the datasets, \ndo not delete\n this database, as there may be map documents referring to the data here-within \n\n\nMyWorkplace.gdb: similar to 'Experiment' folder but in geodatabase format\n\n\nPapers: feeder folder for Mendeley, the reference management system for papers\n\n\nRed_List_data\n: raw Red List data releases from 2013\n\n\nRemote sensing: test data for raw satellite images (mostly Landsat) collected throughout my time for ad-hoc projects\n\n\nScripts\n: arguably the most important folder. Sub folder structure below\n\n\n/geoprocessing\n: scripts and libraries for undertaking spatial analysis, generic data analysis, workflow automation etc. See the \ngeoprocessing\n section for more information. This is also backed up on \nGitHub\n\n\n/scripts\n: historic place holder for scripts. Most one off, ad-hoc scripts are here.\n\n\n/mysql20110516: outdated SQL scripts for maintaining the PostgreSQL database\n\n\n\n\n\n\nsites_improve_geometry_XXX: series of attempts to improve the qualities of WH datasets until 2015\n\n\nsitesXXXX: digitised boundaries for new nominations, and successful inscriptions and/or modifications. \n\n\nTentativeList: outdated tentative list sites improvement. \ndo not use\n\n\nUpdates: outdated map document to update the database and no longer relevant \ndo not use\n\n\nWDPA: copies of WDPA monthly releases for analysis\n\n\nWH_benefits: project folder for the first WH benefits analysis\n\n\nWH_stats\n: statistics after COM\n\n\nWHO_geom_updates: methodology for updating the maps in the old WH Outlook system. This may not be relevant any more\n\n\nWHS.gdb\n: spatial data for natural and mixed World Heritage sites, in geodatabase format\n\n\nWHS_arcgisonlineXXX.gdb\n: spatial data to be zipped and uploaded to the arcgisonline. This is the data behind the \nesri feature service\n\n\nWHS_dump_ATTR\n: attributes/non spatial data for natural and mixed World Heritage sites\n\n\nWHS_dump_KML: historical dumps of WH data in KML format, superseded by the feature service\n\n\nWHS_dump_SHP: shapefile format of the WH data\n\n\nWHS_map_batcher: map template to automate map production\n\n\nWHS_quality_check: map comparisons between GIS, official maps from UNESCO, including retrospective ones. The numerous comparisons underpin and empower the systematic check and subsequently the improvement of the data.\n\n\nYichuan: miscellaneous", 
            "title": "Folder structure"
        }
    ]
}